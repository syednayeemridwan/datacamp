{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The process of :\n",
    "    - reviewing and cleaning data\n",
    "    - deriving insights (descriptive statistics, correlation)\n",
    "    - generate hypothesis for experiments\n",
    "    - explore the grouped distribution\n",
    "    - Visualize numeric distribution (histogram) and categorical distribution (barplot)\n",
    "        - Use kdeplot for visualizing subgroup distribution in one plot for comparison with `cut` specified\n",
    "    - Handle outliers\n",
    "        - See and Visualize numeric mean, max with boxplot to gain insight about outliers\n",
    "        - filter data to drop outliers\n",
    "    - Handle missing data:\n",
    "        - Drop data if missing data in the column <= 5%\n",
    "        - Impute missing values otherwise\n",
    "        - Impute by sub-group since each subgroup may contain different type of data\n",
    "    - Handle diverse categories:\n",
    "        - There might be so many diverse categories that needs to be generalized\n",
    "        - eg: job as NLP engineer and Text Analyst may Fall into Machine Learning Engineer job\n",
    "    - Convert numeric data according to desired standard\n",
    "        - eg: EUR to USD value, or standardization\n",
    "    - Visualize correlation with scatterplot (or use pairplot)\n",
    "        - correlation only gives linear relationship information\n",
    "        - correlation might be close to 0, but scatter plot may show strong non-linear relationship\n",
    "        - Correlation might be high, but scatterplot may show quadratic relationship\n",
    "    - Check if sample accurately represents the population\n",
    "        - Check class imbalance with value counts\n",
    "        - Check combination of classes with crosstab for both median value and counts\n",
    "    - Create necessary columns for further analysis\n",
    "        - From category or string column to numeric column\n",
    "        - From range of value to categorical by cutting into bins\n",
    "        - From date column to more granular representation (weekday, month etc)\n",
    "    - Generate hypothesis and arrive at conclusive evidence\n",
    "        - Gather enough data and check correlations\n",
    "        - Do all correlation make sense? Test with hypothesis testing\n",
    "        - Hypothesis test will tell if given enough data, the relationship is true or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Group \"another_col\" column by \"col1\" and \"col2\" and\n",
    "# produce min, max and sum of the grouped data\n",
    "df.groupby([\"col1\",\"col2\"])[\"another_col\"].agg([min,max,sum])\n",
    "# Way 2\n",
    "df.groupby(\"cat_col\").agg({\"col1\": [\"mean\", \"std\"], \"col2\": [\"median\"]})\n",
    "# Way 3\n",
    "books.groupby(\"some_col\").agg(\n",
    "    mean_col1=(\"col1\", \"mean\"),\n",
    "    std_col2=(\"col2\", \"std\"),\n",
    "    median_col3=(\"col3\", \"median\")\n",
    ")\n",
    "# Multi-index groupby\n",
    "df.groupby(level=0).agg({'col':'mean'}) # Outermost = level 0\n",
    "# Size per group\n",
    "df.groupby('col').size()\n",
    "# Adding subgroup aggregation information into dataframe\n",
    "df[\"std_dev\"] = df.groupby(\"cat_col\")[\"num_col\"].transform(lambda x: x.std())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal EDA procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# See sample data\n",
    "df.head()\n",
    "# See dataframe info\n",
    "df.info()\n",
    "# See info about columns\n",
    "df.dtypes\n",
    "# See specified data type columns\n",
    "df.select_dtypes(\"number\").head()\n",
    "# Convert to desired data type\n",
    "df[\"col\"] = df[\"col\"].astype(int)\n",
    "# See categories, label frequencies and value distribution\n",
    "df[\"cat_col\"].value_counts(normalize=True) # Check class imbalance\n",
    "pd.crosstab(df[\"cat_col1\" ], df[ \"cat_col2\" ]) # Check distribution of class combinations\n",
    "pd.crosstab(df[\"cat_col1\" ], df[ \"cat_col2\" ],values=planes[\"num_col\"], aggfunc=\"median\")) # Check distribution of median values in class combinations\n",
    "# Validating categorical columns\n",
    "df[\"cat_col\"].isin([\"Cat 1\", \"Cat 2\"])\n",
    "# See numerical statistics\n",
    "df.describe()\n",
    "# Visualize distribution of numeric data\n",
    "sns.histplot(data=df, x=\"col\", binwidth=.1)\n",
    "sns.boxplot(data=df, x=\"num_col\", y=\"cat_col\") # Also looks for outlier\n",
    "# Visualize distribution of categorical data\n",
    "sns.barplot(data=df, x=\"cat_col\", y=\"num_col\")\n",
    "# Explore group distribution\n",
    "df.groupby(\"cat_col\").agg({\"col1\": [\"mean\", \"std\"], \"col2\": [\"median\"]})\n",
    "# Generalize categorical data : Select n-th generalized category for n-th satisfied condition\n",
    "df[\"Generalized_Category\"] = np.select(condition_list, generalized_category_list, default=\"Other\")\n",
    "# Filtering outliers\n",
    "seventy_fifth = df[\"num_col\"].quantile(0.75)\n",
    "twenty_fifth = df[\"num_col\"].quantile(0.25)\n",
    "iqr = seventy_fifth - twenty_fifth\n",
    "upper = seventy_fifth + (1.5 * iqr)\n",
    "lower = twenty_fifth - (1.5 * iqr)\n",
    "df_without_outliers = df[(df[\"num_col\"] > lower) & (df[\"num_col\"] < upper)]\n",
    "# Create new categorical columns\n",
    "twenty_fifth = df[\"num_col\"].quantile(0.25)\n",
    "median = df[\"num_col\"].median()\n",
    "seventy_fifth = df[\"num_col\"].quantile(0.75)\n",
    "maximum = df[\"num_col\"].max()\n",
    "labels = [\"A\", \"B\", \"C\",\"D\"]\n",
    "bins = [0, twenty_fifth, median, seventy_fifth, maximum]\n",
    "planes[\"Price_Category\"] = pd.cut(df[\"num_col\"], labels=labels, bins=bins)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Check missing data\n",
    "df.isna().any()\n",
    "df.isna().sum()\n",
    "# Visualize missing data information\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "msno.matrix(df)\n",
    "plt.show()\n",
    "\n",
    "# Drop missing data column\n",
    "df_dropped = df.dropna(subset = ['col'], axis = 1) # 0 for row\n",
    "df.dropna(axis = 0) # Drop entire row for missing value (default)\n",
    "df.dropna(axis = 1) # Drop entire column for missing value\n",
    "\n",
    "# Replace/impute missing data with single value\n",
    "col_mean = df['col'].mean()\n",
    "df_imputed = df.fillna({'col': col_mean})\n",
    "df['col'].replace(to_replace=np.nan, value = some_mean,inplace = True) # Alternative\n",
    "# Replace/impute missing data with series\n",
    "series_imp = df['col1'] * 5\n",
    "df_imputed = df.fillna({'col2':series_imp})\n",
    "\n",
    "df[\"col\"].value_counts() # Look out for suspicious values\n",
    "\n",
    "##### Strategic dropping example ########\n",
    "# Drop missing values where <= 5% of data in column are missing , otherwise impute values\n",
    "threshold = len(df) * 0.05\n",
    "cols_to_drop = df.columns[df.isna().sum() <= threshold]\n",
    "df.dropna(subset=cols_to_drop, inplace=True)\n",
    "cols_with_missing_values = df.columns[salaries.isna().sum() > 0]\n",
    "for col in cols_with_missing_values[:-1]:\n",
    "    df[col].fillna(df[col].mode()[0])\n",
    "subgroup_dict = df.groupby(\"cat_col\")[\"num_col\"].median().to_dict()\n",
    "df[\"num_col\"] = df[\"num_col\"].fillna(df[\"cat_col\"].map(subgroup_dict))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Way 1 : During import\n",
    "df = pd.read_csv('filename.csv', parse_dates = ['date_col1', 'date_col2'])\n",
    "# Way 2 : Parsing using pandas date format\n",
    "df[\"date_col\"] = pd.to_datetime(df[\"date_col\"], \n",
    "                                format = \"%Y-%m-%d %H:%M:%S\",\n",
    "                                errors='coerce')\n",
    "# Way 3  : parsing using python date format\n",
    "df[\"date_col\"] = df[\"date_col\"].dt.strftime(\"%d-%m-%Y\")\n",
    "# Extract month information\n",
    "df[\"date_col\"].dt.month\n",
    "# Extract year information\n",
    "df[\"date_col\"].dt.year\n",
    "\n",
    "# Resampling date\n",
    "df.resample('M', on = 'date_col')['col1'].mean()\n",
    "# Resampling count\n",
    "df.resample('M', on = 'date_col').size()\n",
    "# Add timezone in a datetime column\n",
    "df['date_col'] = df['date_col'].dt.tz_localize('America/New_York', ambiguous='NaT')\n",
    "# Convert to another timezone\n",
    "df['date_col'] = df['date_col'].dt.tz_convert('Europe/London')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "# Create date\n",
    "d =  date(2017, 6, 21) # ISO format: YYYY-MM-DD\n",
    "# Create a datetime\n",
    "dt = datetime(year= 2017 , month= 10 , day= 1 , hour= 15 , minute= 23 , second= 25 , microsecond= 500000 )\n",
    "# Change value of existing datetime\n",
    "dt_changed = dt.replace(minute=0, second=0, microsecond=0)\n",
    "# Sort date\n",
    "dates_ordered = sorted(date_list)\n",
    "# Parse datetime\n",
    "dt = datetime.strptime(\"12/30/2017 15:19:13\", \"%m/%d/%Y %H:%M:%S\")\n",
    "d.isoformat() # Express the date in ISO 8601 format\n",
    "print(d.strftime(\"%Y/%m/%d\")) # Print date in Format: YYYY/MM/DD\n",
    "print(dt.strftime(\"%Y-%m-%d %H:%M:%S\")) # Print datetime in specific format\n",
    "# Extract information\n",
    "d.year # Extract year\n",
    "d.month # Extract month\n",
    "d.day # Extract day\n",
    "d.weekday() # Extract weekday\n",
    "##### Date addition / subtraction\n",
    "from datetime import timedelta\n",
    "delta = d2 - d # Subtract two dates\n",
    "delta.days # Elapsed time in days\n",
    "delta.total_seconds() # Elapsed time in seconds\n",
    "td = timedelta(days=29) # Create a 29 day timedelta\n",
    "print(d + td) # Add delta with existing date\n",
    "# timestamp value\n",
    "ts = 1514665153.0\n",
    "# Convert to datetime from timestamp and print\n",
    "print(datetime.fromtimestamp(ts))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Co-efficient and p-value\n",
    "from scipy import stats\n",
    "pearson_coef, p_val = stats.pearsonr(df[\"col1\"], df[\"col2\"])\n",
    "# Visualize correlation\n",
    "import seaborn as sns\n",
    "sns.lmplot(x=\"col1\", y=\"col2\", data=df, ci=None)\n",
    "plt.show()\n",
    "\n",
    "# Find correlation among all columns in a dataframe\n",
    "correlations = df.corr() \n",
    "# Use heatmap for correlation visualization (Scatterplot is not a good choice for dataframes with more than 2 variables)\n",
    "sns.heatmap(correlations, annot=True) # use cmap='coolwarm' to provide color map\n",
    "# Bend x label ticks 45 degree to avoid overlappings with each-other\n",
    "plt.xticks(rotation=45) \n",
    "plt.title('Correlations')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDE plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sns.kdeplot(data=df, x=\"num_col\", hue=\"cat_col\", cut=0, cumulative=False)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate hypothesis based on assumption of correlation\n",
    "- Find feasibility of assumptions with hypothesis testing to reach conclusive evidence"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
