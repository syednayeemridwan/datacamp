{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create new features (eg: averaging, BMI etc )\n",
    "- Visualize distribution with boxplot, pairplot of dataset to see if Transformation is necessary (eg: log transformation)\n",
    "- Normalize/Standardize/Scale features\n",
    "- Encoding : Convert categories into numeric data\n",
    "    - One-hot encoding : Explainable features, create N columns for N categories\n",
    "    - Dummy encoding : Necessary information without duplication, create N-1 columns for N categories\n",
    "- Merge low frequent categorical values (uncommon categories) into one single category (eg: `other`)\n",
    "- Binarise numeric values (eg: from `num_violations` to `violation_boolean`)\n",
    "- Deal with missing values:\n",
    "    - drop missing values that are beyond threshold (>30% of dataset)\n",
    "    - fill completely random missing values (with mean, median, mode, `Other`, sorted next present value)\n",
    "- Deal with outliers\n",
    "- Validate numeric columns\n",
    "    - remove characters from numeric data (eg: `$` or `,` sign for currency)\n",
    "    - make sure the column is in proper datatype (eg: `float`, `int` etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Visualize distribution\n",
    "sns.pairplot(df)\n",
    "df[['column_1']].boxplot()\n",
    "plt.show()\n",
    "\n",
    "# One-hot encoding\n",
    "pd.get_dummies(df, columns=['cat'], prefix='C')\n",
    "# Dummy encoding\n",
    "pd.get_dummies(df, columns=['cat'], drop_first=True, prefix='C')\n",
    "\n",
    "# Merging low frequency categorical counts\n",
    "counts = df['cat'].value_counts()\n",
    "mask = df['cat'].isin(counts[counts < 5].index) \n",
    "df['cat'][mask] = 'Other'\n",
    "\n",
    "# Binarizing numeric variables\n",
    "df['Binary_col'] = 0 \n",
    "df.loc[df['Number_col'] > 0, 'Binary_col'] = 1\n",
    "import numpy as np\n",
    "df['Binned_Group'] = pd.cut( df['Number_col'], bins=[-np.inf, 0, 2, np.inf], labels=[1, 2, 3])\n",
    "\n",
    "# SCALE / STANDARDIZE DATA\n",
    "# DEAL WITH MISSING VALUES.....\n",
    "# DEAL WITH OUTLIERS\n",
    "\n",
    "# Validate numeric columns\n",
    "df['RawSalary'] = df['RawSalary'].str.replace(',', '').astype('float')\n",
    "coerced_vals = pd.to_numeric(df['RawSalary'], errors='coerce')\n",
    "print(df[coerced_vals.isna()].head()) # Sanity check which values still show errors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# create intervals for equal-sized 5 bins\n",
    "bins = np.linspace(df[\"price\"].min(), df[\"price\"].max(),5)\n",
    "custom_labels = [\"low\",\"medium\",\"high\"]\n",
    "df[\"price_bin\"] = pd.cut(df[\"price\"], bins, labels=custom_labels, include_lowest=True)\n",
    "\n",
    "# Alternative approach\n",
    "df['price_bin'] = pd.qcut(df['price'], q=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# See column names\n",
    "df.columns\n",
    "# Set column names\n",
    "df.columns = ['A', 'B', 'C']\n",
    "# Data type of columns\n",
    "df.dtypes\n",
    "# Select column of specific types only\n",
    "df_ints = df.select_dtypes(include=['int'])\n",
    "# Set type of a column\n",
    "df['num_col']=df['num_col'].astype(int)\n",
    "# See column description\n",
    "df.describe()\n",
    "# See column information\n",
    "df.info()\n",
    "# See frequencies in categorical column\n",
    "df['cat'].value_counts()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# One-hot-encoding on categorical variable\n",
    "df_onehot = pd.get_dummies(df, columns=['cat'], prefix='C')\n",
    "df_dummy = pd.get_dummies(df, columns=['cat'], drop_first=True, prefix='C')\n",
    "\n",
    "# Alternative approach-2\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "onehot_transformed = encoder.fit_transform(df['cat_col'].values.reshape(-1,1))\n",
    "# Convert into dataframe\n",
    "onehot_df = pd.DataFrame(onehot_transformed.toarray())\n",
    "# Add the encoded columns with original dataset, \n",
    "df = pd.concat([df, onehot_df], axis=1)\n",
    "# Drop the original column that you used for encoding \n",
    "df = df.drop('cat_col', axis=1)\n",
    "\n",
    "# Label encoding : Turning string labels into numeric values\n",
    "from sklearn import preprocessing\n",
    "encoder_lvl = preprocessing.LabelEncoder()\n",
    "# Specify the unique categories in the column to apply one-hot encoding\n",
    "encoder_lvl.fit([ 'LOW', 'NORMAL', 'HIGH'])\n",
    "# Apply one hot encoding on the third column of the dataset\n",
    "df[:,2] = encoder_lvl.transform(df[:,2]) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Show number of missing data\n",
    "df.isna().sum()\n",
    "\n",
    "# Visualize missing data information\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "msno.matrix(airquality)\n",
    "plt.show()\n",
    "\n",
    "# Drop missing data\n",
    "df_dropped = df.dropna(subset = ['col'])\n",
    "\n",
    "# Replace/impute missing data with single value\n",
    "col_mean = df['col'].mean()\n",
    "df_imputed = df.fillna({'col': col_mean})\n",
    "\n",
    "# Replace/impute missing data with series\n",
    "series_imp = df['col1'] * 5\n",
    "df_imputed = df.fillna({'col2':series_imp})\n",
    "\n",
    "# Missing values are not always \"NaN\". They can be blank, \"?\" or other symbols (rarely)\n",
    "# Check for values through manual validations first\n",
    "df[\"col\"].value_counts() # Look out for suspicious values\n",
    "# Determine number of missing values in a column\n",
    "df.isna().any()\n",
    "df['col'].isnull().sum()\n",
    "# Drop missing values\n",
    "df.dropna(axis = 0) # Drop entire row for missing value (default)\n",
    "df.dropna(axis = 1) # Drop entire column for missing value\n",
    "# Drop missing values for specific column\n",
    "df.dropna(subset = [\"col\"], axis = 0)\n",
    "# Replace missing values\n",
    "df[\"col\"].replace(np.nan, new_val)\n",
    "df.fillna(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- uniform distribution:\n",
    "\t- all outcomes are equally likely outcome\n",
    "    - flat probability density function across the entire range.\n",
    "- binomial distribution:\n",
    "\t- discrete probability distribution\n",
    "    - binary outcomes (success or failure)\n",
    "    - independent trials\n",
    "- normal distribution:\n",
    "    - symmetrical\n",
    "    - probability never hits 0\n",
    "    - described by mean and std\n",
    "    - standard normal distribution has mean 0 and std 1\n",
    "    - 65% area in 1-SD of mean, 95% area in 2-SD of mean, 99.7% are in 3-SD of mean\n",
    "    - Central Limit Theorem : sampling distribution becomes closer to the normal distribution as the number of trials increases when sampling is done purely randomly and independently (mean of sample means/std etc).\n",
    "- Poisson distribution:\n",
    "    - events appear at certain rate (constant rate) over a fixed interval of time\n",
    "    - expected value (lambda) represents average number of events per unit time interval\n",
    "    - events occurrence is completely random\n",
    "    - Discrete event (Since it represents number of events)\n",
    "    - eg: 5 adoptions each week from a pet shelter. However at which time they will be adopted is random.                                                                        \n",
    "- Exponential distribution:\n",
    "    - probability of time between poisson events\n",
    "    - Same lambda as average rate as poisson distribution\n",
    "    - Continuous event (Since it represents time)\n",
    "    - scale = 1/ lambda, where it measures number of time per unit event\n",
    "    - example: one person requests ticket every 2 minutes. \n",
    "        - So, 1 minute serves 0.5 request. So, poisson rate of lambda = 0.5\n",
    "        - And, Exponential rate of lambda, = 1/ lambda = 1/ 0.5 = 2\n",
    "- t-distribution\n",
    "    - tails are thicker than normal distribution\n",
    "    - Observations are more likely to fall further from the mean\n",
    "    - has degree of freedom that controls the thickness of the tail\n",
    "        - lower degree of freedom = thicker tail + higher std\n",
    "        - higher degree of freedom = thinner tail + lower std = more like normal distribution\n",
    "- Log normal distribution\n",
    "    - logarithm of variable is normally distributed\n",
    "    - Works on mean and standard daviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Method 1\n",
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out\n",
    "\n",
    "# Method 2\n",
    "def remove_outliers(df_in, col_name):\n",
    "    mean = df_in[col_name].mean()\n",
    "    std = df_in[col_name].std()\n",
    "    cut_off = std * 3\n",
    "    lower, upper = mean - cut_off, mean + cut_off\n",
    "    df_out = df_in[(df_in[col_name] < upper) & (df_in[col_name] > lower)]\n",
    "    return df_out\n",
    "\n",
    "# Method 3 : Not recommended\n",
    "def trim_outliers(df_in, col_name, quantile_value=0.95):\n",
    "    quantile = df_in[col_name].quantile(quantile_value)\n",
    "    df_out = df_in[df_in[col_name] < quantile]\n",
    "    return df_out\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Feature Scaling\n",
    "df[\"feature_scaled\"] = df[\"col\"]/ (df[\"col\"].max())\n",
    "# Min-max Scaling\n",
    "df[\"minmax_scaled\"] = (df[\"col\"] - df[\"col\"].min()) / (df[\"col\"].max() - df[\"col\"].min())\n",
    "# Z-score\n",
    "df[\"z_scaled\"] = (df[\"col\"] - df[\"col\"].mean()) / df[\"col\"].std() \n",
    "\n",
    "# Alternative : Using scikit learn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "log_scaler = PowerTransformer()\n",
    "your_scaler.fit(df[['col']])\n",
    "df['scaled_col'] = your_scaler.transform(df[['col']])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
