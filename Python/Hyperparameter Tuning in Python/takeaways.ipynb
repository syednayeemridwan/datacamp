{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finding optimal combination of parameters for a model\n",
    "- parameters: \n",
    "    - the ones that is set by the model after learning from dataset \n",
    "    - eg: co-efficients of linear regression, node decision by the decision trees\n",
    "    - accessible by attribute (in the attribute section in the documentation)\n",
    "- hyperparameters : \n",
    "    - the ones that we have the option to set before creating the model\n",
    "    - print the estimator to see what it contains\n",
    "    - accessible by parameter (in the parameter section in the documentation)\n",
    "- Silly things to do (some examples):\n",
    "    - Creating a random forest with just 2 or 3 trees\n",
    "    - 1,2 neighbors in knn algorithm\n",
    "    - increasing a hyperparameter by a small amount\n",
    "    - Be aware of conflicting hyperparameter choices (The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties.)\n",
    "- Visualize if the hyperparameter has any effect:\n",
    "    - Graph of learning curve : hyperparameter on X-axis and accuracy on Y-axis\n",
    "- Problem: So many models can be build. But among these, find an optimal model that yields optimal result.\n",
    "- Solution: Train with a set of adjustable parameters and compare the results to find the optimal model\n",
    "- Rule of thumb : Cross validation is used to estimate the generalization performance.\n",
    "- Curse of dimensionality : exhaustively searching results in exponential increase of dimensions with the increase of grid.\n",
    "- Best practice : Do this when you really need optimal solution since it does not make a bad model into a good model.\n",
    "- optimal hyperparameters = set of hyperparameters corresponding to the best CV score.\n",
    "- Some algorithms:\n",
    "    - Uninformed Search:\n",
    "        - Grid Search : \n",
    "            - Find result for all possible combination of parameters \n",
    "            - Guaranteed best result\n",
    "            - time consuming process, resource intensive\n",
    "        - Random Search : \n",
    "            - Randomly choose a number of combinations of given parameters \n",
    "            - A good result but may not be the absolute best\n",
    "            - fast \n",
    "            - Idea : You are unlikely to keep completely missing the 'good area' for a long time when randomly picking new spots\n",
    "    - Informed Search :\n",
    "        - Coarse to Fine: (Hybrid of grid search and randomized search)\n",
    "            1. Random search\n",
    "            2. Find promising areas (Narrow Down)\n",
    "            3. Grid search in the smaller area or skip for step 4\n",
    "            4. Continue from step 1 until optimal score is obtained\n",
    "            - Idea : Narrow down the optimal area. The best result will be in that area.\n",
    "        - Bayesian Optimization :\n",
    "            - Inferring the probability of best result by deducing the outcome of past results\n",
    "            - eg: Given a certain event occurs due to another event, we get more clearer idea of the probability of certain outcome\n",
    "            - Idea : Getting better as we get more evidence.\n",
    "        - Genetic Algorithms :\n",
    "            1. We can create some models (that have hyperparameter settings)\n",
    "            2. We can pick the best (by our scoring function). These are the ones that 'survive'\n",
    "            3. We can create new models that are similar to the best ones\n",
    "            4. We add in some randomness so we don't reach a local optimum\n",
    "            5. Repeat until we are happy!\n",
    "            - Idea : With evolution in gene sequence combination of best genes in new generations, we can obtain the optimal being"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, make_scorer, roc_auc_score\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 42)\n",
    "# Instantiate individual classifiers\n",
    "lr = LogisticRegression(random_state=42)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state=42,max_depth=4, min_samples_leaf=0.16)\n",
    "classifiers = [('Logistic Regression', lr),\n",
    "                ('K Nearest Neighbours', knn),\n",
    "                ('Classification Tree', dt)]\n",
    "\n",
    "# Import ensemble classifiers and regressors\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor, BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, AdaBoostRegressor, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "# Voting Ensemble\n",
    "ensemble_model_voting = VotingClassifier(estimators=classifiers)\n",
    "ensemble_model_voting = VotingRegressor(estimators=regressors)\n",
    "# Bagging Ensemble\n",
    "ensemble_model_bagging = BaggingClassifier(base_estimator=dt, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "ensemble_model_bagging = BaggingRegressor(base_estimator=dt, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "oob_score = ensemble_model.oob_score_\n",
    "# Random Forest Ensemble\n",
    "ensemble_model_randomforest = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=42)\n",
    "ensemble_model_randomforest = RandomForestClassifier(n_estimators=400, random_state=42)\n",
    "# Adaboost Ensemble\n",
    "ensemble_model_adaboost = AdaBoostClassifier(base_estimator=dt, n_estimators=100) # dt is weak, has max depth of 1\n",
    "ensemble_model_adaboost = AdaBoostRegressor(base_estimator=dt, n_estimators=100) # dt is weak, has max depth of 1\n",
    "# Gradient Boost Ensemble\n",
    "# (max_features=0.2, subsample=0.8) makes it stochastic gradient boosting due to randomness of data and fraction of data features\n",
    "ensemble_model_gboost = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=42)\n",
    "ensemble_model_gboost = GradientBoostingClassifier(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=42)\n",
    "\n",
    "# Train using traing set\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "# Predict with test set\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "# Evaluate accuracy and ROC AUC for classification\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "y_pred_proba = ensemble_model.predict_proba(X_test)[:,1]\n",
    "clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "# Evaluate RMSE for regression\n",
    "rmse = mean_squared_error(y_test, y_pred)**(1/2)\n",
    "# Visualize features importances\n",
    "importances = pd.Series(ensemble_model.feature_importances_, index = X.columns)\n",
    "sorted_importances = importances.sort_values()\n",
    "sorted_importances.plot(kind='barh', color='lightgreen')\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Split into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_Train, X_Test, y_Train, y_Test = train_test_split(X, y, test_size=0.3, random_state=3)\n",
    "\n",
    "# Make sure to take into account the class imbalance \n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "w_train = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "# Train the classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n",
    "tree_clf.fit(X_Train,y_Train, sample_weight=w_train)\n",
    "\n",
    "# Alternative approach : Train the classifier with snapml (offers multi-threaded CPU/GPU training)\n",
    "from snapml import DecisionTreeClassifier\n",
    "snapml_dt_gpu = DecisionTreeClassifier(max_depth=4, random_state=45, use_gpu=True)\n",
    "snapml_dt_cpu = DecisionTreeClassifier(max_depth=4, random_state=45, n_jobs=4)\n",
    "snapml_dt.fit(X_train, y_train, sample_weight=w_train)\n",
    "# Predict\n",
    "y_pred = tree_clf.predict(X_Test)\n",
    "\n",
    "### Inspecting a random forest\n",
    "# Pull out one tree from the forest (If decision tree is a random forest)\n",
    "chosen_tree = randomforest_model.estimators_[7] # You can visualize it with (graphviz & pydotplus)\n",
    "# Extract node decisions\n",
    "split_column = chosen_tree.tree_.feature[0] # Get the first column it split on\n",
    "split_column_name = X_train.columns[split_column] # Name of the column\n",
    "split_value = chosen_tree.tree_.threshold[1] # Get the theshold value it split on\n",
    "\n",
    "# Compute predicted probabilities\n",
    "y_pred_prob = tree_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Evaluate tree\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "accuracy_score(y_testset, predTree)\n",
    "roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Visualize the graph using plot_tree\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(chosen_tree, feature_names=X_train.columns, filled=True, rounded=True, fontsize=10)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search, Random Search, Coarse Search, Bayes Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, make_scorer\n",
    "# See what parameters can be tuned\n",
    "model.get_params()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "custom_params = {\n",
    "    'max_depth': [3, 4,5, 6],\n",
    "    'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "    'max_features': [0.2, 0.4,0.6, 0.8]\n",
    "}\n",
    "mae_scorer = make_scorer(mean_absolute_error)\n",
    "# Grid Search\n",
    "model_cv1 = GridSearchCV(estimator=model, param_grid=custom_params, cv=kf, scoring= mae_scorer , # 'neg_mean_squared_error' \n",
    "            verbose=1, n_jobs=-1, refit = True, random_state=42)\n",
    "# Randomized Search\n",
    "model_cv2 = RandomizedSearchCV(estimator=model, param_distributions=custom_params, n_iter=10, \n",
    "            cv=kf, scoring=mse_scorer,  # Use custom scorer here\n",
    "            verbose=1, n_jobs=-1, refit=True, random_state=42)\n",
    "# Bayes Search\n",
    "search_spaces = {\n",
    "    'max_depth': (3, 6),\n",
    "    'min_samples_leaf': (0.04, 0.08, 'uniform'),\n",
    "    'max_features': (0.2, 0.8, 'uniform')\n",
    "}\n",
    "model_bayes_cv = BayesSearchCV(estimator=model, search_spaces=search_spaces, cv=kf,\n",
    "    n_iter=50,  # Adjust the number of iterations as needed\n",
    "    scoring=mae_scorer, verbose=1, n_jobs=-1, refit=True, random_state=42 )\n",
    "\n",
    "# use TPOT for GENETIC SEARCH CV\n",
    "\n",
    "model_cv.fit(X_train, y_train)\n",
    "model_cv.cv_results_ # See all information from dictionary\n",
    "best_hyperparams = model_cv.best_params_ # Get the parameters that produce best result\n",
    "best_model = model_cv.best_estimator_  # Get the best model\n",
    "best_model.get_params() # Get the parameters of the best model\n",
    "y_pred = best_model.predict(X_test) # predict with best model\n",
    "best_score = best_model.best_score_ # Best result\n",
    "\n",
    "# Visualize contribution of parameter to get the optimal accuracy (Scatterplot or kdeplot)\n",
    "results_df = pd.DataFrame({\n",
    "    'Accuracy': model_cv.cv_results['mean_test_score'],\n",
    "    'Parameter': model_cv.cv_results['param_name']  # Adjust the parameter name as needed\n",
    "})\n",
    "plt.scatter(results_df['Parameter'], results_df['Accuracy'], s=100, alpha=0.5)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(best_model, 'my_best_model.pkl') # Save the model in pkl file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See function python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "print(inspect.getsource(func_name))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "r2_score(y_true, y_pred) # R-squared\n",
    "mse = mean_squared_error(y_true, y_pred) # MSE\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False) # RMSE\n",
    "mae = mean_absolute_error(y_true, y_pred) # MAE\n",
    "\n",
    "# Classification Performance Measurement\n",
    "from sklearn.metrics import classification_report, confusion_matrix, jaccard_score, log_loss, roc_auc_score, roc_curve, f1_score\n",
    "confusion_matrix(y_test, y_pred) # Confusion matrix\n",
    "classification_report(y_test, y_pred) # TP, FP, TN, FN\n",
    "jaccard_score(y_test, y_pred,pos_label=0) # Jaccard score\n",
    "log_loss(y_test, y_pred_prob) # log loss\n",
    "print(roc_auc_score(y_test, y_pred_prob)) # ROC AUC\n",
    "print(f1_score(y_true, y_pred)) # F1 Score\n",
    "\n",
    "# Visualize ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# Grid-search example for hyperparameter tuning of classification\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "param_grid = {\"alpha\": np.arange(0.0001, 1, 10), \"solver\": [\"sag\", \"lsqr\"]}\n",
    "ridge = Ridge()\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=kf)\n",
    "ridge_cv2 = RandomizedSearchCV(ridge, param_grid, cv=kf, n_iter=2)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "print(ridge_cv.best_params_, ridge_cv.best_score_)\n",
    "\n",
    "### Compare different models distribution\n",
    "results = {\"Model 1\": model1_cv_results, \"Model 2\": model2_cv_results, \"Model 3\": model3_cv_results}\n",
    "plt.boxplot(results.values(), labels=results.keys())\n",
    "plt.show()\n",
    "\n",
    "# Leverage : measurement of how extreme the explanatory variable values are\n",
    "leverage = model.get_influence().hat_matrix_diag\n",
    "# Influence : how much the model would change if you leave the observation out of the dataset when modeling. (eg : cooks distance)\n",
    "influence = model.get_influence().resid_studentized_external\n",
    "cooks_distance = model.get_influence().cooks_distance[0]\n",
    "\n",
    "# Residualplot for regression\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals, color='blue', alpha=0.6)\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Fitted Values (Predicted)')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# Q-Q plot for regression\n",
    "from scipy.stats import probplot\n",
    "probplot(residuals.flatten(), dist='norm', plot=plt)\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Sample Quantiles')\n",
    "plt.show()\n",
    "\n",
    "# Scale location plot\n",
    "plt.scatter(y_pred, np.sqrt(np.abs(residuals)), color='blue', alpha=0.6)\n",
    "plt.xlabel('Fitted Values (Predicted)')\n",
    "plt.ylabel('Square Root of Absolute Residuals')\n",
    "plt.axhline(y=np.mean(np.sqrt(np.abs(residuals))), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/04.01.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Statistical method that uses new evidence to iteratively update probabilistic outcome\n",
    "- Formula : P(A|B) = P(B|A) P(A) / P(B)\n",
    "- Left side of equation P(A|B)\n",
    "    - Probability of event A given another event B (This situation is also called posterior outcome)\n",
    "    - Signifies the updated probability of A, when a new evidence / event B occurs\n",
    "- Right side of equation:  (how it happens)\n",
    "    - P(A) = Probability of event A if there is no event B (This situation is also called prior outcome)\n",
    "    - P(B) = marginal likelihood. (probability of observing new event B )\n",
    "    - P(B|A) = The likelihood of B when event A is present\n",
    "\n",
    "- Example:\n",
    "    - 5% of population have a disease, P(D) = 0.05\n",
    "    - 10% of population are affected by a generic condition, P(G) = 0.1\n",
    "    - We have measured from clinical records that 20% of those that have the disease are also affected by genetic condition, P(G|D) = 0.1\n",
    "    - Question is, what is the chance that the disease is caused by the genetic condition, P(D|G) = ??\n",
    "    - If we did not have data of Genetic condition, P(G), we could conclude that anyone has 5% chance of having the disease\n",
    "    - Since we have more information, we can deduce that the likelihood of a person having the disease changes according to whether the person has genetic condition or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization for hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Assuming `model`, `X_train`, `y_train`, and `kf` are defined earlier in your code\n",
    "\n",
    "# Define the search space for hyperopt\n",
    "search_space = {\n",
    "    'max_depth': hp.quniform('max_depth', 3, 6, 1),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 0.04, 0.08),\n",
    "    'max_features': hp.uniform('max_features', 0.2, 0.8)\n",
    "}\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def objective(params):\n",
    "    model.set_params(**params)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kf, scoring=mae_scorer)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Initialize Trials to store optimization results\n",
    "trials = Trials()\n",
    "\n",
    "# Use fmin from hyperopt to perform Bayesian optimization\n",
    "best_hyperparams = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,  # Adjust the number of evaluations as needed\n",
    "    trials=trials,\n",
    "    verbose=1,\n",
    "    rstate=np.random.RandomState(42)\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_bayes_hyperparams = {key: best_hyperparams[key] for key in search_space}\n",
    "\n",
    "# Set the best hyperparameters to the model\n",
    "model.set_params(**best_bayes_hyperparams)\n",
    "\n",
    "# Fit the model with the best hyperparameters\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(model, 'my_best_bayes_model.pkl')\n",
    "\n",
    "# Access the results from hyperopt\n",
    "hyperopt_results = pd.DataFrame({\n",
    "    'Accuracy': [-trial['result']['loss'] for trial in trials.results],\n",
    "    'Parameter': [trial['misc']['vals'] for trial in trials.trials]\n",
    "})\n",
    "\n",
    "# Visualize contribution of parameter to get the optimal accuracy (Scatterplot or kdeplot)\n",
    "plt.scatter(hyperopt_results['Parameter'], hyperopt_results['Accuracy'], s=100, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.externals import joblib  # Import joblib for model persistence\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define a custom scoring function using make_scorer\n",
    "custom_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Define TPOT configuration for classification\n",
    "tpot_config = {\n",
    "    'sklearn.ensemble.RandomForestClassifier': {\n",
    "        'n_estimators': [10, 50, 100],\n",
    "        'max_depth': [3, 6],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': [0.2, 0.4, 0.6, 0.8]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize TPOTClassifier with custom scorer\n",
    "tpot_classifier = TPOTClassifier(\n",
    "    generations=5,  # Adjust the number of generations as needed\n",
    "    population_size=20,\n",
    "    random_state=42,\n",
    "    verbosity=2,\n",
    "    config_dict=tpot_config,\n",
    "    cv=kf,\n",
    "    scoring=custom_scorer,  # Use custom scorer for classification\n",
    "    n_jobs=-1,\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "# Fit TPOTClassifier\n",
    "tpot_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Access TPOT results\n",
    "tpot_results = tpot_classifier.evaluated_individuals_\n",
    "\n",
    "# Export the best TPOT pipeline to a .pkl file\n",
    "best_tpot_pipeline = tpot_classifier.fitted_pipeline_\n",
    "joblib.dump(best_tpot_pipeline, 'best_tpot_classifier_model.pkl')\n",
    "\n",
    "# Save the best TPOT pipeline\n",
    "tpot_classifier.export('best_tpot_classifier_pipeline.py')\n",
    "\n",
    "# Visualize contribution of pipeline to get the optimal accuracy (Scatterplot or kdeplot)\n",
    "results_df = pd.DataFrame({\n",
    "    'Accuracy': tpot_results['internal_cv_score'],\n",
    "    'Pipeline': tpot_results['pipeline']\n",
    "})\n",
    "plt.scatter(results_df['Pipeline'], results_df['Accuracy'], s=100, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
