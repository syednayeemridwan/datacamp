{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "\t.master('local[*]') \\\n",
    "    .appName(\"Load and Query CSV with SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+\n",
      "|                user|              song|plays|\n",
      "+--------------------+------------------+-----+\n",
      "|fd50c4007b68a3737...|SOBONKR12A58A7A7E0|    1|\n",
      "|fd50c4007b68a3737...|SOEGIYH12A6D4FC0E3|    1|\n",
      "|fd50c4007b68a3737...|SOFLJQZ12A6D4FADA6|    1|\n",
      "+--------------------+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('user', 'string'), ('song', 'string'), ('plays', 'int')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msd = spark.read.csv(\"dataset/kaggle_visible_evaluation_triplets.txt\", inferSchema=True, sep='\\t')\n",
    "msd = msd.withColumnRenamed(\"_c0\", \"user\")\\\n",
    "        .withColumnRenamed(\"_c1\", \"song\")\\\n",
    "        .withColumnRenamed(\"_c2\", \"plays\")\n",
    "msd.show(3)\n",
    "msd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "users = msd.select(\"user\").distinct().coalesce(1) # coalesce to put them in a single partition for consistent increase of id\n",
    "users = users.withColumn(\"userId\", monotonically_increasing_id()).persist() # caching to make sure the values do not change\n",
    "songs = msd.select(\"song\").distinct().coalesce(1) # coalesce to put them in a single partition for consistent increase of id\n",
    "items = songs.withColumn(\"songId\", monotonically_increasing_id()).persist() # caching to make sure the values do not change\n",
    "msd = msd.join(users, \"user\", \"left\").join(items, \"song\", \"left\") # If rating dataframe already exists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirm understanding of implicit rating concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between \"implicit\" ratings and \"explicit\" ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explicit ratings are values that users have given to explicitly rate their preferences. Implicit ratings are \"implied\" from user behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSD summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get familiar with the Million Songs Echo Nest Taste Profile data subset. For purposes of this course, we'll just call it the Million Songs dataset or msd. Let's get the number of users and the number of songs. Let's also see which songs have the most plays from this subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd = msd.sample(fraction=0.01, seed=42).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----+------+------+\n",
      "|              song|                user|plays|userId|songId|\n",
      "+------------------+--------------------+-----+------+------+\n",
      "|SOSASFL12A6D4F7B02|b815763c18263b545...|    3| 23896|     0|\n",
      "|SOUZDNN12A6701C563|0e041cb084b8da194...|    5| 24932|   264|\n",
      "|SOPZXJL12A6310D817|8d051ab32db6e1da1...|    1| 45709|   252|\n",
      "+------------------+--------------------+-----+------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Number of users:  13443\n",
      "Number of songs:  10251\n"
     ]
    }
   ],
   "source": [
    "# Look at the data\n",
    "msd.show(3)\n",
    "\n",
    "# Count the number of distinct userIds\n",
    "user_count = msd.select(\"userId\").distinct().count()\n",
    "print(\"Number of users: \", user_count)\n",
    "\n",
    "# Count the number of distinct songIds\n",
    "song_count = msd.select(\"songId\").distinct().count()\n",
    "print(\"Number of songs: \", song_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are going to combine the .groupBy() and .filter() methods that you've used previously to calculate the min() and avg() number of users that have rated each song, and the min() and avg() number of songs that each user has rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd = msd.withColumnRenamed(\"plays\", \"num_plays\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum implicit ratings for a song: \n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n",
      "Average implicit ratings per song: \n",
      "+------------------+\n",
      "|        avg(count)|\n",
      "+------------------+\n",
      "|1.4247390498487953|\n",
      "+------------------+\n",
      "\n",
      "Minimum implicit ratings from a user: \n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n",
      "Average implicit ratings per user: \n",
      "+------------------+\n",
      "|        avg(count)|\n",
      "+------------------+\n",
      "|1.0864390389050063|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,avg, min, max\n",
    "# Min num implicit ratings for a song\n",
    "print(\"Minimum implicit ratings for a song: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"songId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings per songs\n",
    "print(\"Average implicit ratings per song: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"songId\").count().select(avg(\"count\")).show()\n",
    "\n",
    "# Min num implicit ratings from a user\n",
    "print(\"Minimum implicit ratings from a user: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"userId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings for users\n",
    "print(\"Average implicit ratings per user: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"userId\").count().select(avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many recommendation engines use implicit ratings. In many cases these datasets don't include behavior counts for items that a user has never purchased. In these cases, you'll need to add them and include zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z = msd.withColumnRenamed(\"songId\", \"productId\")\n",
    "# # View the data\n",
    "# Z.show()\n",
    "\n",
    "# # Extract distinct userIds and productIds\n",
    "# users = Z.select(\"userId\").distinct()\n",
    "# products = Z.select(\"productId\").distinct()\n",
    "\n",
    "# # Cross join users and products\n",
    "# cj = users.crossJoin(products)\n",
    "\n",
    "# # Join cj and Z\n",
    "# Z_expanded = cj.join(Z, [\"userId\", \"productId\"], \"left\").fillna(0)\n",
    "\n",
    "# # View Z_expanded\n",
    "# Z_expanded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify ALS hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now going to build your first implicit rating recommendation engine using ALS. To do this, you will first tell Spark what values you want it to try when finding the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the lists below\n",
    "ranks = [10, 20, 30, 40]\n",
    "maxIters = [10, 20, 30, 40]\n",
    "regParams = [.05, .1, .15]\n",
    "alphas = [20, 40, 60, 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build implicit models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have all of your hyperparameter values specified, let's have Spark build enough models to test each combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ALS_874a3d2d3102, ALS_1cb757d7b50a, ALS_d5d5d593863b, ALS_3c1adaa8758d, ALS_6c0d22fb2e96, ALS_721976a90760, ALS_928d66ca0bdd, ALS_f25c2e118c90, ALS_685ebf3350a2, ALS_6ae6f4889021, ALS_ae70b9ef5776, ALS_b1bcc0715909, ALS_aee901af5d6f, ALS_9c935460b57a, ALS_e9b8c6305e94, ALS_f58ce5e70d91, ALS_4b6d5585eff3, ALS_a2e7eaad3c54, ALS_835170778132, ALS_a8398acb3218, ALS_abf0cbae7998, ALS_35a226d32a82, ALS_1a3d3e2f9307, ALS_7253f4d795e2, ALS_9cdead0ad7e5, ALS_7ae4b71e737e, ALS_c6688d1c2f3b, ALS_0c45a6b0d63e, ALS_f6d4ab44317f, ALS_a03d3d86bec4, ALS_946fc3fc8469, ALS_5f42acc8462e, ALS_84a218f12c2c, ALS_0cefd72f2458, ALS_284328efb959, ALS_db068cfef072, ALS_9a5eca83ec70, ALS_fe2d7d10f377, ALS_05d31a0570d1, ALS_8991874169f9, ALS_a1f5274093cc, ALS_0d712edd6c83, ALS_5caf48c1cd03, ALS_952446eef5cb, ALS_8192407f54be, ALS_40b7f40a0bae, ALS_11355873a821, ALS_4840ecade58d, ALS_ccd5c444214b, ALS_fee5146ff8c1, ALS_6db28d3e0ff8, ALS_df3f80def4db, ALS_8859a7b3e40e, ALS_1ab331cf2a9d, ALS_9f9ddcabf9f2, ALS_5a2a65297737, ALS_ce7673a95769, ALS_1fbc2a926ed0, ALS_6dfe9bdf9173, ALS_623a6a5b4fd4, ALS_c166f3096575, ALS_0286e556699e, ALS_ac8f594a01ca, ALS_820541bdeecc, ALS_7903c01f767b, ALS_21d7f4f36cc0, ALS_f2fc98c63c66, ALS_2cb4b73bfecd, ALS_efae46220042, ALS_8540283d87bb, ALS_21d2c946a4a5, ALS_9dc04adcee5d, ALS_051ddeaf6c51, ALS_398b4d816eb2, ALS_4199bb4694b2, ALS_21023866ec8c, ALS_5f69423d77cc, ALS_675e14b9318b, ALS_2ba5584d083e, ALS_ff64aae4ac75, ALS_8944735acad7, ALS_b4e232e9734b, ALS_91393608e010, ALS_5509c9bc835a, ALS_8bdfaa28dd3a, ALS_6f67011888ad, ALS_df1a8cd34cab, ALS_ce2d0fbc366e, ALS_d16c5a9382ed, ALS_49ceef2c2774, ALS_9b0b32c83752, ALS_9c48b4a82513, ALS_cb7fafbfd35e, ALS_9c1ce636f911, ALS_0260fad4df3b, ALS_6905c8abbcc4, ALS_2e028d9de7cd, ALS_67d159cabf3d, ALS_39dd75bd6bc0, ALS_b1f056c24faa, ALS_fa4741a1289e, ALS_e6d61ff9e939, ALS_04d0b2d61805, ALS_cb6e54151f5b, ALS_e85618d669da, ALS_197e828747e7, ALS_3db2df0a5d68, ALS_671b3dc5772d, ALS_da46f7af0323, ALS_0759f023a82a, ALS_a46ac6812899, ALS_216b6d4bd163, ALS_2d1d87a00013, ALS_2640552a422e, ALS_4ac6fa61ad93, ALS_5d6e998403e7, ALS_36769707c934, ALS_abad1d9a0000, ALS_be46ba957e43, ALS_2aa927ddc5a4, ALS_145cb0923e20, ALS_3a94388f260e, ALS_42e9639e15bd, ALS_5c1e326db9d1, ALS_1500549a6d80, ALS_d761124030a6, ALS_ba61f24b2777, ALS_d6770e3a5a44, ALS_30ee4fc239b8, ALS_f340a695448c, ALS_91a0f0d72974, ALS_718c30566c3f, ALS_653a10299eda, ALS_3bd2dae7f36b, ALS_9a189275360f, ALS_f809b7b2b393, ALS_7835836491c9, ALS_6c91e53ded1b, ALS_945259b5b437, ALS_0ae92595a6b4, ALS_d12b3de749dd, ALS_fbbb39a262e5, ALS_2eb9fe2c45e8, ALS_c477fd6c5b90, ALS_064783c2388a, ALS_09503e76e523, ALS_4345fa5a6224, ALS_36b7a9850b9d, ALS_b79d071f73ba, ALS_87292c320ad5, ALS_a9f4f1bd7c13, ALS_996102ecfaad, ALS_5f996b871d43, ALS_a80071f93db3, ALS_f5d91445dc55, ALS_27d0421516a1, ALS_2d6b2f1e64fb, ALS_7b9369be3a6b, ALS_62896a7f3053, ALS_03896bff9aaa, ALS_83cbdaad8f82, ALS_43935401480f, ALS_cd89272fdb57, ALS_1ae3b7aaba0a, ALS_eeec8101977f, ALS_ef0e839593a0, ALS_308171cf32d0, ALS_f8e10c9ba66d, ALS_48d85747b66e, ALS_78a3e7f82f2a, ALS_2d02739341d9, ALS_8bb815ace10b, ALS_8b1cbcc8eaf9, ALS_6ee274969598, ALS_8bc1bc89d7f8, ALS_80cbaef6f53c, ALS_52fd1931977c, ALS_73b7037c0874, ALS_4527f08314b8, ALS_ea8c4da491b7, ALS_9c6a1c0619ca, ALS_04a871eb84ae, ALS_2ca1e106a7f3, ALS_94801f8e0eb2, ALS_97de66629ece, ALS_a183f85335aa, ALS_76e4224bb26a, ALS_6dd7c3ce96b8, ALS_c32f583fba29, ALS_2c2464ff09a1, ALS_4bdad087dc31, ALS_9fb4e19f59bc] Length of model_list:  192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "model_list = []\n",
    "# For loop will automatically create and store ALS models\n",
    "for r in ranks:\n",
    "    for mi in maxIters:\n",
    "        for rp in regParams:\n",
    "            for a in alphas:\n",
    "                model_list.append(ALS(userCol= \"userId\", itemCol= \"songId\", ratingCol= \"num_plays\", rank = r, maxIter = mi, regParam = rp, alpha = a, coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = True))\n",
    "\n",
    "# Print the model list, and the length of model_list\n",
    "print (model_list, \"Length of model_list: \", len(model_list))\n",
    "\n",
    "# Validate\n",
    "len(model_list) == (len(ranks)*len(maxIters)*len(regParams)*len(alphas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a cross-validated implicit ALS model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have several ALS models, each with a different set of hyperparameter values, we can train them on a training portion of the msd dataset using cross validation, and then run them on a test set of data and evaluate how well each one performs using the ROEM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ROEM is a metric used to evaluate the performance of recommendation systems for implicit ratings of ALS algorithm.\n",
    "# # ROEM stands for Rank Ordering Error Metric \n",
    "# # Unfortunately, pyspark do not provide native support for ROEM.\n",
    "# # Here is a custom implementation of ROEM\n",
    "\n",
    "# def ROEM(predictions, userCol=\"userId\", itemCol=\"songId\", ratingCol=\"num_plays\"):\n",
    "#     # Create table that can be queried\n",
    "#     predictions.createOrReplaceTempView(\"predictions\")\n",
    "#     # Sum of total number of plays of all songs\n",
    "#     denominator = predictions.groupBy().sum(ratingCol).collect()[0][0]\n",
    "#     # Calculating rankings of songs predictions by user\n",
    "#     spark.sql(\n",
    "#         \"SELECT \" + userCol + \" , \" + ratingCol + \" , PERCENT_RANK() OVER (PARTITION BY \" + userCol + \" ORDER BY prediction DESC) AS rank FROM predictions\"\n",
    "#     ).createOrReplaceTempView(\"rankings\")\n",
    "#     # Multiplies the rank of each song by the number of plays and adds the products together\n",
    "#     numerator = spark.sql('SELECT SUM(' + ratingCol + ' * rank) FROM rankings').collect()[0][0]\n",
    "#     # Compute ROEM\n",
    "#     roem = numerator / denominator\n",
    "#     return roem\n",
    "    \n",
    "# # Split the data into training and test sets\n",
    "# (training, test) = msd.randomSplit([0.8, 0.2])\n",
    "# #Building 5 folds within the training set.\n",
    "# train1, train2, train3, train4, train5 = training.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], seed = 1)\n",
    "# fold1 = train2.union(train3).union(train4).union(train5)\n",
    "# fold2 = train3.union(train4).union(train5).union(train1)\n",
    "# fold3 = train4.union(train5).union(train1).union(train2)\n",
    "# fold4 = train5.union(train1).union(train2).union(train3)\n",
    "# fold5 = train1.union(train2).union(train3).union(train4)\n",
    "\n",
    "# foldlist = [(fold1, train1), (fold2, train2), (fold3, train3), (fold4, train4), (fold5, train5)]\n",
    "\n",
    "# # Empty list to fill with ROEMs from each model\n",
    "# ROEMS = []\n",
    "\n",
    "# # Loops through all models and all folds\n",
    "# for model in model_list:\n",
    "#     for ft_pair in foldlist:\n",
    "#         # Fits model to fold within training data\n",
    "#         fitted_model = model.fit(ft_pair[0])\n",
    "#         # Generates predictions using fitted_model on respective CV test data\n",
    "#         predictions = fitted_model.transform(ft_pair[1])\n",
    "#         # Generates and prints a ROEM metric CV test data\n",
    "#         r = ROEM(predictions)\n",
    "#         print (\"ROEM: \", r)\n",
    "#     # Fits model to all of training data and generates preds for test data\n",
    "#     v_fitted_model = model.fit(training)\n",
    "#     v_predictions = v_fitted_model.transform(test)\n",
    "#     v_ROEM = ROEM(v_predictions)\n",
    "#     # Adds validation ROEM to ROEM list\n",
    "#     ROEMS.append(v_ROEM)\n",
    "#     print (\"Validation ROEM: \", v_ROEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of smallest ROEM: 5\n",
      "Smallest ROEM:  0.12871287128712883\n"
     ]
    }
   ],
   "source": [
    "ROEMS = [0.22772277227722793,\n",
    " 0.3762376237623761,\n",
    " 0.29702970297029685,\n",
    " 0.4455445544554455,\n",
    " 0.13861386138613874,\n",
    " 0.12871287128712883,\n",
    " 0.3663366336633662]\n",
    "# Import numpy\n",
    "import numpy\n",
    "\n",
    "# Find the index of the smallest ROEM\n",
    "i = numpy.argmin(ROEMS)\n",
    "print(\"Index of smallest ROEM:\", i)\n",
    "\n",
    "# Find ith element of ROEMS\n",
    "print(\"Smallest ROEM: \", ROEMS[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now tested 192 different models on the msd dataset, and you found the best ROEM and its respective model (model 38).\n",
    "\n",
    "You now need to extract the hyperparameters. The model_list you created previously is provided here. It contains all 192 models you generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the best_model\n",
    "# best_model = model_list[38]\n",
    "\n",
    "# # Extract the Rank\n",
    "# print (\"Rank: \", best_model.getRank())\n",
    "\n",
    "# # Extract the MaxIter value\n",
    "# print (\"MaxIter: \", best_model.getMaxIter())\n",
    "\n",
    "# # Extract the RegParam value\n",
    "# print (\"RegParam: \", best_model.getRegParam())\n",
    "\n",
    "# # Extract the Alpha value\n",
    "# print (\"Alpha: \", best_model.getAlpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the binary_test_predictions from this model to see what we can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the col function\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # Look at the test predictions\n",
    "# binary_test_predictions.show()\n",
    "\n",
    "# # Evaluate ROEM on test predictions\n",
    "# ROEM(binary_test_predictions)\n",
    "\n",
    "# # Look at user 42's test predictions\n",
    "# binary_test_predictions.filter(col(\"userId\") == 42).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations from binary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you see from the ROEM, these models can still generate meaningful test predictions. Let's look at the actual recommendations now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View user 26's original ratings\n",
    "# print (\"User 26 Original Ratings:\")\n",
    "# original_ratings.filter(col(\"userId\") == 26).show()\n",
    "\n",
    "# # View user 26's recommendations\n",
    "# print (\"User 26 Recommendations:\")\n",
    "# binary_recs.filter(col(\"userId\") == 26).show()\n",
    "\n",
    "# # View user 99's original ratings\n",
    "# print (\"User 99 Original Ratings:\")\n",
    "# original_ratings.filter(col(\"userId\") == 99).show()\n",
    "\n",
    "# # View user 99's recommendations\n",
    "# print (\"User 99 Recommendations:\")\n",
    "# binary_recs.filter(col(\"userId\") == 99).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
