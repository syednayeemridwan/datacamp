{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some movies have multiple genres (a movie can be of both comedy and romance genre)\n",
    "- However, individual people might experience only a single genre from these movies\n",
    "- How to tell which person thinks the movie falls morely on which genre?\n",
    "- Latent feature (also known as Rank) tells that. \n",
    "- ALS algorithm decomposes original table into 2 matrices : User-Movie = User-LatentFeature X LatentFeature-Movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01.01.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How ALS figures out recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.02.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.03.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original Rating table is decomposed into User and Product tables with random numbers\n",
    "- The random values in User table and Product table is adjusted through a number of iterations by reducing r-squared values\n",
    "- The minimized factorized table closely resembles the original Rating table.\n",
    "- While the original table has many missing values, the factorized table now contains the predicted values which can be considered as recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# View left factor matrix\n",
    "print(U)\n",
    "# View right factor matrix\n",
    "print(P)\n",
    "# Multiply factor matrices\n",
    "UP = np.matmul(U,P)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "print(pd.DataFrame(UP, columns = P.columns, index = U.index))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.04.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.05.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements to apply pyspark ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataframe must be in long format (NOT wide format)\n",
    "- 2 separate tables (User table and Product table) should have unique id in integer data-type\n",
    "- the individual tables should be coalesced into one partition to make the id consistent\n",
    "- when using `monotonically_increasing_id`, make sure to cache/persist the dataframes or the values might change\n",
    "- the tables should be merged together to create the Rating table. \n",
    "- ALS is applied on the Rating table\n",
    "- use `nonnegative = True` to ensure positive values\n",
    "- use `rank` to specify number of latent features you want to use\n",
    "- use `implicitPrefs= True` along with `alpha` only when you do not have explicit `rating` column\n",
    "- use `maxIter` to adjust weights for n number of iterations for reduced r-squared\n",
    "- use `coldStartStrategy = \"drop\"` to avoid testing on unknown user-product pair (if training data has no information on particular userid and productid )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.06.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ALS Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data (User ID, Item ID, Rating, Additional Column1, Additional Column2)\n",
    "data = [\n",
    "    (1, 1, 5, \"A\", \"X\"),\n",
    "# .............................\n",
    "    (3, 2, 2, \"F\", \"U\")\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"userId\", \"itemId\", \"rating\", \"user_cat\", \"item_cat\"])\n",
    "# If user ID and/or Item ID do not exist and rather only categories exist, make unique identifier in separate dataframes for each distinct category and then join\n",
    "users = df.select(\"user_cat\").distinct().coalesce(1) # coalesce to put them in a single partition for consistent increase of id\n",
    "users = users.withColumn(\"userId\", monotonically_increasing_id()).persist() # caching to make sure the values do not change\n",
    "items = df.select(\"item_cat\").distinct().coalesce(1) # coalesce to put them in a single partition for consistent increase of id\n",
    "items = items.withColumn(\"itemId\", monotonically_increasing_id()).persist() # caching to make sure the values do not change\n",
    "new_rating_df = df.join(users, \"user_cat\", \"left\").join(items, \"item_cat\", \"left\") # If rating dataframe already exists\n",
    "## NOTE: If rating dataframe does not exist, then create one:\n",
    "users = df1.select(\"userId\").distinct()\n",
    "items = df2.select(\"itemId\").distinct()\n",
    "custom_rating_df = users.crossJoin(items).join(df3, [\"userId\", \"itemId\"], \"left\").fillna(0)\n",
    "\n",
    "# Sparsity = (Number of empty entries) / (Total number of entries)\n",
    "numerator = ratings.count()\n",
    "num_users = ratings.select(\"userId\").distinct().count()\n",
    "num_items = ratings.select(\"itemId\").distinct().count()\n",
    "denominator = num_users * num_items\n",
    "sparsity = (1.0 - (numerator *1.0)/denominator)*100\n",
    "\n",
    "(training_data, test_data) = df.randomSplit([0.8, 0.2]) # Split data into training and test sets\n",
    "\n",
    "# k flod Cross-validation\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "als = ALS(userCol=\"userId\", itemCol=\"itemId\", ratingCol=\"rating\",\n",
    "        coldStartStrategy=\"drop\" , nonnegative =True , implicitPrefs = False)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "param_grid = ParamGridBuilder()\n",
    "                .addGrid(als.rank, [5, 40, 80, 120])\n",
    "                .addGrid(als.maxIter, [5, 100, 250, 500])\n",
    "                .addGrid(als.regParam, [.05, .1, 1.5])\n",
    "                .build()\n",
    "cv = CrossValidator(estimator = als,\n",
    "                estimatorParamMaps = param_grid,\n",
    "                evaluator = evaluator,\n",
    "                numFolds = 5)\n",
    "len(param_grid) # Total number of models in the grid\n",
    "model = cv.fit(training_data) # Train with ALS model\n",
    "best_model = model.bestModel # Best Model\n",
    "predictions = best_model.transform(test_data) # Make predictions on test data\n",
    "rmse = evaluator.evaluate(predictions) # Evaluate predictions using RMSE\n",
    "best_model.getRank()\n",
    "best_model.getMaxIter()\n",
    "best_model.getRegParam()\n",
    "recommendations_df = best_model.recommendForAllUsers(5) # Generate all recommendations for users up to userId 5\n",
    "from pyspark.sql.functions import explode\n",
    "# Explode recommendations column to separate itemId and prediction\n",
    "df = recommendations_df.withColumn(\"itemId_rating\", explode(\"recommendations\")) # Put each recommendation in new row\n",
    "df = df.withColumn(\"itemId\", col(\"itemId_rating\").getField(\"itemId\"))\\\n",
    "                .withColumn(\"rating\", col(\"itemId_rating\").getField(\"rating\")) # Generate separate columns from the pair\n",
    "df.join(ratings, [\"userId\", \"itemId\"], \"left\").filter(ratings['rating'].isNull()).show() # Only show recommendation for items not rated yet\n",
    "\n",
    "### NOTE: For implicit rating columns (eg: No of clicks / binary rating etc) use custom evaluation metric (ROEM) instead of RMSE\n",
    "### Or create weighted rating column by feature engineering. Also, make sure to put \"implicitPrefs = True\" in ALS model param\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "class ROEMEvaluator(Evaluator):\n",
    "    def __init__(self, userCol=\"userId\", itemCol=\"itemId\", ratingCol=\"num_clicks\"):\n",
    "        self.userCol = userCol\n",
    "        self.itemCol = itemCol\n",
    "        self.ratingCol = ratingCol\n",
    "\n",
    "    def _evaluate(self, predictions):\n",
    "        predictions.createOrReplaceTempView(\"predictions\")\n",
    "        denominator = predictions.groupBy().sum(self.ratingCol).collect()[0][0]\n",
    "        rankings = spark.sql(\"SELECT \" + self.userCol + \", \" + self.ratingCol + \", PERCENT_RANK() OVER (PARTITION BY \" + self.  userCol + \" ORDER BY prediction DESC) AS rank FROM predictions\")\n",
    "        rankings.createOrReplaceTempView(\"rankings\")\n",
    "        numerator = spark.sql(\"SELECT SUM(\" + self.ratingCol + \" * rank) FROM rankings\").collect()[0][0]\n",
    "        performance = numerator / denominator\n",
    "        return performance\n",
    "evaluator = ROEMEvaluator( ratingCol=\"colName\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# ROEM is a metric used to evaluate the performance of recommendation systems for implicit ratings of ALS algorithm.\n",
    "# ROEM stands for Rank Ordering Error Metric \n",
    "# Unfortunately, pyspark do not provide native support for ROEM.\n",
    "# Here is a custom implementation of ROEM\n",
    "\n",
    "def ROEM(predictions, userCol=\"userId\", itemCol=\"songId\", ratingCol=\"num_plays\"):\n",
    "    # Create table that can be queried\n",
    "    predictions.createOrReplaceTempView(\"predictions\")\n",
    "    # Sum of total number of plays of all songs\n",
    "    denominator = predictions.groupBy().sum(ratingCol).collect()[0][0]\n",
    "    # Calculating rankings of songs predictions by user\n",
    "    spark.sql(\n",
    "        \"SELECT \" + userCol + \" , \" + ratingCol + \" , PERCENT_RANK() OVER (PARTITION BY \" + userCol + \" ORDER BY prediction DESC) AS rank FROM predictions\"\n",
    "    ).createOrReplaceTempView(\"rankings\")\n",
    "    # Multiplies the rank of each song by the number of plays and adds the products together\n",
    "    numerator = spark.sql('SELECT SUM(' + ratingCol + ' * rank) FROM rankings').collect()[0][0]\n",
    "    # Compute ROEM\n",
    "    roem = numerator / denominator\n",
    "    return roem\n",
    "    \n",
    "# Split the data into training and test sets\n",
    "(training, test) = msd.randomSplit([0.8, 0.2])\n",
    "#Building 5 folds within the training set.\n",
    "train1, train2, train3, train4, train5 = training.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], seed = 1)\n",
    "fold1 = train2.union(train3).union(train4).union(train5)\n",
    "fold2 = train3.union(train4).union(train5).union(train1)\n",
    "fold3 = train4.union(train5).union(train1).union(train2)\n",
    "fold4 = train5.union(train1).union(train2).union(train3)\n",
    "fold5 = train1.union(train2).union(train3).union(train4)\n",
    "\n",
    "foldlist = [(fold1, train1), (fold2, train2), (fold3, train3), (fold4, train4), (fold5, train5)]\n",
    "\n",
    "# Empty list to fill with ROEMs from each model\n",
    "ROEMS = []\n",
    "\n",
    "# Loops through all models and all folds\n",
    "for model in model_list:\n",
    "    for ft_pair in foldlist:\n",
    "        # Fits model to fold within training data\n",
    "        fitted_model = model.fit(ft_pair[0])\n",
    "        # Generates predictions using fitted_model on respective CV test data\n",
    "        predictions = fitted_model.transform(ft_pair[1])\n",
    "        # Generates and prints a ROEM metric CV test data\n",
    "        r = ROEM(predictions)\n",
    "        print (\"ROEM: \", r)\n",
    "    # Fits model to all of training data and generates preds for test data\n",
    "    v_fitted_model = model.fit(training)\n",
    "    v_predictions = v_fitted_model.transform(test)\n",
    "    v_ROEM = ROEM(v_predictions)\n",
    "    # Adds validation ROEM to ROEM list\n",
    "    ROEMS.append(v_ROEM)\n",
    "    print (\"Validation ROEM: \", v_ROEM)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
