{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine Learning : Ability of machines to learn to make decisions from data\n",
    "- Supervised Learning : Machine Learning on labeled data\n",
    "- Unsupervised Learning : Machine Learning on unlabeled data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine Learning of labeled data\n",
    "- Predict target values of unseen data, given the features\n",
    "- Feature = predictor variable = independent variable\n",
    "- Target variable = dependent variable = response variable\n",
    "- 2 types:\n",
    "    - Classification : Target is a category\n",
    "    - Regression : Target is a continuous value\n",
    "- Requirements for supervised learning\n",
    "    - Must have no missing values\n",
    "    - Data is numeric\n",
    "    - X should be in 2D array, y should be in 1D array\n",
    "    - Data is in array (for python, it is the numpy array)\n",
    "    - Perform EDA to see if the data is formatted correctly\n",
    "- Generic Workflow:\n",
    "    1. Create a generic model\n",
    "    2. Fit training data in the model\n",
    "    3. Predict with the model for test data\n",
    "- Some popular algorithms:\n",
    "    - KNN \n",
    "    - Linear Regression\n",
    "    - Ridge Regression (With regularization for large co-efficients)\n",
    "    - Lasso Regression (With regularization for large co-efficients)\n",
    "- Metric for  measuring model performance\n",
    "    - accuracy, F1 score, Precision, Recall for classification\n",
    "    - r-squared (percentage of explanability), RMSE (Average error) for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Supervised Learning Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.module import Model\n",
    "model = Model()\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X_new)\n",
    "print(predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classify label using majority vote of nearest neighbors within a given number of closest neighbors\n",
    "- Large k = simpler model = underfitting = less able to detect relationship\n",
    "- Small k = complex model = overfitting = more prone/sensitive to detect noise\n",
    "- Complexity graph : k on X-axis, model accuracy of train and test set on Y-axis [for different values of k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# First convert dataset to numpy since sklearn uses numpy\n",
    "y = df['target'].values\n",
    "X = df.drop('target', axis=1).values\n",
    "# Normalize the whole dataset before modeling\n",
    "X = preprocessing\\\n",
    "\t.StandardScaler()\\\n",
    "\t.fit(X)\\\n",
    "\t.transform(X.astype(float))\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "# Initialize and train model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='minkowski')\n",
    "knn.fit(X_train, y_train)\n",
    "# Predict the test set class with the trained model\n",
    "predicted_y = knn.predict(X_test)\n",
    "# Measure probability score of prediction for the test set with the trained model\n",
    "predicted_y_prob = knn.predict_proba(X_test)\n",
    "# Measure accuracy on testing set\n",
    "print(accuracy_score(y_test, predicted_y)*100)\n",
    "# Visualize normal distribution of accuracy for different Ks\n",
    "# Compute the above steps for different K and find mean, std etc\n",
    "plt.plot(range(1,Ks),mean_acc,'g')\n",
    "plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n",
    "plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\n",
    "plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Plot complexity graph with list of train and test accuracies\n",
    "plt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n",
    "plt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear regression is a method that let us understand the relationship between dependent and independent variables. \n",
    "- It predicts continuous value. example: y = mx + c\n",
    "- y is target, x is independent feature, m is slope, c is intercept\n",
    "- By training on many datapoints, the model understands value of c and m. Then taking any value x, the model can estimate the value y.\n",
    "- Simple linear regression = Use 1 independent variable for predicting 1 dependent variable\n",
    "- Multiple linear regression = Use multiple independent variables for predicting 1 dependent variable\n",
    "- Noise : \n",
    "    - Error in prediction. \n",
    "    - The error is gaussian noise and the residuals show normal distribution properties. \n",
    "    - The more the error, the more spread out the normal distribution (sigma or standard deviation in normal distribution)\n",
    "    - The best fit line has the least noise.\n",
    "- We define an error function for m and c and choose the line that reduces the error and gain the optimized value for m and c\n",
    "- Error function = loss function = cost function\n",
    "- Residual = distance between datapoint and the fitted line\n",
    "- Our loss function can be RSS (Residual Sum of Squares) the sum of the residuals and our goal is to minimize this value.\n",
    "- metrics \n",
    "    - r-squared : percentage of the variance in target values explained by the features. range is 0 to 1\n",
    "    - RMSE : Root Mean squared error (Average error in prediction)\n",
    "- cross validation : do train-test process in multiple folds and take average to consolidate r-squared.\n",
    "- Regularization : penalizes large co-efficients to reduce overfitting. Some regressions that uses regularizations:\n",
    "    - Lasso Regression\n",
    "    - Ridge Regression\n",
    "- Hyperparameter : Variables used to optimize model parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Construct model\n",
    "lm=LinearRegression()\n",
    "# Simple linear regression uses 1 column with eqn: y = mx + c\n",
    "# Multiple linear regression uses multiple columns with eqn: z = mx + ny +c\n",
    "# Fit the model\n",
    "lm.fit(X_train, y_train)\n",
    "# Predicted estimation\n",
    "y_pred = lm.predict(X_test)\n",
    "# This is intercept of the line (Also known as bias co-efficient)\n",
    "intercept = lm.intercept_\n",
    "# This is slope (m) of the line y=mx+c (Also known as relevant variable's co-efficient)\n",
    "slope = lm.coef_\n",
    "# Percentage of target values explained by the features\n",
    "rsquared = lm.score(X_test, y_test)\n",
    "# RMSE : Average error in prediction\n",
    "mean_squared_error(y_test, y_pred, squared=False)\n",
    "# Prediction of specific range\n",
    "new_x = np.arange(1,101,1).reshape(-1,1) # Or you can make it dataframe\n",
    "new_pred_y = lm.predict(new_x)\n",
    "\n",
    "# Do k-fold cross validation\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "cv_results = cross_val_score(lm, X_train, y_train, cv=kf)\n",
    "# Mean, std and confidence interval of the cross-validation\n",
    "print(np.mean(cv_results), np.std(cv_results), np.quantile(cv_results, [0.025, 0.975]))\n",
    "\n",
    "# Visualize Feature importance\n",
    "names = df.drop(\"target\", axis=1).columns\n",
    "importance = lm.fit(X, y).coef_\n",
    "plt.bar(names, importance)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression and Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge and Lasso regression has regularization parameter alpha which is same as k for knn.\n",
    "- alpha controls model complexity.\n",
    "- large alpha = underfitting = simpler model\n",
    "- small alpha = overfitting = complex model\n",
    "- Complexity graph : alpha on X-axis and r-squared on y-axis\n",
    "- Lasso regression selects important features of dataset (shrinks co-efficients of less important features to 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_scores = []\n",
    "for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    y_pred = ridge.predict(X_test)\n",
    "    ridge_scores.append(ridge.score(X_test, y_test))\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_scores = []\n",
    "for alpha in [0.01, 1.0, 10.0, 20.0, 50.0]:\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    y_pred = lasso.predict(X_test)\n",
    "    lasso_scores.append(lasso.score(X_test, y_test))\n",
    "\n",
    "# Visualize Feature importance\n",
    "names = df.drop(\"target\", axis=1).columns\n",
    "lasso_coef = lasso.fit(X, y).coef_\n",
    "ridge_coef = ridge.fit(X, y).coef_\n",
    "plt.bar(names, lasso_coef, label=\"Lasso co-efficients\")\n",
    "plt.bar(names, ridge_coef, label=\"Ridge co-efficients\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
