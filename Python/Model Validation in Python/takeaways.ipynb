{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- determines MODEL PERFORMANCE\n",
    "- Way to determine that the model is performing in the as expected\n",
    "- common way : Accuracy in unseen data (test data) is same as seen data (train data)\n",
    "- Goal : Choose right model, right parameters, right accuracy metrics, high accuracy on new data\n",
    "- Split dataset : split dataset into 80-20 split. train with 80% and test with 20%\n",
    "- validation dataset : Split the train again into training and validation dataset with 75-25 split.\n",
    "- tune hyperparameters : You need validation dataset for tuning hyperparameters\n",
    "- Accuracy metrics for assessing model performance:\n",
    "    - Regression :\n",
    "        - Good rule of thumb : make your y into percentage, then the metric will also generate percentage value\n",
    "        - Mean absolute error (MAE) : Treats all points equally\n",
    "        - Mean Squared error (MSE) : Issues penalty for large difference\n",
    "    - Classification: \n",
    "        - Measured from confusion matrix : `cm[<true_category_index>, <predicted_category_index>]`\n",
    "        - precision : True positives out of all predicted positive values\n",
    "            - Used when you do not want to over predic-positive values (you need assurity for cancer paitent test)\n",
    "        - recall : True positives out of all real positive values\n",
    "            - Used when you cannot afford to miss any positive values (you need assurity for cancer paitent test)\n",
    "        - accuracy: overall ability of model to predict correct class\n",
    "        - Specificity \n",
    "        - F1-score\n",
    "- Cross validation : \n",
    "    - gets rid of bias result that occurs due to sampling\n",
    "    - breaks training data further into training and validating set for producing compact training result\n",
    "    - LOOCV : trains with n-1 data and validates with only 1 data point (generally used for less data. computationally expensive). During cross validation, put `cv=len(X.shape[0])`\n",
    "- Overfitting : \n",
    "    - accuracy in test data is lower than accuracy in train data (High variance on train data)\n",
    "    - model pays too much attention to the details of training data and also learns noise \n",
    "    - model becomes complex, more data may be needed\n",
    "- Underfitting : \n",
    "    - accuracy in both train and test data is lower since the model is too simple to capture pattern (High bias, low variance)\n",
    "    - model fails to find relationship between the data and the response target value\n",
    "    - model is too simple\n",
    "- BIAS VARIANCE TRADEOFF : \n",
    "    - variance : model pays too much attention to the details of training data and also learns noise\n",
    "    - bias : models fails to learn the details\n",
    "- Regression model : Target is a continuous value\n",
    "- Classification model : Target is a discrete value / category\n",
    "- hyper-parameter tuning : \n",
    "    - 2 types of parameters.\n",
    "    - parameters that do not exist before training the model : you cannot change those (eg: co-efficient of linear regression)\n",
    "    - parameters of the model that can be manually selected to see what parameters might produce optimal result. (eg: maximum depth of a tree)\n",
    "    - GridSearch: Brute force on all available hyper-parameters\n",
    "    - Random Search: randomly selecting from available hyper-parameters\n",
    "    - Bayesian Search : use pass test on each step for the next run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# splits = kf.split(X) # See how they are splitted\n",
    "# r-squared results for 5-fold cross validation score  \n",
    "mae_scorer = make_scorer(mean_absolute_error)\n",
    "scores = cross_val_score(your_model, X, y, cv=kf, scoring=mae_scorer)  # a list of error terms\n",
    "avg_score = np.mean(scores)\n",
    "# predicted_y results for 5-fold cross validation prediction\n",
    "predicted_y = cross_val_predict(your_model, X, y, cv=5) # a list of predictions\n",
    "avg_predicted_y = np.mean(predicted_y)\n",
    "\n",
    "### example of ridge regression with grid search with k-fold cross validation\n",
    "param_grid = {\"alpha\": np.arange(0.0001, 1, 10), \"solver\": [\"sag\", \"lsqr\"]}\n",
    "ridge = Ridge()\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=kf)\n",
    "ridge_cv2 = RandomizedSearchCV(ridge, param_grid, cv=kf, n_iter=2)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, make_scorer\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 42)\n",
    "# Instantiate individual classifiers\n",
    "lr = LogisticRegression(random_state=42)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state=42,max_depth=4, min_samples_leaf=0.16)\n",
    "classifiers = [('Logistic Regression', lr),\n",
    "                ('K Nearest Neighbours', knn),\n",
    "                ('Classification Tree', dt)]\n",
    "\n",
    "# Instantiate an ensemble VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble_model = VotingClassifier(estimators=classifiers)\n",
    "\n",
    "# Instantiate an ensemble VotingRegressor\n",
    "ensemble_model = VotingRegressor(estimators=regressors)\n",
    "\n",
    "# Instantiate an ensemble BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "ensemble_model = BaggingClassifier(base_estimator=dt, n_estimators=300,oob_score=True, n_jobs=-1)\n",
    "oob_accuracy = bc.oob_score_\n",
    "\n",
    "# Instantiate an ensemble BaggingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "base_regressor = DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)\n",
    "ensemble_model = BaggingRegressor(base_estimator=base_regressor, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "oob_score = ensemble_model.oob_score_\n",
    "\n",
    "# Instantiate an ensemble RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "ensemble_model = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=42)\n",
    "\n",
    "# Instantiate an ensemble RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ensemble_model = RandomForestClassifier(n_estimators=400, random_state=42)\n",
    "\n",
    "# Instantiate an ensemble AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ensemble_model = AdaBoostClassifier(base_estimator=dt, n_estimators=100) # dt is weak, has max depth of 1\n",
    "y_pred_proba = ensemble_model.predict_proba(X_test)[:,1]\n",
    "# Evaluate testing roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Instantiate an ensemble GradientBoostingRegressor, (max_features=0.2, subsample=0.8) makes it stochastic gradient boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "ensemble_model = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=42)\n",
    "\n",
    "# Train using traing set\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "# Predict with test set\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "# Evaluate accuracy for classification\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "# Evaluate RMSE for regression\n",
    "rmse = MSE(y_test, y_pred)**(1/2)\n",
    "# Visualize features importances\n",
    "importances = pd.Series(ensemble_model.feature_importances_, index = X.columns)\n",
    "sorted_importances = importances.sort_values()\n",
    "sorted_importances.plot(kind='barh', color='lightgreen')\n",
    "plt.show()\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "# See what parameters can be tuned\n",
    "ryour_dt_model.get_params()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "params_dt = {\n",
    "    'max_depth': [3, 4,5, 6],\n",
    "    'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "    'max_features': [0.2, 0.4,0.6, 0.8]\n",
    "}\n",
    "mae_scorer = make_scorer(mean_absolute_error)\n",
    "model_cv = GridSearchCV(estimator=your_dt_model,\n",
    "    param_grid=params_dt,\n",
    "    cv=kf, # scorer = mae_scorer\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1)\n",
    "model_cv.fit(X_train, y_train)\n",
    "best_hyperparams = model_cv.best_params_# Get the parameters with best result\n",
    "best_model = model_cv.best_estimator_ # Get the best model\n",
    "best_model.get_params() # See all parameters\n",
    "y_pred = best_model.predict(X_test) # predict with best model\n",
    "best_score = best_model.best_score_\n",
    "model_cv.cv_results_ # See all information from dictionary\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(best_model, 'my_best_model.pkl') # Save the model in pkl file\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
