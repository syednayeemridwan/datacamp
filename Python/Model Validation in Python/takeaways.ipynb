{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- determines MODEL PERFORMANCE\n",
    "- Way to determine that the model is performing in the as expected\n",
    "- common way : Accuracy in unseen data (test data) is same as seen data (train data)\n",
    "- Goal : Choose right model, right parameters, right accuracy metrics, high accuracy on new data\n",
    "- Split dataset : split dataset into 80-20 split. train with 80% and test with 20%\n",
    "- validation dataset : Split the train again into training and validation dataset with 75-25 split.\n",
    "- tune hyperparameters : You need validation dataset for tuning hyperparameters\n",
    "- Accuracy metrics for assessing model performance:\n",
    "    - Regression :\n",
    "        - Good rule of thumb : make your y into percentage, then the metric will also generate percentage value\n",
    "        - Mean absolute error (MAE) : Treats all points equally\n",
    "        - Mean Squared error (MSE) : Issues penalty for large difference\n",
    "    - Classification: \n",
    "        - Measured from confusion matrix : `cm[<true_category_index>, <predicted_category_index>]`\n",
    "        - precision : True positives out of all predicted positive values\n",
    "            - Used when you do not want to over predic-positive values (you need assurity for cancer paitent test)\n",
    "        - recall : True positives out of all real positive values\n",
    "            - Used when you cannot afford to miss any positive values (you need assurity for cancer paitent test)\n",
    "        - accuracy: overall ability of model to predict correct class\n",
    "        - Specificity \n",
    "        - F1-score\n",
    "- Cross validation : \n",
    "    - gold standard for model validation\n",
    "    - Every sample produce a slightly different result.\n",
    "    - gets rid of bias result that occurs due to sampling by taking the average of many samples without replacement\n",
    "    - breaks training data further into training and validating set for producing compact training result\n",
    "    - LOOCV : trains with n-1 data and validates with only 1 data point (generally used for less data. computationally expensive). During cross validation, put `cv=len(X.shape[0])`\n",
    "- Overfitting : \n",
    "    - accuracy in test data is lower than accuracy in train data (High variance on train data)\n",
    "    - model pays too much attention to the details of training data and also learns noise \n",
    "    - model becomes complex, more data may be needed\n",
    "- Underfitting : \n",
    "    - accuracy in both train and test data is lower since the model is too simple to capture pattern (High bias, low variance)\n",
    "    - model fails to find relationship between the data and the response target value\n",
    "    - model is too simple\n",
    "- BIAS VARIANCE TRADEOFF : \n",
    "    - variance : model pays too much attention to the details of training data and also learns noise\n",
    "    - bias : models fails to learn the details\n",
    "- Regression model : Target is a continuous value\n",
    "- Classification model : Target is a discrete value / category\n",
    "- hyper-parameter tuning : \n",
    "    - 2 types of parameters.\n",
    "    - parameters that do not exist before training the model : you cannot change those (eg: co-efficient of linear regression)\n",
    "    - parameters of the model that can be manually selected to see what parameters might produce optimal result. (eg: maximum depth of a tree)\n",
    "    - GridSearch: Brute force on all available combination of hyper-parameters\n",
    "    - Random Search: randomly selecting combination from available hyper-parameters\n",
    "    - Bayesian Search : use pass test on each step for the next run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.05.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matrix_confusion = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(matrix_confusion, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification:\n",
    "    - Accuracy is not a good metric for all cases. eg: class imbalanced dataset\n",
    "    - accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
    "    - precision = TP / (TP + FP)\n",
    "    - High precision = lower false positive rate\n",
    "    - recall = TP / (TP + FN)\n",
    "    - High recall = lower false negative rate\n",
    "    - f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    - ROC AUC = area under the curve of TP rate vs FP rate graph\n",
    "    - jaccard score\n",
    "    - log loss\n",
    "- Regression:\n",
    "    - r-squared : Percentage of variability of y explained by independent variable\n",
    "    - RMSE : Root mean squared error. Average error in target\n",
    "    - RSE : Residual standard error. Same as RMSE, except instead of length (n), it requires degree of freedom (n-1)\n",
    "    - MAE : Average absolute error in target\n",
    "    - VResidual vs fit plot : Fitted values on X axis vs Residual on Y axis. Good model has positive and negative values distributed evenly on both side of a line (gaussian noise).\n",
    "    - Q-Q Plot : normal distribution on X axis vs dataset distribution on Y axis. Good model has linear relationship of equation line Y= mX + c\n",
    "    - Distance location plot : Fitted values on X-axis vs root of standardized residuals on Y-axis. \n",
    "    - measurement of extreme values (outliers):\n",
    "        - leverage : measurement of how extreme the explanatory variable values are\n",
    "        - influence : how much the model would change if you leave the observation out of the dataset when modeling. (eg : cooks distance)\n",
    "- Hyperparameter tuning = \n",
    "    - Hyperparameters are Parameters we specify before fitting the model\n",
    "    - We compare model outcomes by changing these hyperparametes \n",
    "    - Use cross-validation to avoid overfitting\n",
    "    - example : gridsearch\n",
    "- Use boxplot of cross-validation results of different models to compare their distribution of scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting : \n",
    "    - Model also memorises / trains on noise that resides within training data. \n",
    "    - Model performs well when evaluating on training data but does not perform well on unseen data\n",
    "    - High variance is responsible for this error because of also capturing noise.\n",
    "    - Diagnosis: cross-val prediction on test set has high error than prediction on train set\n",
    "    - Possible remedy : Decrease model complexity, gather more data, \n",
    "- Underfitting :\n",
    "    - Model is too simple to catch the pattern, model is not good enough to capture the underlying pattern.\n",
    "    - Model is bad on both training and unseen data\n",
    "    - Model is not flexibple enough to approximate the prediction values\n",
    "    - High bias is responsible for this error\n",
    "    - Diagnosis: cross-val prediction on train and test set are roughly equal but have very high errors that is undesirable\n",
    "    - Possible remedy : Increase model complexity, gather more features, \n",
    "- Bias-Variance trade-off :\n",
    "    - Generalization error = bias^2 + variance + irreducable error (noise)\n",
    "    - bias = error term that tells how on average real value is different from predicted value\n",
    "    - variance = error term that tells how predicted value varies over different training sets\n",
    "    - When model complexity increases, variance increases and bias decreases\n",
    "    - When model complexity decreases, variance decreases and bias increases\n",
    "    - The sweet spot is the minimised generalization error, which gives the optimised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.02.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.03.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.04.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# splits = kf.split(X) # See how they are splitted, each split contains index of training and validation\n",
    "# r-squared results for 5-fold cross validation score  \n",
    "mae_scorer = make_scorer(mean_absolute_error)\n",
    "scores = cross_val_score(your_model, X, y, cv=kf, scoring=mae_scorer)  # a list of error terms\n",
    "avg_score = np.mean(scores)\n",
    "# predicted_y results for 5-fold cross validation prediction\n",
    "predicted_y = cross_val_predict(your_model, X, y, cv=5) # a list of predictions\n",
    "avg_predicted_y = np.mean(predicted_y)\n",
    "\n",
    "### example of ridge regression with grid search with k-fold cross validation\n",
    "param_grid = {\"alpha\": np.arange(0.0001, 1, 10), \"solver\": [\"sag\", \"lsqr\"]}\n",
    "ridge = Ridge()\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=kf)\n",
    "ridge_cv2 = RandomizedSearchCV(ridge, param_grid, cv=kf, n_iter=2)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, make_scorer\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 42)\n",
    "# Instantiate individual classifiers\n",
    "lr = LogisticRegression(random_state=42)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state=42,max_depth=4, min_samples_leaf=0.16)\n",
    "classifiers = [('Logistic Regression', lr),\n",
    "                ('K Nearest Neighbours', knn),\n",
    "                ('Classification Tree', dt)]\n",
    "\n",
    "# Instantiate an ensemble VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble_model = VotingClassifier(estimators=classifiers)\n",
    "\n",
    "# Instantiate an ensemble VotingRegressor\n",
    "ensemble_model = VotingRegressor(estimators=regressors)\n",
    "\n",
    "# Instantiate an ensemble BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "ensemble_model = BaggingClassifier(base_estimator=dt, n_estimators=300,oob_score=True, n_jobs=-1)\n",
    "oob_accuracy = bc.oob_score_\n",
    "\n",
    "# Instantiate an ensemble BaggingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "base_regressor = DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)\n",
    "ensemble_model = BaggingRegressor(base_estimator=base_regressor, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "oob_score = ensemble_model.oob_score_\n",
    "\n",
    "# Instantiate an ensemble RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "ensemble_model = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=42)\n",
    "\n",
    "# Instantiate an ensemble RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ensemble_model = RandomForestClassifier(n_estimators=400, random_state=42)\n",
    "\n",
    "# Instantiate an ensemble AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ensemble_model = AdaBoostClassifier(base_estimator=dt, n_estimators=100) # dt is weak, has max depth of 1\n",
    "y_pred_proba = ensemble_model.predict_proba(X_test)[:,1]\n",
    "# Evaluate testing roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Instantiate an ensemble GradientBoostingRegressor, (max_features=0.2, subsample=0.8) makes it stochastic gradient boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "ensemble_model = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=42)\n",
    "\n",
    "# Train using traing set\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "# Predict with test set\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "# Evaluate accuracy for classification\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "# Evaluate RMSE for regression\n",
    "rmse = MSE(y_test, y_pred)**(1/2)\n",
    "# Visualize features importances\n",
    "importances = pd.Series(ensemble_model.feature_importances_, index = X.columns)\n",
    "sorted_importances = importances.sort_values()\n",
    "sorted_importances.plot(kind='barh', color='lightgreen')\n",
    "plt.show()\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "# See what parameters can be tuned\n",
    "ryour_dt_model.get_params()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "params_dt = {\n",
    "    'max_depth': [3, 4,5, 6],\n",
    "    'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "    'max_features': [0.2, 0.4,0.6, 0.8]\n",
    "}\n",
    "mae_scorer = make_scorer(mean_absolute_error)\n",
    "model_cv = GridSearchCV(estimator=your_dt_model,\n",
    "    param_grid=params_dt,\n",
    "    cv=kf, # scorer = mae_scorer\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1)\n",
    "model_cv.fit(X_train, y_train)\n",
    "best_hyperparams = model_cv.best_params_# Get the parameters with best result\n",
    "best_model = model_cv.best_estimator_ # Get the best model\n",
    "best_model.get_params() # See all parameters\n",
    "y_pred = best_model.predict(X_test) # predict with best model\n",
    "best_score = best_model.best_score_\n",
    "model_cv.cv_results_ # See all information from dictionary\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(best_model, 'my_best_model.pkl') # Save the model in pkl file\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
