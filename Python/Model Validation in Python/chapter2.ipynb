{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create one holdout set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your boss has asked you to create a simple random forest model on the `tic_tac_toe` dataset. She doesn't want you to spend much time selecting parameters; rather she wants to know how well the model will perform on future data. For future Tic-Tac-Toe games, it would be nice to know if your model can predict which player will win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top-Left</th>\n",
       "      <th>Top-Middle</th>\n",
       "      <th>Top-Right</th>\n",
       "      <th>Middle-Left</th>\n",
       "      <th>Middle-Middle</th>\n",
       "      <th>Middle-Right</th>\n",
       "      <th>Bottom-Left</th>\n",
       "      <th>Bottom-Middle</th>\n",
       "      <th>Bottom-Right</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Top-Left Top-Middle Top-Right Middle-Left Middle-Middle Middle-Right  \\\n",
       "0        x          x         x           x             o            o   \n",
       "1        x          x         x           x             o            o   \n",
       "2        x          x         x           x             o            o   \n",
       "3        x          x         x           x             o            o   \n",
       "4        x          x         x           x             o            o   \n",
       "\n",
       "  Bottom-Left Bottom-Middle Bottom-Right     Class  \n",
       "0           x             o            o  positive  \n",
       "1           o             x            o  positive  \n",
       "2           o             o            x  positive  \n",
       "3           o             b            b  positive  \n",
       "4           b             o            b  positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tic_tac_toe = pd.read_csv(\"dataset/tic-tac-toe.csv\")\n",
    "tic_tac_toe.head()\n",
    "# tic_tac_toe[\"Top-Right\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tic_tac_toe[\"Class\"].apply(lambda val: 1 if val == \"positive\" else 0)\n",
    "# X = tic_tac_toe.drop(\"Class\", axis=1)\n",
    "# X = pd.get_dummies(X, columns=list(X.columns))\n",
    "# print(X.shape)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Create dummy variables using pandas\n",
    "X = pd.get_dummies(tic_tac_toe.iloc[:,0:9])\n",
    "y = tic_tac_toe.iloc[:, 9]\n",
    "\n",
    "# Create training and testing datasets. Use 10% for the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , random_state=1111,test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create two holdout sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You recently created a simple random forest model to predict Tic-Tac-Toe game wins for your boss, and at her request, you did not do any parameter tuning. Unfortunately, the overall model accuracy was too low for her standards. This time around, she has asked you to focus on model performance.\n",
    "\n",
    "Before you start testing different models and parameter sets, you will need to split the data into training, validation, and testing datasets. Remember that after splitting the data into training and testing datasets, the validation dataset is created by splitting the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary training and final testing datasets\n",
    "X_temp, X_test, y_temp, y_test  = train_test_split(X,y , random_state=1111,test_size=0.2, shuffle=True)\n",
    "\n",
    "# Create the final training and validation datasets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use holdout sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand when you would use three datasets (training, validation, and testing) instead of two (training and testing). There is no point in creating an additional dataset split if you are not going to use it.\n",
    "\n",
    "When should you consider using training, validation, and testing datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When testing parameters, tuning hyper-parameters, or anytime you are frequently evaluating model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communicating modeling results can be difficult. However, most clients understand that on average, a predictive model was off by some number. This makes explaining the mean absolute error easy. For example, when predicting the number of wins for a basketball team, if you predict 42, and they end up with 40, you can easily explain that the error was two wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predictions = np.array([60, 62, 42, 42, 30, 50, 52, 42, 44, 35, 30, 30, 35, 40, 15, 72, 58,\n",
    "       60, 40, 42, 45, 46, 40, 35, 25, 40, 20, 34, 25, 24])\n",
    "y_test = np.array([53, 51, 51, 49, 43, 42, 42, 41, 41, 37, 36, 31, 29, 28, 20, 67, 61,\n",
    "       55, 51, 51, 47, 43, 41, 40, 34, 33, 32, 31, 26, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a manual calculation, the error is 5.9\n",
      "Using scikit-learn, the error is 5.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Manually calculate the MAE\n",
    "n = len(predictions)\n",
    "mae_one = sum(abs(y_test - predictions)) / n\n",
    "print('With a manual calculation, the error is {}'.format(mae_one))\n",
    "\n",
    "# Use scikit-learn to calculate the MAE\n",
    "mae_two = mean_absolute_error(y_test, predictions)\n",
    "print('Using scikit-learn, the error is {}'.format(mae_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the 2017 NBA predictions again. Every year, there are at least a couple of NBA teams that win way more games than expected. If you use the MAE, this accuracy metric does not reflect the bad predictions as much as if you use the MSE. Squaring the large errors from bad predictions will make the accuracy look worse.\n",
    "\n",
    "In this example, NBA executives want to better predict team wins. You will use the mean squared error to calculate the prediction error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a manual calculation, the error is 49.1\n",
      "Using scikit-learn, the error is 49.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n = len(predictions)\n",
    "# Finish the manual calculation of the MSE\n",
    "mse_one = sum((y_test - predictions)**2) / n\n",
    "print('With a manual calculation, the error is {}'.format(mse_one))\n",
    "\n",
    "# Use the scikit-learn function to calculate MSE\n",
    "mse_two = mean_squared_error(y_test, predictions)\n",
    "print('Using scikit-learn, the error is {}'.format(mse_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on data subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In professional basketball, there are two conferences, the East and the West. Coaches and fans often only care about how teams in their own conference will do this year.\n",
    "\n",
    "You have been working on an NBA prediction model and would like to determine if the predictions were better for the East or West conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the East conference teams\n",
    "# east_teams = labels == \"E\"\n",
    "\n",
    "# # Create arrays for the true and predicted values\n",
    "# true_east = y_test[east_teams]\n",
    "# preds_east = predictions[east_teams]\n",
    "\n",
    "# # Print the accuracy metrics\n",
    "# print('The MAE for East teams is {}'.format(\n",
    "#     mae(true_east, preds_east)))\n",
    "\n",
    "# # Print the West accuracy\n",
    "# print('The MAE for West conference is {}'.format(west_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices are a great way to start exploring your model's accuracy. They provide the values needed to calculate a wide range of metrics, including sensitivity, specificity, and the F1-score.\n",
    "\n",
    "You have built a classification model to predict if a person has a broken arm based on an X-ray image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall accuracy is  0.86\n",
      "The precision is  0.97\n",
      "The recall is  0.80\n"
     ]
    }
   ],
   "source": [
    "TN_00 = 324\n",
    "FN_10 = 123\n",
    "FP_01 = 15\n",
    "TP_11  = 491\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = (TP_11 + TN_00) / (953)\n",
    "print(\"The overall accuracy is {0: 0.2f}\".format(accuracy))\n",
    "\n",
    "# Calculate and print the precision\n",
    "precision = (TP_11) / (TP_11 + FP_01)\n",
    "print(\"The precision is {0: 0.2f}\".format(precision))\n",
    "\n",
    "# Calculate and print the recall\n",
    "recall = (TP_11) / (TP_11 + FN_10)\n",
    "print(\"The recall is {0: 0.2f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrices, again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a confusion matrix in Python is simple. The biggest challenge will be making sure you understand the orientation of the matrix. This exercise makes sure you understand the sklearn implementation of confusion matrices. Here, you have created a random forest model using the `tic_tac_toe` dataset `rfc` to predict outcomes of 0 (loss) or 1 (a win) for Player One."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(958, 27)\n",
      "(958,)\n"
     ]
    }
   ],
   "source": [
    "y = tic_tac_toe[\"Class\"].apply(lambda val: 1 if val == \"positive\" else 0)\n",
    "X = tic_tac_toe.drop(\"Class\", axis=1)\n",
    "X = pd.get_dummies(X, columns=list(X.columns))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , random_state=104,test_size=0.25, shuffle=True)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "# Fit the rfc model. \n",
    "rfc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 76   3]\n",
      " [  3 158]]\n",
      "The number of true positives is: 158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create predictions\n",
    "test_predictions = rfc.predict(X_test)\n",
    "\n",
    "# Create and print the confusion matrix\n",
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "print(cm)\n",
    "\n",
    "# Print the true positives (actual 1s that were predicted 1s)\n",
    "print(\"The number of true positives is: {}\".format(cm[1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision vs. recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metrics you use to evaluate your model should always be based on the specific application. For this example, let's assume you are a really sore loser when it comes to playing Tic-Tac-Toe, but only when you are certain that you are going to win.\n",
    "\n",
    "Choose the most appropriate accuracy metric, either precision or recall, to complete this example. But remember, if you think you are going to win, you better win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision value is 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "test_predictions = rfc.predict(X_test)\n",
    "\n",
    "# Create precision or recall score based on the metric you imported\n",
    "score = precision_score(y_test, test_predictions)\n",
    "\n",
    "# Print the final result\n",
    "print(\"The precision value is {0:.2f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error due to under/over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The candy dataset is prime for overfitting. With only 85 observations, if you use 20% for the testing dataset, you are losing a lot of vital data that could be used for modeling. Imagine the scenario where most of the chocolate candies ended up in the training data and very few in the holdout sample. Our model might only see that chocolate is a vital factor, but fail to find that other attributes are also important. In this exercise, you'll explore how using too many features (columns) in a random forest model can lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is 0.09\n",
      "The testing error is 0.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Update the rfr model\n",
    "rfr = RandomForestRegressor(n_estimators=25,\n",
    "                            random_state=1111,\n",
    "                            max_features=2)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and testing accuracies \n",
    "print('The training error is {0:.2f}'.format(\n",
    "  mae(y_train, rfr.predict(X_train))))\n",
    "print('The testing error is {0:.2f}'.format(\n",
    "  mae(y_test, rfr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is 0.05\n",
      "The testing error is 0.13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Update the rfr model\n",
    "rfr = RandomForestRegressor(n_estimators=25,\n",
    "                            random_state=1111,\n",
    "                            max_features=11)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and testing accuracies \n",
    "print('The training error is {0:.2f}'.format(\n",
    "  mae(y_train, rfr.predict(X_train))))\n",
    "print('The testing error is {0:.2f}'.format(\n",
    "  mae(y_test, rfr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is 0.08\n",
      "The testing error is 0.21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Update the rfr model\n",
    "rfr = RandomForestRegressor(n_estimators=25,\n",
    "                            random_state=1111,\n",
    "                            max_features=4)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and testing accuracies \n",
    "print('The training error is {0:.2f}'.format(\n",
    "  mae(y_train, rfr.predict(X_train))))\n",
    "print('The testing error is {0:.2f}'.format(\n",
    "  mae(y_test, rfr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Am I underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are creating a random forest model to predict if you will win a future game of Tic-Tac-Toe. Using the `tic_tac_toe` dataset, you have created training and testing datasets, `X_train`, `X_test`, `y_train`, and `y_test`.\n",
    "\n",
    "You have decided to create a bunch of random forest models with varying amounts of trees (1, 2, 3, 4, 5, 10, 20, and 50). The more trees you use, the longer your random forest model will take to run. However, if you don't use enough trees, you risk underfitting. You have created a for loop to test your model at the different number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training scores were: [0.89, 0.92, 0.98, 0.98, 0.99, 1.0, 1.0, 1.0]\n",
      "The testing scores were: [0.77, 0.8, 0.89, 0.88, 0.88, 0.9, 0.95, 0.98]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_scores, train_scores = [], []\n",
    "for i in [1, 2, 3, 4, 5, 10, 20, 50]:\n",
    "    rfc = RandomForestClassifier(n_estimators=i, random_state=1111)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    # Create predictions for the X_train and X_test datasets.\n",
    "    train_predictions = rfc.predict(X_train)\n",
    "    test_predictions = rfc.predict(X_test)\n",
    "    # Append the accuracy score for the test and train predictions.\n",
    "    train_scores.append(round(accuracy_score(y_train, train_predictions), 2))\n",
    "    test_scores.append(round(accuracy_score(y_test, test_predictions), 2))\n",
    "# Print the train and test scores.\n",
    "print(\"The training scores were: {}\".format(train_scores))\n",
    "print(\"The testing scores were: {}\".format(test_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
