{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correlation = root of r-squared\n",
    "- Correlation between time series:\n",
    "    - if 2 different stocks are trending, their correlation is high even if they do not show same pattern\n",
    "    - Correct way : find correlation between the stock returns instead (eg: correlation between daily percentage change of two stocks)\n",
    "- Predicting future points using regression : dependent time series = independent time series * slope + intercept + error\n",
    "- Auto-correlation:\n",
    "    - correlation between a time series with a lagged version of itself\n",
    "    - an \"echo\" that exists in all points in a time series with other points in the past\n",
    "    - eg: 1,2,3,4,5,6,7 in this series second number = first number + 1, third number = second number + 1.. this exist for all points\n",
    "    - Negative autocorrelation = mean reverting\n",
    "        - stocks have historically negative autocorrelation over weeks\n",
    "        - strategy to make money : buy down -> sell up\n",
    "    - Positive autocorrrelation = momentum\n",
    "        - commodities and currencies have historically positive autocorrelation over months\n",
    "        - strategy to make money : buy up -> sell down\n",
    "    - An autocorrelation graph \n",
    "        - shows how many past points (lags) can we use to predict the future (including the present point).\n",
    "        - Shows suitable model for prediction\n",
    "- White Noise\n",
    "    - constant mean over time\n",
    "    - constant variance over time\n",
    "    - 0 autocorrelation at all lags\n",
    "    - Gaussian White Noise : the white noise has gaussian distribution and show bell curve\n",
    "- Random Walk and White noise\n",
    "    - Stock market follow a random walk, and so the return (gain or percent change) is white noise (Yesterday price - Today price = noise)\n",
    "    - You cannot forecast a random walk. The best guess : todays price is same as yesterdays price\n",
    "    - random walk with drift = random walk + mean (drift)\n",
    "    - So, although we cannot forecast a random walk, we can guess the direction of the walk with the value of drift\n",
    "    - How do we make sure if a series is rendom walk?\n",
    "        - Dickey Fuller Test : You can test if a series is random walk\n",
    "        - Augmented Dickey Fuller Test : Test if a series is random walk with more than one lags through augmentation\n",
    "- Stationarity\n",
    "    - Strong stationarity : Entire distribution of data is time invariant\n",
    "    - Weak stationarity : mean, variance and autocorrelation of data are time invariant\n",
    "    - stationary data is easy to model due to less number of parameters\n",
    "    - non-stationary data is hard to model due to large number of parameters (new parameters found for each point in time)\n",
    "    - eg: stock price (random walk) is non-stationary. reason : price of today will differ from price of 10 years into the future\n",
    "    - eg: white noise is stationary. reason : mean, variance and auto-correlation of 100 data is same as 1000 data points\n",
    "    - non-stationary to stationary : may require several transformations like:\n",
    "        1. log transformation\n",
    "        2. take the difference between current and a lagged version of itself (the right lag = look at acf graph)\n",
    "    - Regression model:\n",
    "        1. AR model : \n",
    "            - Theory : The next value should retain some information from the previous VALUE\n",
    "            - todays value = mean + co-efficient * yesterday's value + noise(y = mx + c)\n",
    "            - co-efficient = phi. Negative phi = mean reversion, positive phi = momentum\n",
    "            - -1 < phi < +1 for stationary series\n",
    "            - phi = 1 for random walk (high autocorrelation) , phi = 0 for white noise (no autocorrelation) , \n",
    "            - autocorrelation decays exponentially at a rate of phi\n",
    "        1. MA model : \n",
    "            - Theory: The next value should retain some information from the previous NOISE\n",
    "            - todays value = mean + co-efficient * yesterday's noise + today's noise (y = mx + c)\n",
    "            - co-efficient = theta. Negative theta = mean reversion, positive theta = momentum\n",
    "            - stationary for all values of co-efficient, theta\n",
    "            - theta = 1 for random walk (high autocorrelation) , theta = 0 for white noise (no autocorrelation) , \n",
    "            - autocorrelation decays exponentially at a rate of phi\n",
    "\n",
    "- Partial auto-correlation : \n",
    "    - incremental benefit of adding another lag\n",
    "    - quantifies how significance adding n-th lag is when there is already (n-1)th lag\n",
    "- Information Criteria : adjusts penalties on number of parameters in the model. The best model has least AIC or/and BIC model among the peers.\n",
    "- Cointegration model: \n",
    "    - two series can be random walk, however the distance between them may be mean reverting. \n",
    "    - Check P-cQ with dicky fuller to see if that is random walk\n",
    "    - eg: Owner and his dog with a leash. steps of owner or dog is separately random walk. but the distance between them is mean reverting\n",
    "        - If dog falls too far behind, it gets pulled forward\n",
    "        - If dog gets too far ahead, it gets pulled back\n",
    "    - eg : Natural gas vs heating oil stock movement. or, economic substitutes or competing company stocks.\n",
    "- Modeling flow:\n",
    "    1. See if the series is stationary (dicky fuller test, higher p-value means non-stationary or random walk)\n",
    "    2. take difference / percent change (to make it stationary)\n",
    "    3. compute ACF and PACF\n",
    "    4. Fit few AR, MA and ARMA models\n",
    "    5. compare AIC, BIC to find the best model\n",
    "    6. Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df['num_col'].autocorr() # autocorrelation value\n",
    "# Plot ACF and PACF graph\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "plot_acf(df['num_col'], lags= 20, alpha=0.05) # alpha = 1 - confidence interval\n",
    "plot_pacf(df['num_col'], lags= 20, alpha=0.05)\n",
    "from statsmodels.tsa.stattools import acf\n",
    "acf(df['num_col']) # See acf values\n",
    "# White noise\n",
    "import numpy as np\n",
    "noise = np.random.normal(loc=0, scale=1, size=500)\n",
    "# Dickey Fuller test for random walk\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "adfuller(df['num_col'])\n",
    "# Pure AR Series generation\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "phi = 0.9 # or theta for MA\n",
    "ar_argen, ma_argen = np.array([1, -phi]), np.array([1]) # For pure AR series generation\n",
    "ar_magen, ma_magen = np.array([1]), np.array([1, theta]) # For pure MA series generation\n",
    "AR_object = ArmaProcess(ar_gen, ma_gen)\n",
    "simulated_data = AR_object.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data)\n",
    "# ARIMA modeling \n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "model = ARIMA(data, trend='t', order=(1,0,0)) # (AR, diff, MA), t for linear trend\n",
    "result = model.fit()\n",
    "forecast = result.get_forecast(steps=50) # Forecast next 50 values\n",
    "print(result.summary())\n",
    "print(result.params) # Returns constant meu, and co-efficient phi\n",
    "print(result.aic, result.bic) # AIC and BIC values of the model\n",
    "# Plotting forecast\n",
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(ax=ax)\n",
    "plot_predict(result, start='2012-09-27', end='2012-10-06', alpha=0.05, ax=ax)\n",
    "plt.show()\n",
    "# Visualize best model : AR or MA values on X axis and AIC, BIC on Y axis\n",
    "plt.plot(ar_values, aic_values, label='AIC', marker='o')\n",
    "plt.plot(ar_values, bic_values, label='BIC', marker='o')\n",
    "# Co-integration model (Also check P-cQ with dicky fuller)\n",
    "from statsmodels.tsa.stattools import coint\n",
    "coint(P,Q)\n",
    "# Regress BTC on ETH (y = mx+c)\n",
    "ETH = sm.add_constant(ETH)\n",
    "result = sm.OLS(BTC,ETH).fit()\n",
    "b = result.params[1] \n",
    "adf_stats = adfuller(BTC['Price'] - b*ETH['Price']) # Compute ADF\n",
    "adf_p_stat =  adf_stats[1]\n",
    "### Running rate of return\n",
    "row_gain_series = data.SP500.pct_change() # period return\n",
    "real_val_after_gain = row_gain_series.add(1)\n",
    "cumulative_return = real_val_after_gain.cumprod().sub(1)\n",
    "percentage_cum_gain = cumulative_return.mul(100).\n",
    "### Alternative : Use a function to apply change in each row of the column\n",
    "def cumulative_return_func(row):\n",
    "    return np.prod(row + 1) - 1\n",
    "row_gain_series = time_df[\"close\"].pct_change() # period return\n",
    "cumulative_return = row_gain_series.rolling('30D').apply(cumulative_return_func)\n",
    "percentage_cum_gain = cumulative_return.mul(100).\n",
    "### Weighted index\n",
    "# company_capital = no_shares * stock_price\n",
    "# total_market_capital_worldwide = sum(company_capital)\n",
    "# company_weight = company_capital / total_market_capital_worldwide\n",
    "# company_index = company_weight * company_pct_change\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "# Create date\n",
    "d =  date(2017, 6, 21) # ISO format: YYYY-MM-DD\n",
    "# Create a datetime\n",
    "dt = datetime(year= 2017 , month= 10 , day= 1 , hour= 15 , minute= 23 , second= 25 , microsecond= 500000 )\n",
    "# Change value of existing datetime\n",
    "dt_changed = dt.replace(minute=0, second=0, microsecond=0)\n",
    "# Sort date\n",
    "dates_ordered = sorted(date_list)\n",
    "# Parse datetime\n",
    "dt = datetime.strptime(\"12/30/2017 15:19:13\", \"%m/%d/%Y %H:%M:%S\")\n",
    "d.isoformat() # Express the date in ISO 8601 format\n",
    "print(d.strftime(\"%Y/%m/%d\")) # Print date in Format: YYYY/MM/DD\n",
    "print(dt.strftime(\"%Y-%m-%d %H:%M:%S\")) # Print datetime in specific format\n",
    "##### Date addition / subtraction\n",
    "from datetime import timedelta\n",
    "delta = d2 - d # Subtract two dates\n",
    "delta.days # Elapsed time in days\n",
    "delta.total_seconds() # Elapsed time in seconds\n",
    "td = timedelta(days=29) # Create a 29 day timedelta\n",
    "print(d + td) # Add delta with existing date\n",
    "# timestamp value\n",
    "ts = 1514665153.0\n",
    "# Convert to datetime from timestamp and print\n",
    "print(datetime.fromtimestamp(ts))\n",
    "# Parsing date\n",
    "df = pd.read_csv('filename.csv', parse_dates = ['date_col1', 'date_col2'], index_col='date_col3') # during import\n",
    "df[\"date_col\"] = pd.to_datetime(df[\"date_col\"], format = \"%Y-%m-%d %H:%M:%S\", errors='coerce') # Using pandas format specified\n",
    "df[\"date_col\"] = df[\"date_col\"].dt.strftime(\"%d-%m-%Y\") # Using python library format specified\n",
    "# Extract information\n",
    "df[\"date_col\"].dt.month # Extract month information\n",
    "df[\"date_col\"].dt.day_name() # Extract day name : Sunday, Monday etc\n",
    "df[\"date_col\"].dt.year # Extract year information\n",
    "# Shift dates\n",
    "df[\"date_col\"].shift(periods=1) # Push values 1 row below, first value becomes null. LEAD\n",
    "df[\"date_col\"].shift(periods=-1) # Pull values 1 row above, last value becomes null. LAG\n",
    "# Numeric operations on date column\n",
    "df[\"date_co1l\"].div(df[\"date_col2\"]) # percentage changes between 2 date columns\n",
    "df[\"date_co1l\"].pct_change(periods=3) # percentage change of same date column after 3 shifts\n",
    "df[\"date_co1l\"].diff() # Difference in value between 2 adjacent rows of same column\n",
    "df[\"date_co1l\"].sub(1).mul(100) # subtracting 1 from the column, then multiply 100 with the column\n",
    "# Creating date\n",
    "time_stamp1 = pd.Timestamp(datetime(2017, 1, 1))\n",
    "time_stamp2 = pd.Timestamp('2017-01-01')\n",
    "# Creating period\n",
    "period = pd.Period('2017-01') # default: month-end period\n",
    "period + 2 # period after 2 unit ('2017-03' in this case)\n",
    "period.asfreq('D') # convert to daily\n",
    "period.to_timestamp() # Convert period to timestamp\n",
    "timestamp_1.to_period('M') # Convert timestamp to period\n",
    "# Add missing time values / change frequency (can be alternative to .asfreq)\n",
    "monthly_dates = pd.date_range(start, end, freq=\"M\")\n",
    "monthly = pd.Series(data=df[\"x\"], index=monthly_dates)\n",
    "weekly_dates = pd.date_range(start, end, freq=\"W\")\n",
    "monthly.reindex(weekly_dates)\n",
    "# Create time series\n",
    "t_series = pd.date_range(start='2017-1-1', periods=12, freq='M')\n",
    "df.set_index('date_col', inplace=True) # setting the time series as index of dataframe\n",
    "# Sampling date (Make sure the index of dataframe is time series), try to always use .resample\n",
    "timed_df.resample('DS').asfreq().agg(['mean']) # Down-sampling to day start using .resample and mean aggregation\n",
    "timed_df.resample('1H').interpolate(method='linear') # Up-sampling with .resample, and fill missing values linearly\n",
    "timed_df.asfreq('1H', method='ffill') # Up-sampling with .asfreq and fill missing values with forward fill\n",
    "timed_df.asfreq(freq='3H', method='linear') # Down-sampling (less values, aggregated values, linearly interpolated)\n",
    "timed_df.resample('M', on = 'date_col')['col1'].mean() # Standard syntax\n",
    "resampled_df.size() # Resampling count\n",
    "# Normalization and comparison of time series data\n",
    "first_row = time_df.iloc[0]\n",
    "normalized = time_df.div(first_row).mul(100)\n",
    "comparison_df = normalized.sub(df['normalized_benchmark_series'], axis=0)\n",
    "# Add timezone in a datetime column\n",
    "df['date_col'] = df['date_col'].dt.tz_localize('America/New_York', ambiguous='NaT')\n",
    "# Convert to another timezone\n",
    "df['date_col'] = df['date_col'].dt.tz_convert('Europe/London')\n",
    "# Window functions:\n",
    "time_df.rolling(window='30D').agg(['mean', 'std']) # moving range / rolling window\n",
    "time_df.expanding().agg(['mean', 'sum']) # expanding range / cumulative expanding window\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between values vs Correlation between percent changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src=\"images/01.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive and Negative Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01.02.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/01.03.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.02.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White Noise : A perfect example of stationary time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.03.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk and Dicky-Fuller Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.04.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.05.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-stationary time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.06.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.07.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.08.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation : non-stationary to stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.09.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.10.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.11.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR series with different phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.01.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of phi on Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.02.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR model with multiple lags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.03.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PACF of AR with different lags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.04.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA with multiple lags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/04.02.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of theta on Autocorrelation of MA(1) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/04.01.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR model can be converted to MA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/04.04.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA Model : The combination of both AR and MA with the integration of difference (percent change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/04.03.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
