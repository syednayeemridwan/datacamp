{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll explore the effect of L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and validaton errors initialized as empty list\n",
    "# train_errs = list()\n",
    "# valid_errs = list()\n",
    "\n",
    "# # Loop over values of C_value\n",
    "# for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "#     # Create LogisticRegression object and fit\n",
    "#     lr = LogisticRegression(C=C_value)\n",
    "#     lr.fit(X_train, y_train)\n",
    "    \n",
    "#     # Evaluate error rates and append to lists\n",
    "#     train_errs.append( 1.0 - lr.score(X_train, y_train) )\n",
    "#     valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )\n",
    "    \n",
    "# # Plot results\n",
    "# plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
    "# plt.legend((\"train\", \"validation\"))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll perform feature selection on the movie review sentiment data set using L1 regularization. The features and targets are already loaded for you in `X_train` and `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# import numpy as np\n",
    "# # Specify L1 regularization\n",
    "# lr = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "\n",
    "# # Instantiate the GridSearchCV object and run the search\n",
    "# searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "# searcher.fit(X_train, y_train)\n",
    "\n",
    "# # Report the best parameters\n",
    "# print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# # Find the number of nonzero coefficients (selected features)\n",
    "# best_lr = searcher.best_estimator_\n",
    "# coefs = best_lr.coef_\n",
    "# print(\"Total number of features:\", coefs.size)\n",
    "# print(\"Number of selected features:\", np.count_nonzero(coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the most positive and negative words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we'll try to interpret the coefficients of a logistic regression fit on the movie review sentiment dataset. The model object is already instantiated and fit for you in the variable `lr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the indices of the sorted cofficients\n",
    "# inds_ascending = np.argsort(lr.coef_.flatten()) \n",
    "# inds_descending = inds_ascending[::-1]\n",
    "\n",
    "# # Print the most positive words\n",
    "# print(\"Most positive words: \", end=\"\")\n",
    "# for i in range(5):\n",
    "#     print(vocab[inds_descending][i], end=\", \")\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Print most negative words\n",
    "# print(\"Most negative words: \", end=\"\")\n",
    "# for i in range(5):\n",
    "#     print(vocab[inds_ascending][i], end=\", \")\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following transformations would make sense for transforming the raw model output of a linear classifier into a class probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.06.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will observe the effects of changing the regularization strength on the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the regularization strength\n",
    "# model = LogisticRegression(C=1)\n",
    "\n",
    "# # Fit and plot\n",
    "# model.fit(X,y)\n",
    "# plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# # Predict probabilities on training points\n",
    "# prob = model.predict_proba(X)\n",
    "# print(\"Maximum predicted probability\", prob[:,0].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.07.svg\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the regularization strength\n",
    "# model = LogisticRegression(C=0.1)\n",
    "\n",
    "# # Fit and plot\n",
    "# model.fit(X,y)\n",
    "# plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# # Predict probabilities on training points\n",
    "# prob = model.predict_proba(X)\n",
    "# print(\"Maximum predicted probability\", np.max(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.08.svg\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing easy and difficult examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you'll visualize the examples that the logistic regression model is most and least confident about by looking at the largest and smallest predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "# lr.fit(X,y)\n",
    "\n",
    "# # Get predicted probabilities\n",
    "# proba = lr.predict_proba(X)\n",
    "\n",
    "# # Sort the example indices by their maximum probability\n",
    "# proba_inds = np.argsort(np.max(proba,axis=1))\n",
    "\n",
    "# # Show the most confident (least ambiguous) digit\n",
    "# show_digit(proba_inds[-1], lr)\n",
    "\n",
    "# # Show the least confident (most ambiguous) digit\n",
    "# show_digit(proba_inds[0], lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you fit a logistic regression model on a classification problem with 3 classes and 100 features, how many coefficients would you have, including intercepts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting multi-class logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you'll fit the two types of multi-class logistic regression, one-vs-rest and softmax/multinomial, on the handwritten digits data set and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit one-vs-rest logistic regression classifier\n",
    "# lr_ovr = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42)\n",
    "# lr_ovr.fit(X_train, y_train)\n",
    "\n",
    "# print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "# print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
    "\n",
    "# # Fit softmax classifier\n",
    "# lr_mn = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)\n",
    "# lr_mn.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "# print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing multi-class logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we'll continue with the two types of multi-class logistic regression, but on a toy 2D data set specifically designed to break the one-vs-rest scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print training accuracies\n",
    "# print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "# print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "\n",
    "# # Create the binary classifier (class 1 vs. rest)\n",
    "# lr_class_1 = LogisticRegression(multi_class='ovr', solver='liblinear', C=100)\n",
    "# lr_class_1.fit(X_train, y_train==1)\n",
    "\n",
    "# # Plot the binary classifier (class 1 vs. rest)\n",
    "# plot_classifier(X_train, y_train==1, lr_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-vs-rest SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As motivation for the next and final chapter on support vector machines, we'll repeat the previous exercise with a non-linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We'll use SVC instead of LinearSVC from now on\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Create/plot the binary classifier (class 1 vs. rest)\n",
    "# svm_class_1 = SVC()\n",
    "# svm_class_1.fit(X_train, y_train==1)\n",
    "# plot_classifier(X_train, y_train==1, svm_class_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
