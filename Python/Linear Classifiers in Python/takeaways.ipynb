{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# First convert dataset to numpy since sklearn uses numpy\n",
    "y = df['target'].values\n",
    "X = df.drop('target', axis=1).values\n",
    "# Normalize the whole dataset before modeling\n",
    "X = preprocessing\\\n",
    "\t.StandardScaler()\\\n",
    "\t.fit(X)\\\n",
    "\t.transform(X.astype(float))\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "# Initialize and train model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='minkowski')\n",
    "knn.fit(X_train, y_train)\n",
    "# Predict the test set class with the trained model\n",
    "predicted_y = knn.predict(X_test)\n",
    "# Measure probability score of prediction for the test set with the trained model\n",
    "predicted_y_prob = knn.predict_proba(X_test)\n",
    "# Measure accuracy on testing set\n",
    "print(accuracy_score(y_test, predicted_y)*100)\n",
    "# Visualize normal distribution of accuracy for different Ks\n",
    "# Compute the above steps for different K and find mean, std etc\n",
    "plt.plot(range(1,Ks),mean_acc,'g')\n",
    "plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n",
    "plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\n",
    "plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Plot complexity graph with list of train and test accuracies\n",
    "plt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n",
    "plt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Specify independent and dependent features\n",
    "X = np.asarray(df[['A', 'B', 'C', 'D', 'E', 'F', 'G']])\n",
    "y = np.asarray(df['target'])\n",
    "\n",
    "# Preprocess dataset\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "\n",
    "# Split into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Train the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(C=0.01, solver='liblinear')\n",
    "LR.fit(X_train,y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = LR.predict(X_test)\n",
    "\n",
    "# See classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "classification_report(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred, labels=[1,0])\n",
    "\n",
    "# Predicted probability on test set for positive/target class\n",
    "y_pred_prob = LR.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import jaccard_score\n",
    "jaccard_score(y_test, y_pred,pos_label=0)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_pred_prob)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_test, y_pred_prob))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Method 1\n",
    "from sklearn.svm import LinearSVC\n",
    "# OR from sklearn.svm import SVC \n",
    "# instatiate a scikit-learn SVM model\n",
    "# to indicate the class imbalance at fit time, set class_weight='balanced'\n",
    "# for reproducible output across multiple function calls, set random_state to a given integer value\n",
    "svm = LinearSVC(class_weight='balanced', random_state=42, loss=\"hinge\", fit_intercept=False) \n",
    "# svm = SVC(kernel='linear', gamma=.5, probability=True)  # Another way\n",
    "# train a linear Support Vector Machine model using Scikit-Learn\n",
    "t0 = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "sklearn_time = time.time() - t0\n",
    "\n",
    "# Method 2 : Use snapml library\n",
    "# in contrast to scikit-learn's LinearSVC, Snap ML offers multi-threaded CPU/GPU training of SVMs\n",
    "from snapml import SupportVectorMachine\n",
    "snapml_svm_gpu = SupportVectorMachine(class_weight='balanced', random_state=42, use_gpu=True, fit_intercept=False)\n",
    "snapml_svm_cpu = SupportVectorMachine(class_weight='balanced', random_state=42, n_jobs=4, fit_intercept=False)\n",
    "t0 = time.time()\n",
    "model = snapml_svm_cpu.fit(X_train, y_train)\n",
    "snapml_time = time.time() - t0\n",
    "\n",
    "# Predict\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Get confidence score for probability\n",
    "y_pred_conf = svm.decision_function(X_test)\n",
    "\n",
    "# Evaluate hinge loss\n",
    "hinge_loss(y_test, y_pred_conf)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Terms / Jargons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decision boundary: the surface separating different predicted classes\n",
    "- linear classifier: a classifier that learns linear decision boundaries\n",
    "- linearly separable: a data set can be perfectly explained by a linear classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/01.02.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting, Underfitting, Bias variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting : \n",
    "    - Model also memorises / trains on noise that resides within training data. \n",
    "    - Model performs well when evaluating on training data but does not perform well on unseen data\n",
    "    - High variance is responsible for this error because of also capturing noise.\n",
    "    - Diagnosis: cross-val prediction on test set has high error than prediction on train set\n",
    "    - Possible remedy : Decrease model complexity, gather more data, \n",
    "- Underfitting :\n",
    "    - Model is too simple to catch the pattern, model is not good enough to capture the underlying pattern.\n",
    "    - Model is bad on both training and unseen data\n",
    "    - Model is not flexibple enough to approximate the prediction values\n",
    "    - High bias is responsible for this error\n",
    "    - Diagnosis: cross-val prediction on train and test set are roughly equal but have very high errors that is undesirable\n",
    "    - Possible remedy : Increase model complexity, gather more features, \n",
    "- Bias-Variance trade-off :\n",
    "    - Generalization error = bias^2 + variance + irreducable error (noise)\n",
    "    - bias = error term that tells how on average real value is different from predicted value\n",
    "    - variance = error term that tells how predicted value varies over different training sets\n",
    "    - When model complexity increases, variance increases and bias decreases\n",
    "    - When model complexity decreases, variance decreases and bias increases\n",
    "    - The sweet spot is the minimised generalization error, which gives the optimised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
