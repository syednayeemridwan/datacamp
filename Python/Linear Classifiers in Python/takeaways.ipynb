{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# First convert dataset to numpy since sklearn uses numpy\n",
    "y = df['target'].values\n",
    "X = df.drop('target', axis=1).values\n",
    "# Normalize the whole dataset before modeling\n",
    "X = preprocessing\\\n",
    "\t.StandardScaler()\\\n",
    "\t.fit(X)\\\n",
    "\t.transform(X.astype(float))\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "# Initialize and train model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='minkowski')\n",
    "knn.fit(X_train, y_train)\n",
    "# Predict the test set class with the trained model\n",
    "predicted_y = knn.predict(X_test)\n",
    "# Measure probability score of prediction for the test set with the trained model\n",
    "predicted_y_prob = knn.predict_proba(X_test)\n",
    "# Measure accuracy on testing set\n",
    "print(accuracy_score(y_test, predicted_y)*100)\n",
    "# Visualize normal distribution of accuracy for different Ks\n",
    "# Compute the above steps for different K and find mean, std etc\n",
    "plt.plot(range(1,Ks),mean_acc,'g')\n",
    "plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n",
    "plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\n",
    "plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Plot complexity graph with list of train and test accuracies\n",
    "plt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n",
    "plt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Specify independent and dependent features\n",
    "X = np.asarray(df[['A', 'B', 'C', 'D', 'E', 'F', 'G']])\n",
    "y = np.asarray(df['target'])\n",
    "\n",
    "# Preprocess dataset\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "\n",
    "# Split into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Train the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(C=0.01, solver='liblinear')\n",
    "LR.fit(X_train,y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = LR.predict(X_test)\n",
    "\n",
    "# See classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "classification_report(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred, labels=[1,0])\n",
    "\n",
    "# Predicted probability on test set for positive/target class\n",
    "y_pred_prob = LR.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import jaccard_score\n",
    "jaccard_score(y_test, y_pred,pos_label=0)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_pred_prob)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_test, y_pred_prob))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Method 1\n",
    "from sklearn.svm import LinearSVC\n",
    "# OR from sklearn.svm import SVC \n",
    "# instatiate a scikit-learn SVM model\n",
    "# to indicate the class imbalance at fit time, set class_weight='balanced'\n",
    "# for reproducible output across multiple function calls, set random_state to a given integer value\n",
    "svm = LinearSVC(class_weight='balanced', random_state=42, loss=\"hinge\", fit_intercept=False) \n",
    "# svm = SVC(kernel='linear', gamma=.5, probability=True)  # Another way\n",
    "# train a linear Support Vector Machine model using Scikit-Learn\n",
    "t0 = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "sklearn_time = time.time() - t0\n",
    "\n",
    "# Method 2 : Use snapml library\n",
    "# in contrast to scikit-learn's LinearSVC, Snap ML offers multi-threaded CPU/GPU training of SVMs\n",
    "from snapml import SupportVectorMachine\n",
    "snapml_svm_gpu = SupportVectorMachine(class_weight='balanced', random_state=42, use_gpu=True, fit_intercept=False)\n",
    "snapml_svm_cpu = SupportVectorMachine(class_weight='balanced', random_state=42, n_jobs=4, fit_intercept=False)\n",
    "t0 = time.time()\n",
    "model = snapml_svm_cpu.fit(X_train, y_train)\n",
    "snapml_time = time.time() - t0\n",
    "\n",
    "# Predict\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Get confidence score for probability\n",
    "y_pred_conf = svm.decision_function(X_test)\n",
    "\n",
    "# Evaluate hinge loss\n",
    "hinge_loss(y_test, y_pred_conf)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Terms / Jargons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decision boundary: the surface separating different predicted classes\n",
    "- linear classifier: a classifier that learns linear decision boundaries\n",
    "- linearly separable: a data set can be perfectly explained by a linear classifier\n",
    "- loss function : a function that provides penalty score that determines how poorly the model performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/01.02.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting, Underfitting, Bias variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting : \n",
    "    - Model also memorises / trains on noise that resides within training data. \n",
    "    - Model performs well when evaluating on training data but does not perform well on unseen data\n",
    "    - High variance is responsible for this error because of also capturing noise.\n",
    "    - Diagnosis: cross-val prediction on test set has high error than prediction on train set\n",
    "    - Possible remedy : Decrease model complexity, gather more data, \n",
    "- Underfitting :\n",
    "    - Model is too simple to catch the pattern, model is not good enough to capture the underlying pattern.\n",
    "    - Model is bad on both training and unseen data\n",
    "    - Model is not flexibple enough to approximate the prediction values\n",
    "    - High bias is responsible for this error\n",
    "    - Diagnosis: cross-val prediction on train and test set are roughly equal but have very high errors that is undesirable\n",
    "    - Possible remedy : Increase model complexity, gather more features, \n",
    "- Bias-Variance trade-off :\n",
    "    - Generalization error = bias^2 + variance + irreducable error (noise)\n",
    "    - bias = error term that tells how on average real value is different from predicted value\n",
    "    - variance = error term that tells how predicted value varies over different training sets\n",
    "    - When model complexity increases, variance increases and bias decreases\n",
    "    - When model complexity decreases, variance decreases and bias increases\n",
    "    - The sweet spot is the minimised generalization error, which gives the optimised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dot product of features and co-efficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Under the hood, prediction, y = dot product of co-efficient and X\n",
    "# changing intercept shifts the boundary up or down\n",
    "# changing co-efficient changes the sloper of the boundary\n",
    "y = model.coef_ @ X + model.intercept_ \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression vs Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear regression:\n",
    "    - Finds a line that fits and aligns tightly with the data\n",
    "    - goal: line is the trend, any new value will appear *ON* the line\n",
    "    - Predicts the value itself\n",
    "    - Predicted value is a continuous value that exceeds 0 or 1\n",
    "- Logistic regression\n",
    "    - Finds a line / plane that separates the data by maximizing the distance\n",
    "    - goal : Line is a no-man's land. New value will appear on *EITHER SIDE* of the line\n",
    "    - predicts which class will the value fall in (sigmoid of the value).\n",
    "    - Predicted value is a discrete value that should be between 0 or 1\n",
    "    - construction: https://vitalflux.com/wp-content/uploads/2022/03/logistic-regression-model-3.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why cannot we use Linear regression in Linear classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regression loss : \n",
    "    - loss is higher when it is further away from true target value. \n",
    "    - loss happens both ways (since it is continuous value).\n",
    "    - squared loss curve perfectly captures this behavior\n",
    "    - Goal : Capture the closeness of values to the original continuous value on both side (positive or negative)\n",
    "- logistic loss: \n",
    "    - loss is higher only for incorrect classifications. \n",
    "    - loss happens in one direction (since it is binary classification.)\n",
    "    - squared loss captures only one direction correctly, the other direction mistakes as \"the perfect model also has squared loss, and so the perfect model is the worst model\"\n",
    "    - Goal : Capture the probability of the incorrectly classified values on incorrect side (sign does not matter as long as the dicrete value is an incorrect value. Correct value has 0 loss)\n",
    "    - we need to eliminate the mistaken side by introducing the logistic function, that only takes range from 0 to 1.\n",
    "- Hinge loss:\n",
    "    - Correct prediction has 0 loss\n",
    "    - Incorrect prediction has linear loss\n",
    "- 0-1 loss:\n",
    "    - It counts the number of misclassifications and averages it over the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/02.04.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss : \n",
    "    - Loss is the error of the model\n",
    "    - Loss tells if a specified decision boundary has misclassified any data.\n",
    "- Cost function: \n",
    "    - A way to calculate the total error (loss) of the model.\n",
    "    - Simply, Cost is the sum of loss\n",
    "    - The lower the cost, the better the model.\n",
    "    - Goal of Optimization algorithms is to minimize the cost\n",
    "    - tells us which model is the best fit \n",
    "    - eg: gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization (minimization problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how gradient descent works\n",
    "- We need to find the minimum value for a given function (eg:  loss function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define a cubic function to minimize: z = Ax^3 + By + C\n",
    "def cubic_function(x, A, B, C):\n",
    "    return A * x[0]**3 + B * x[1] + C\n",
    "\n",
    "# Coefficients for the cubic function\n",
    "A_coefficient = 2.0\n",
    "B_coefficient = -3.0\n",
    "C_constant = 5.0\n",
    "\n",
    "# Initial guess\n",
    "initial_guess = [1, 1]\n",
    "\n",
    "# Minimize the cubic function\n",
    "result = minimize(cubic_function, initial_guess, args=(A_coefficient, B_coefficient, C_constant), method='Nelder-Mead')\n",
    "\n",
    "# Print the result\n",
    "print(\"Minimum found at x:\", result.x)\n",
    "print(\"Minimum function value (z):\", result.fun)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log loss and hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Mathematical functions for logistic and hinge losses\n",
    "def log_loss(raw_model_output):\n",
    "   return np.log(1+np.exp(-raw_model_output))\n",
    "def hinge_loss(raw_model_output):\n",
    "   return np.maximum(0,1-raw_model_output)\n",
    "\n",
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-2,2,1000)\n",
    "plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/03.02.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimizes model co-efficients\n",
    "- An extra term that penalizes large values of co-efficients\n",
    "- Adds up with the loss function: Regularized loss = original loss + large co-efficient penalty\n",
    "- Without regularization = maximize training accuracy\n",
    "- How it affects unseen data: compromises large contribution \n",
    "- reduces overfitting\n",
    "- Lasso : \n",
    "    - Linear regression with l1 regularization\n",
    "    - Makes some co-efficients into 0\n",
    "    - Performs feature selection by filtering out features with less contribution\n",
    "- Ridge : \n",
    "    - Linear regression with l2 regularization\n",
    "    - Shrinks co-eficients\n",
    "- example of regularization on logistic regression:\n",
    "    - Logistic curve = probability curve between 0 to 1 of a given target class (for 2-class problem)\n",
    "    - Ratio of co-efficients : slope of the boundary\n",
    "    - Magnitude of co-efficients : confidence of the line\n",
    "    - Without regularization : \n",
    "        - high co-efficient leads to high confidence (over-confidence), \n",
    "        - Over-confidence leads to high probability\n",
    "        - this leads to overfitting\n",
    "    - With regularization : \n",
    "        - low co-efficient leads to low confidence, \n",
    "        - This leads to low probability\n",
    "        - With low probability, we have the standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of multi class logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.05.webp\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/03.04.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/03.03.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One-vs-Rest (OvR) or One-vs-All:\n",
    "    - a two class classifier trained for each class against the rest of the classes.\n",
    "    -  Classifier with the highest confidence is chosen as the final prediction.\n",
    "- One-vs-One (OvO):\n",
    "    - binary classifiers are trained for each pair of combination of classes.\n",
    "    - For n classes, n-1 classifiers are trained with pairs\n",
    "    - Final output is determined by majority of votes amongst the classifiers\n",
    "- Multinomial (Softmax):\n",
    "    - directly trains a single classifier to predict the probabilities of each class.\n",
    "    - uses a \"softmax\" activation function, which converts the raw outputs into class probabilities.\n",
    "    - Calculates hyperplane distance with features and weights for each class. The distance is then converted to probability.\n",
    "    - class with the highest probability is chosen as the final prediction.\n",
    "    - nothing but the generalized argmax of multiple singular logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-vs-One Classifier\n",
    "ovo_classifier = OneVsOneClassifier(LogisticRegression(solver='lbfgs', random_state=42))\n",
    "ovo_classifier.fit(X_train, y_train)\n",
    "ovo_predictions = ovo_classifier.predict(X_test)\n",
    "ovo_accuracy = accuracy_score(y_test, ovo_predictions)\n",
    "print(\"One-vs-One Accuracy:\", ovo_accuracy)\n",
    "\n",
    "# One-vs-Rest Classifier\n",
    "ovr_classifier = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42)\n",
    "ovr_classifier.fit(X_train, y_train)\n",
    "ovr_predictions = ovr_classifier.predict(X_test)\n",
    "ovr_accuracy = accuracy_score(y_test, ovr_predictions)\n",
    "print(\"One-vs-Rest Accuracy:\", ovr_accuracy)\n",
    "\n",
    "# Multinomial Classifier\n",
    "multinomial_classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)\n",
    "multinomial_classifier.fit(X_train, y_train)\n",
    "multinomial_predictions = multinomial_classifier.predict(X_test)\n",
    "multinomial_accuracy = accuracy_score(y_test, multinomial_predictions)\n",
    "print(\"Multinomial Accuracy:\", multinomial_accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not all datapoints are linearly separable on lower dimension\n",
    "- Transform such dataset to a higher dimensional space where it can be linearly separable by a hyperplane\n",
    "- Support vectors: \n",
    "    - examples/data points closest to the hyperplane\n",
    "    - both classified and misclassified datapoints are counted\n",
    "    - If a datapoint is not a support vector, removing it will not affect the model\n",
    "    - Small number of support vectors = fast kernel SVMs\n",
    "- margin : distance from a support vector to decision boundary\n",
    "- best decision boundary has equal distance from all support vectors\n",
    "- The best separable line is the hyperplane has the biggest margin\n",
    "- Measure of closeness : Regularization parameters (hinge loss and l2)\n",
    "- Boundary decision lines : The lines that touches the support vectors / closest the the support vectors\n",
    "- Soft margin SVM : Used when the classes are not separable (Controlled by regularization parameter)\n",
    "- kernel : Sometimes it is difficult to caculate the mapping of transformation. So we use a shortcut called kernel that is computationally less expensive.\n",
    "    - RBF : support vector = Difference between 2 inputs: X and X` \n",
    "    - C hyperparameter = regularization\n",
    "    - gamma hyperparameter = smoothness of the boundary\n",
    "- Stochastic gradient descent (SGD) : \n",
    "    - Similar to SVM, but scales well for large dataset\n",
    "    - how : uses gradient descent to find out the maximised margin among possible margins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/04.01.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/04.02.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Method 1 : SVM (Support Vector Machine)\n",
    "from sklearn.svm import LinearSVC, SVC \n",
    "svm = LinearSVC(class_weight='balanced', random_state=42, loss=\"hinge\", fit_intercept=False) \n",
    "svm_alternative = SVC(kernel='linear', gamma=.5, probability=True) # Alternative 1\n",
    "\n",
    "# in contrast to scikit-learn's LinearSVC, Snap ML offers multi-threaded CPU/GPU training of SVMs\n",
    "from snapml import SupportVectorMachine\n",
    "snapml_svm_cpu = SupportVectorMachine(class_weight='balanced', n_jobs=4, fit_intercept=False) # Alternative 2 \n",
    "snapml_svm_gpu = SupportVectorMachine(class_weight='balanced', use_gpu=True, fit_intercept=False) # Alternative 3\n",
    "\n",
    "# Method 2 : SGD (Stochastic Gradient Descent)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss='hinge', class_weight='balanced', random_state=42, fit_intercept=False)\n",
    "\n",
    "# Measure training time\n",
    "t0 = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "train_time = time.time() - t0\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Get confidence score for probability\n",
    "y_pred_conf = model.decision_function(X_test)\n",
    "\n",
    "# Evaluate hinge loss\n",
    "hinge_loss(y_test, y_pred_conf)\n",
    "\n",
    "# See index of support vectors in X\n",
    "svm.support_\n",
    "# Fetch support vector points from training set (Training with only support will yield same model)\n",
    "X_small = X[svm.support_]\n",
    "y_small = y[svm.support_]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
