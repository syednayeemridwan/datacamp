{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- General purpose data processing engine designed for big data.\n",
    "- Spark is a platform for cluster computing.\n",
    "- Spark lets you spread data and computations over clusters with multiple nodes (each node as a separate computer). \n",
    "- Very large datasets are split into smaller datasets and  each node only works with a small amount of data.\n",
    "- Data processing and computation are performed in parallel over the nodes in the cluster. \n",
    "- However, with greater computing power comes greater complexity.\n",
    "- Can be used for Analytics, Data Integration, Machine learning, Stream Processing.\n",
    "- Master and Worker:\n",
    "    - Master: \n",
    "        - Connected to the rest of the computers in the cluster, which are called worker\n",
    "        - sends the workers data and calculations to run\n",
    "    - Worker: \n",
    "        - They send their results back to the master.\n",
    "- Spark's core data structure is the Resilient Distributed Dataset (RDD)\n",
    "- Instead of RDDs, it is easier to work with Spark DataFrame abstraction built on top of RDDs ( Operations using DataFrames are automatically optimized.)\n",
    "- spark dataframes are immutable, you need to return a new instance after modification \n",
    "- You start working with `SparkSession` or `SparkContext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark K-means example\") \\\n",
    "    .getOrCreate()\n",
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Load CSV file into DataFrame\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "# Perform basic operations or transformations on the DataFrame as needed\n",
    "# For example, you can filter rows, perform aggregations, etc.\n",
    "# Load a spark table in the DataFrame\n",
    "df_new = spark.table(\"table_name\")\n",
    "# Stop SparkSession\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Create a SparkConf object to configure the SparkContext\n",
    "conf = SparkConf().setAppName(\"YourAppName\").setMaster(\"local[*]\")\n",
    "\n",
    "# Create a SparkContext with the configured SparkConf object\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Verify SparkContext\n",
    "print(sc)\n",
    "\n",
    "# Print Spark version\n",
    "print(sc.version)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Add a new result column\n",
    "df = df.withColumn(\"new_col\",df.old_col+10)\n",
    "# Selecting column\n",
    "calculated_col = (df.col1/(df.col2/60)).alias(\"another_col\")\n",
    "df = df.select(\"col1\", \"col2\", \"col3\", calculated_col)\n",
    "df = df.select(df.col1, df.col2, df.col3)\n",
    "df = df.selectExpr(\"col1\", \"col2\", \"col3\", \"col1/(col2/60) as another_col\")\n",
    "\n",
    "# Filtering (Both produces same results)\n",
    "df.filter(\"col_name > 120\").show()\n",
    "df.filter(df.col_name > 120).show()\n",
    "# Chaining filters\n",
    "filterA = df.col1 == \"SEA\"\n",
    "filterB = df.col2 == \"PDX\"\n",
    "result = temp.filter(filterA).filter(filterB)\n",
    "\n",
    "# Group by\n",
    "df.groupBy(\"col_name\").count().show()\n",
    "# Aggregation\n",
    "df.filter(df.col == 'value').groupBy().max(\"another_col\").show()\n",
    "\n",
    "# Drop nulls\n",
    "df = df.na.drop(subset=[\"col_name\"])\n",
    "# Rename column\n",
    "df = df.withColumnRenamed(\"old_col_name\", \"new_col_name\")\n",
    "\n",
    "# Casting / Converting column type\n",
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"col_name\", col(\"col_name\").cast(\"float\"))\n",
    "df = df.withColumn(\"col_name\", df.col_name.cast(\"float\"))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# One-hot encoding\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "string_indexer = StringIndexer(inputCol=\"col\",outputCol=\"string_index\") # categorical vector\n",
    "one_hot_encoder = OneHotEncoder(inputCol=\"string_index\",outputCol=\"onehot_feature\") # One-hot encoding\n",
    "# Make a VectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"col1\", \"air_time\", \"onehot_feature1\", \"onehot_feature2\", \"plane_age\"], outputCol=\"features\")\n",
    "\n",
    "# Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[string_indexer1, one_hot_encoder1, string_indexer2, one_hot_encoder2, vec_assembler])\n",
    "pipeline_model = pipeline.fit(df) # Fit the dataframe\n",
    "transformed_df = pipeline_model.transform(df) # Transform the dataframe\n",
    "# Split the data into training and test sets\n",
    "training, test = transformed_df.randomSplit([.6, .4])\n",
    "\n",
    "# LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Evaluation metric\n",
    "import pyspark.ml.evaluation as evals\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "\n",
    "# Create the parameter grid and Fine tuning\n",
    "import pyspark.ml.tuning as tune\n",
    "grid = tune.ParamGridBuilder()\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "grid = grid.build()\n",
    "\n",
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator)\n",
    "models = cv.fit(training) # Fit the training set\n",
    "best_lr = models.bestModel # Find the best model\n",
    "# best_lr = lr.fit(training)\n",
    "print(best_lr)\n",
    "test_results = best_lr.transform(test) # Use the model to predict the test set\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
