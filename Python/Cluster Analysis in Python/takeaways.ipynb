{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unsupervised learning : Machine learning on unlabeled data\n",
    "- pure pattern discovery with unguided learning\n",
    "- dimension = number of features or columns in dataset\n",
    "- Algorithms :\n",
    "    - clustering: Organize data into similar groups\n",
    "        - k-means : \n",
    "            - Number of groups formed by samples (rows in dataset)\n",
    "            - calculates the mean of each cluster as centroid\n",
    "            - randomly assign centroids and shift the centroids based on mean of nearest samples in specified iterations\n",
    "            - Inertia : how far the samples are from centroids. good clustering has low inertia\n",
    "        - Scatterplot visualization\n",
    "        - t-SNE : \n",
    "            - maps data from high dimension to 2 dimensional space for visualization\n",
    "            - A black box : Does not provide valid interpretation. Only gives insight about cluster numbers.\n",
    "            - Produces different result on different runs\n",
    "        - Hierarchical clustering : \n",
    "            - split into tree of subgroups\n",
    "            - All leaf clusters are sample themselves\n",
    "            - 2 different clusters merge in each step based on distance (linkage condition)\n",
    "                - for n samples, n-1 steps are taken to complete the whole merge process\n",
    "                - complete linkage = when distance between clusters is maximum distance between samples\n",
    "                - single linkage = when distance between clusters is minimum distance between samples\n",
    "            - At final step, there is only one cluster (\"agglomerative\") or the merging is done until the specified no of clusters are created\n",
    "            - The reverse way of doing the same thing by splitting : Divisive clustering\n",
    "            - Computation time increases exponentially with the increase of datapoints, therefore not optimal choice\n",
    "    - Dimension reduction : Reduce redundant features in data in order to produce simpler model\n",
    "        - Identifies less informative features as noisy features\n",
    "        - Pattern Information achieved in a compressed form\n",
    "        - PCA:\n",
    "            - Step 1 decorrelation : \n",
    "                - principal components = the direction or axis where the sample varies the most\n",
    "                - rotates data so that data aligns with axis with respect to principal components\n",
    "                - the mean becomes 0 \n",
    "                - no data loss\n",
    "                - due to rotation, any correlated features become decorrelated\n",
    "            - Step 2  dimension reduction :\n",
    "                - Intrinsic dimension = number of features needed to approximate the dataset\n",
    "        - NMF\n",
    "            - Non-negative matrix factorization\n",
    "            - De-composes samples as sum of parts\n",
    "                - documents : \n",
    "                    - combination of common themes (here components = topics)\n",
    "                    - eg : tf-idf\n",
    "                - images : \n",
    "                    - combination of common patterns (here components = frequent patterns)\n",
    "                    - eg : grayscale image\n",
    "                    - need to flatten from 2D to 1D row in order to feed into NMF\n",
    "                    - later can be reshaped from 1D to 2D to re-construct the image\n",
    "            - works well with both normal and sparse arrays\n",
    "            - Models are interpretable\n",
    "            - All sample features must be non-negative\n",
    "            - sum of (components * feature value) = reconstruction of sample\n",
    "        - TruncatedSVD : PCA alternative that works on sparse dataset where most entries are 0. (eg : \"tf-idf\")\n",
    "- Market basket analysis: Find items that are frequently bought together (eg: pencil and eraser) \n",
    "- Anomaly detection : When data appears outside of normal range (outlier). eg: Suspicious Credit Card Transactions\n",
    "- A list here: https://mlforall.files.wordpress.com/2019/09/machinelearningalgorithms.png\n",
    "- and here: https://www.theinsaneapp.com/wp-content/uploads/2022/02/Machine-Learning-Algorithms-PDF.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Normalize the whole dataset before modeling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler()\\\n",
    "\t.fit(X)\\\n",
    "\t.transform(X.astype(float))\n",
    "# There are other normalization methods available like MinMaxScaler, Z-score etc\n",
    "\n",
    "# Alternative approach\n",
    "from sklearn.preprocessing import normalize\n",
    "X = normalize(X.astype(float), norm=\"l1\")\n",
    "\n",
    "# Alternative approach\n",
    "from sklearn.preprocessing import Normalizer\n",
    "X = Normalizer()\\\n",
    "\t.fit(X)\\\n",
    "\t.transform(X.astype(float))\n",
    "\n",
    "# Alternative approach with scipy : normalize to a standard deviation of 1\n",
    "from scipy.cluster.vq import whiten\n",
    "scaled_data = whiten(data) # Works with multi-dimensional data\n",
    "\n",
    "# Normalizing without library\n",
    "# Feature Scaling\n",
    "df[\"feature_scaled\"] = df[\"col\"]/ (df[\"col\"].max())\n",
    "# Min-max Scaling\n",
    "df[\"minmax_scaled\"] = (df[\"col\"] - df[\"col\"].min()) / (df[\"col\"].max() - df[\"col\"].min())\n",
    "# Z-score\n",
    "df[\"z_scaled\"] = (df[\"col\"] - df[\"col\"].mean()) / df[\"col\"].std()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.01.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "sample = df.drop(\"target\", axis=1)\n",
    "mergings = linkage(sample.values, method='complete', metric='euclidean', optimal_ordering=False)\n",
    "# Visualize\n",
    "dendrogram(mergings, labels=df[\"target].values, leaf_rotation=90, leaf_font_size=6)\n",
    "plt.show()\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "# Take only specified portion of cluster based on distance\n",
    "labels = fcluster(mergings, 15, criterion='distance')\n",
    "print(labels)\n",
    "df['predicted_labels'] = labels\n",
    "# See distribution of samples \n",
    "ct = pd.crosstab(df['predicted_labels'], df['target'])\n",
    "print(ct)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Timing performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import time\n",
    "# Measure execution time\n",
    "start = time.time()\n",
    "### Your code .......\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "# Alternative approach 1 : simulating many runs on a function\n",
    "import timeit\n",
    "time_taken = timeit.timeit(example_function, number=1000)\n",
    "\n",
    "# Alternative approach 2 : simulating many runs on a function\n",
    "%timeit example_function()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
