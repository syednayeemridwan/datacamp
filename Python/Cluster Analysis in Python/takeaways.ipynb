{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unsupervised learning : Machine learning on unlabeled data\n",
    "- pure pattern discovery with unguided learning\n",
    "- dimension = number of features or columns in dataset\n",
    "- Algorithms :\n",
    "    - clustering: Organize data into similar groups\n",
    "        - k-means : \n",
    "            - Number of groups formed by samples (rows in dataset)\n",
    "            - calculates the mean of each cluster as centroid\n",
    "            - randomly assign centroids and shift the centroids based on mean of nearest samples in specified iterations\n",
    "            - Inertia / Distortion : how far the samples are from centroids. good clustering has low inertia or distortion\n",
    "            - Limitation : biased towards equal size clustering, not good for uniform data\n",
    "        - Scatterplot visualization\n",
    "        - t-SNE : \n",
    "            - maps data from high dimension to 2 dimensional space for visualization\n",
    "            - A black box : Does not provide valid interpretation. Only gives insight about cluster numbers.\n",
    "            - Produces different result on different runs\n",
    "        - Hierarchical clustering : \n",
    "            - split into tree of subgroups\n",
    "            - All leaf clusters are sample themselves\n",
    "            - 2 different clusters merge in each step based on distance (linkage condition)\n",
    "                - for n samples, n-1 steps are taken to complete the whole merge process\n",
    "                - complete linkage = when distance between clusters is maximum distance between samples\n",
    "                - single linkage = when distance between clusters is minimum distance between samples\n",
    "            - At final step, there is only one cluster (\"agglomerative\") or the merging is done until the specified no of clusters are created\n",
    "            - The reverse way of doing the same thing by splitting : Divisive clustering\n",
    "            - Limitation : Computation time increases exponentially with the increase of datapoints, therefore not optimal choice\n",
    "    - Dimension reduction : Reduce redundant features in data in order to produce simpler model\n",
    "        - Identifies less informative features as noisy features\n",
    "        - Pattern Information achieved in a compressed form\n",
    "        - PCA:\n",
    "            - Step 1 decorrelation : \n",
    "                - principal components = the direction or axis where the sample varies the most\n",
    "                - rotates data so that data aligns with axis with respect to principal components\n",
    "                - the mean becomes 0 \n",
    "                - no data loss\n",
    "                - due to rotation, any correlated features become decorrelated\n",
    "            - Step 2  dimension reduction :\n",
    "                - Intrinsic dimension = number of features needed to approximate the dataset\n",
    "        - NMF\n",
    "            - Non-negative matrix factorization\n",
    "            - De-composes samples as sum of parts\n",
    "                - documents : \n",
    "                    - combination of common themes (here components = topics)\n",
    "                    - eg : tf-idf\n",
    "                - images : \n",
    "                    - combination of common patterns (here components = frequent patterns)\n",
    "                    - eg : grayscale image\n",
    "                    - need to flatten from 2D to 1D row in order to feed into NMF\n",
    "                    - later can be reshaped from 1D to 2D to re-construct the image\n",
    "            - works well with both normal and sparse arrays\n",
    "            - Models are interpretable\n",
    "            - All sample features must be non-negative\n",
    "            - sum of (components * feature value) = reconstruction of sample\n",
    "        - TruncatedSVD : PCA alternative that works on sparse dataset where most entries are 0. (eg : \"tf-idf\")\n",
    "- Market basket analysis: Find items that are frequently bought together (eg: pencil and eraser) \n",
    "- Anomaly detection : When data appears outside of normal range (outlier). eg: Suspicious Credit Card Transactions\n",
    "- A list here: https://mlforall.files.wordpress.com/2019/09/machinelearningalgorithms.png\n",
    "- and here: https://www.theinsaneapp.com/wp-content/uploads/2022/02/Machine-Learning-Algorithms-PDF.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Normalize the whole dataset before modeling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler()\\\n",
    "\t.fit(X)\\\n",
    "\t.transform(X.astype(float))\n",
    "# There are other normalization methods available like MinMaxScaler, Z-score etc\n",
    "\n",
    "# Alternative approach\n",
    "from sklearn.preprocessing import normalize\n",
    "X = normalize(X.astype(float), norm=\"l1\")\n",
    "\n",
    "# Alternative approach\n",
    "from sklearn.preprocessing import Normalizer\n",
    "X = Normalizer()\\\n",
    "\t.fit(X)\\\n",
    "\t.transform(X.astype(float))\n",
    "\n",
    "# Alternative approach with scipy : normalize to a standard deviation of 1\n",
    "from scipy.cluster.vq import whiten\n",
    "scaled_data = whiten(data) # Works with multi-dimensional data\n",
    "\n",
    "# Normalizing without library\n",
    "# Feature Scaling\n",
    "df[\"feature_scaled\"] = df[\"col\"]/ (df[\"col\"].max())\n",
    "# Min-max Scaling\n",
    "df[\"minmax_scaled\"] = (df[\"col\"] - df[\"col\"].min()) / (df[\"col\"].max() - df[\"col\"].min())\n",
    "# Z-score\n",
    "df[\"z_scaled\"] = (df[\"col\"] - df[\"col\"].mean()) / df[\"col\"].std()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/02.01.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "sample = df.drop(\"target\", axis=1)\n",
    "mergings = linkage(sample.values, method='complete', metric='euclidean', optimal_ordering=False)\n",
    "# Visualize\n",
    "dendrogram(mergings, labels=df[\"target].values, leaf_rotation=90, leaf_font_size=6)\n",
    "plt.show()\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "# Take only specified portion of cluster based on distance\n",
    "labels = fcluster(mergings, 15, criterion='distance')\n",
    "print(labels)\n",
    "df['predicted_labels'] = labels\n",
    "# See distribution of samples \n",
    "ct = pd.crosstab(df['predicted_labels'], df['target'])\n",
    "print(ct)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Timing performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import time\n",
    "# Measure execution time\n",
    "start = time.time()\n",
    "### Your code .......\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "# Alternative approach 1 : simulating many runs on a function\n",
    "import timeit\n",
    "time_taken = timeit.timeit(example_function, number=1000)\n",
    "\n",
    "# Alternative approach 2 : simulating many runs on a function\n",
    "%timeit example_function()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Import necessary libraries\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Make sure the columns you are working with are in \"float\" datatype\n",
    "# Otherwise scipy generates an error while doing kmeans\n",
    "df1['x'] = df1['x'].astype(float)\n",
    "df1['y'] = df1['y'].astype(float)\n",
    "# Step1 : Compute cluster centers (centroids) and the overall mean of squared distance of datapoints from the centers (distorion)\n",
    "centroids, overall_distortion = kmeans(df1[['x', 'y']], 2)\n",
    "\n",
    "# Step 2 : Predict the labels of clusters and the squared distance of datapoints from the centers (distorion)\n",
    "df1['cluster_labels'], distortion_list = vq(df1[['x', 'y']], centroids)\n",
    "\n",
    "# Plot the points with seaborn\n",
    "sns.scatterplot(x='x', y='y', hue='cluster_labels', data=df1)\n",
    "plt.show()\n",
    "\n",
    "# Find best k through elbow plot: k on X-axis and overall_distortion on Y-axis\n",
    "plt.plot(x=k_list, y=overall_distortion_list)\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "def remove_noise(text, stop_words = []):\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = re.sub('[^A-Za-z0-9]+', '', token)\n",
    "        if len(token) > 1 and token.lower() not in stop_words:\n",
    "            # Get lowercase\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Calling the function to remove noise and get cleaned tokens\n",
    "token_list = remove_noise(\"It is lovely weather we are having. I hope the weather continues.\")    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "documents = ['cats say meow', 'dogs say woof', 'dogs chase cats']\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create a TfidfVectorizer: tfidf\n",
    "# maximum and minimum fraction a word should occur in is 20% to 80%,  keep top 50 termms\n",
    "tfidf = TfidfVectorizer(max_df=0.8, min_df=0.2, max_features=50, stop_words=stop_words)\n",
    "# Apply fit_transform to document: csr_mat\n",
    "csr_mat = tfidf.fit_transform(documents)\n",
    "# Print result of toarray() method\n",
    "print(csr_mat.toarray())\n",
    "# Get the words: words\n",
    "words = tfidf.get_feature_names() # tfidf.get_feature_names_out()\n",
    "# Print words\n",
    "print(words)\n",
    "# Create a dataframe from this sparse matrix representation\n",
    "df = pd.DataFrame(data=csr_mat.toarray(), columns=words)\n",
    "# From dataframe to sparse dataframe\n",
    "sparse_df = some_df.sparse.to_coo()\n",
    "# From sparse to dense dataframe\n",
    "dense_df = sparse_df.to_dense()\n",
    "\n",
    "# Clustering on tf-idf\n",
    "num_clusters = 2  # You can adjust this as needed\n",
    "cluster_centers, distortion = kmeans(csr_mat.todense(), num_clusters)\n",
    "df['cluster_labels'], _ = vq(csr_mat.todense(), cluster_centers)\n",
    "# Display top terms for each cluster\n",
    "for i in range(num_clusters):\n",
    "    cluster_df = df[df['cluster_labels'] == i]\n",
    "    top_terms = cluster_df.mean().sort_values(ascending=False).head().index\n",
    "    print(top_terms)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
