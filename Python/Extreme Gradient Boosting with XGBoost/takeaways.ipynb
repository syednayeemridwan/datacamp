{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost under the hood:\n",
    "    - uses an ensemble model that uses many weak base CART learners into a strong learner\n",
    "    - These weak base learners are only slightly better at prediction than pure random chance\n",
    "    - For each learner, the contribution is calculated as weights\n",
    "    - To get the final output, the weighted sum of all weak learners determine the model output. \n",
    "    - (weight1 * model1 + weight2 + model2 + .. = output)\n",
    "    - Objective function : a way to quantify how much the prediction is away from the actual value (Another name of loss function)\n",
    "    - Goal : Use objective function to produce optimal result\n",
    "- Advantages:\n",
    "    - Fast and efficient\n",
    "    - Core algorithm is parallelizable\n",
    "    - Consistently outperforms single-algorithm methods\n",
    "    - State-of-the-art performance in many ML tasks\n",
    "    - Uses CART (Classification and Regression Tree)\n",
    "    - individual decision trees contain data as decision value at leaves, that is why these trees tend to overfit\n",
    "    - XGBoost tree contains real-valued score at leaves which are generalized numeric values than can be used as threshold that can even help for classification\n",
    "    - Can use 2 types of learners:\n",
    "        1. Linear base learners\n",
    "        2. Tree-based base learners (non-linear)\n",
    "- When to use:\n",
    "    - For > 1000 samples\n",
    "    - For < 100 features\n",
    "    - just numeric features or mixture of numeric and categorical features\n",
    "- When not to use\n",
    "    - image processing\n",
    "    - natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "# Simple fit-predict\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123) # Classification\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123) # Regression\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Cross validation (Method 1 : Using the xgboost API, it has cv, train, predict which is unlike fit-predict in sklearn)\n",
    "dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
    "params_clf={\"objective\":\"binary:logistic\",\"max_depth\":4}        # Classification parameters\n",
    "params_reg={\"objective\":\"binary:logistic\",\"booster\":\"gblinear\"}  # Regression parameters with specified base learners\n",
    "# Regularization parameters: \"alpha\" for l1, \"lambda\" for l2, \"gamma\" for penalty weight for splitting on a node according to tree complexity\n",
    "cv_results = xgb.cv(dtrain=dmatrix, params=params_clf_reg, nfold=4, num_boost_round=10, \n",
    "        metrics=\"error\", as_pandas=True, stratified=True, early_stopping_rounds=10, verbose_eval=1)\n",
    "# accuracy_cv = 1 - cv_results['test-error-mean'].iloc[-1]\n",
    "# Train the final model with the best number of boosting rounds\n",
    "best_num_boost_round = len(cv_results)\n",
    "final_model = xgb.train(params = params_clf_reg, dtrain = dmatrix, num_boost_round=best_num_boost_round)\n",
    "# Make predictions on the testing dataset\n",
    "dtest = xgb.DMatrix(X_test) # ,y_test\n",
    "y_pred_prob = final_model.predict(dtest)\n",
    "y_pred_binary = np.round(y_pred_prob)  # Convert probabilities to binary predictions\n",
    "accuracy_final = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "# Cross validation (Method 2 : Using scikit-learn)\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_pred_cv = cross_val_predict(xgb_model, X_test, y=None, cv=cv)\n",
    "accuracy_final = accuracy_score(y_test, y_pred_cv)\n",
    "\n",
    "# GridSearch / RandomizedSearch (HYPERPARAMETER TUNING)\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "param_grid = {'learning_rate': np.arange(0.05,1.05,.05), 'n_estimators': [200], 'subsample': np.arange(0.05,1.05,.05)}\n",
    "gbm = xgb.XGBRegressor()\n",
    "tuning_models = Grid_RandomizedSearchCV(estimator=gbm, param_distributions=param_grid, n_iter=25, \n",
    "        scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "tuning_models.fit(X, y)\n",
    "tuning_models.best_params_ # See the parameters that give the best results\n",
    "# Visualize tree\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin' ### MAKE SURE TO INSTALL GRAPHVIZ AND ADD THE INSTALLATION PATH\n",
    "xgb.plot_tree(xg_model, num_trees=0) # , rankdir=\"LR\" for aligning tree sideways from left to right\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
