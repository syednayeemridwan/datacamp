{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 and Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost under the hood:\n",
    "    - uses an ensemble model that uses many weak base CART learners into a strong learner\n",
    "    - These weak base learners are only slightly better at prediction than pure random chance\n",
    "    - For each learner, the contribution is calculated as weights\n",
    "    - To get the final output, the weighted sum of all weak learners determine the model output. \n",
    "    - (weight1 * model1 + weight2 + model2 + .. = output)\n",
    "    - Objective function : a way to quantify how much the prediction is away from the actual value (Another name of loss function)\n",
    "    - Goal : Use objective function to produce optimal result\n",
    "- Advantages:\n",
    "    - Fast and efficient\n",
    "    - Core algorithm is parallelizable\n",
    "    - Consistently outperforms single-algorithm methods\n",
    "    - State-of-the-art performance in many ML tasks\n",
    "    - Uses CART (Classification and Regression Tree)\n",
    "    - individual decision trees contain data as decision value at leaves, that is why these trees tend to overfit\n",
    "    - XGBoost tree contains real-valued score at leaves which are generalized numeric values than can be used as threshold that can even help for classification\n",
    "    - Can use 2 types of learners:\n",
    "        1. Linear base learners\n",
    "        2. Tree-based base learners (non-linear)\n",
    "- When to use:\n",
    "    - For > 1000 samples\n",
    "    - For < 100 features\n",
    "    - just numeric features or mixture of numeric and categorical features\n",
    "- When not to use\n",
    "    - image processing\n",
    "    - natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "# Simple fit-predict\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123) # Classification\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123) # Regression\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Cross validation (Method 1 : Using the xgboost API, it has cv, train, predict which is unlike fit-predict in sklearn)\n",
    "dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
    "params_clf={\"objective\":\"binary:logistic\",\"max_depth\":4}        # Classification parameters\n",
    "params_reg={\"objective\":\"binary:logistic\",\"booster\":\"gblinear\"}  # Regression parameters with specified base learners\n",
    "# Regularization parameters: \"alpha\" for l1, \"lambda\" for l2, \"gamma\" for penalty weight for splitting on a node according to tree complexity\n",
    "cv_results = xgb.cv(dtrain=dmatrix, params=params_clf_reg, nfold=4, num_boost_round=10, \n",
    "        metrics=\"error\", as_pandas=True, stratified=True, early_stopping_rounds=10, verbose_eval=1)\n",
    "# accuracy_cv = 1 - cv_results['test-error-mean'].iloc[-1]\n",
    "# Train the final model with the best number of boosting rounds\n",
    "best_num_boost_round = len(cv_results)\n",
    "final_model = xgb.train(params = params_clf_reg, dtrain = dmatrix, num_boost_round=best_num_boost_round)\n",
    "# Make predictions on the testing dataset\n",
    "dtest = xgb.DMatrix(X_test) # ,y_test\n",
    "y_pred_prob = final_model.predict(dtest)\n",
    "y_pred_binary = np.round(y_pred_prob)  # Convert probabilities to binary predictions\n",
    "accuracy_final = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "# Cross validation (Method 2 : Using scikit-learn)\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_pred_cv = cross_val_predict(xgb_model, X_test, y=None, cv=cv)\n",
    "accuracy_final = accuracy_score(y_test, y_pred_cv)\n",
    "\n",
    "# GridSearch / RandomizedSearch (HYPERPARAMETER TUNING)\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "param_grid = {'learning_rate': np.arange(0.05,1.05,.05), 'n_estimators': [200], 'subsample': np.arange(0.05,1.05,.05)}\n",
    "gbm = xgb.XGBRegressor()\n",
    "tuning_models = Grid_RandomizedSearchCV(estimator=gbm, param_distributions=param_grid, n_iter=25, \n",
    "        scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "tuning_models.fit(X, y)\n",
    "tuning_models.best_params_ # See the parameters that give the best results\n",
    "# Visualize tree\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin' ### MAKE SURE TO INSTALL GRAPHVIZ AND ADD THE INSTALLATION PATH\n",
    "xgb.plot_tree(xg_model, num_trees=0) # , rankdir=\"LR\" for aligning tree sideways from left to right\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# splits = kf.split(X) # See how they are splitted, each split contains index of training and validation\n",
    "# r-squared results for 5-fold cross validation score  \n",
    "mae_scorer = make_scorer(mean_absolute_error)\n",
    "scores = cross_val_score(your_model, X, y, cv=kf, scoring=mae_scorer)  # a list of error terms\n",
    "avg_score = np.mean(scores)\n",
    "# predicted_y results for 5-fold cross validation prediction\n",
    "predicted_y = cross_val_predict(your_model, X, y, cv=5) # a list of predictions\n",
    "avg_predicted_y = np.mean(predicted_y)\n",
    "\n",
    "### example of ridge regression with grid search with k-fold cross validation\n",
    "param_grid = {\"alpha\": np.arange(0.0001, 1, 10), \"solver\": [\"sag\", \"lsqr\"]}\n",
    "ridge = Ridge()\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=kf)\n",
    "ridge_cv2 = RandomizedSearchCV(ridge, param_grid, cv=kf, n_iter=2)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "X = df.drop(\"target\", axis = 1).values\n",
    "y = df[\"target\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "\n",
    "# Define sequential stages of your model (Only the last step should contain model, others are transformers)\n",
    "steps = [('scale',StandardScaler()), \n",
    "         ('knn', KNeighborsClassifier())]\n",
    "# Construct the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Perform cross validation on pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "custom_scorer = make_scorer(mean_squared_error)\n",
    "scores = cross_val_score(pipeline,X_train,y_train, scoring= custom_scorer,cv=10) # \"neg_mean_squared_error\"\n",
    "\n",
    "# Perform gridsearch on pipeline\n",
    "parameters = {\"knn__n_neighbors\": np.arange(1, 50)} # Use format: step-name + __ + parameter_name \n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "# Train\n",
    "cv.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "### You can break down the pipeline and add the results of each step in the output\n",
    "# Create a feature union of transformers : allows you to concatenate the results of multiple transformer objects along the second axis\n",
    "combined_features = FeatureUnion([\n",
    "    ('scaler', scaler),\n",
    "    ('poly_features', poly_features),\n",
    "    ('pca', pca)\n",
    "])\n",
    "\n",
    "# Define the classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a pipeline with FeatureUnion and the classifier\n",
    "pipeline = Pipeline([\n",
    "    ('features', combined_features),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Binary Encoding\n",
    "df[\"cat_col\"] = df[\"cat_col\"].apply(lambda val: 1 if val == \"y\" else 0)\n",
    "\n",
    "# One-hot-encoding on categorical variable\n",
    "df_onehot = pd.get_dummies(df, columns=['cat'], prefix='C')\n",
    "df_dummy = pd.get_dummies(df, columns=['cat'], drop_first=True, prefix='C')\n",
    "\n",
    "# Alternative approach-2\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "onehot_transformed = encoder.fit_transform(df['cat_col'].values.reshape(-1,1))\n",
    "# Convert into dataframe\n",
    "onehot_df = pd.DataFrame(onehot_transformed.toarray())\n",
    "# Add the encoded columns with original dataset, \n",
    "df = pd.concat([df, onehot_df], axis=1)\n",
    "# Drop the original column that you used for encoding \n",
    "df = df.drop('cat_col', axis=1)\n",
    "\n",
    "# Label encoding : Turning string labels into numeric values\n",
    "from sklearn import preprocessing\n",
    "encoder_lvl = preprocessing.LabelEncoder()\n",
    "# Specify the unique categories in the column to apply one-hot encoding\n",
    "encoder_lvl.fit([ 'LOW', 'NORMAL', 'HIGH'])\n",
    "# Apply one hot encoding on the third column of the dataset\n",
    "df[:,2] = encoder_lvl.transform(df[:,2]) \n",
    "\n",
    "# Alternative approach : DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "df_dict = df.to_dict(\"records\") # Convert df into a list of dictionary\n",
    "dv = DictVectorizer(sparse = False)\n",
    "df_encoded = dv.fit_transform(df_dict)\n",
    "print(df_encoded[:5,:]) # Print first five rows\n",
    "# Print the vocabulary (how the features are mapped to columns in the resulting matrix.)\n",
    "print(dv.vocabulary_)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
