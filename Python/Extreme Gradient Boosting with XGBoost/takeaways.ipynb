{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost under the hood:\n",
    "    - uses an ensemble model that uses many weak base CART learners into a strong learner\n",
    "    - These weak base learners are only slightly better at prediction than pure random chance\n",
    "    - For each learner, the contribution is calculated as weights\n",
    "    - To get the final output, the weighted sum of all weak learners determine the model output. \n",
    "    - (weight1 * model1 + weight2 + model2 + .. = output)\n",
    "- Advantages:\n",
    "    - Fast and efficient\n",
    "    - Core algorithm is parallelizable\n",
    "    - Consistently outperforms single-algorithm methods\n",
    "    - State-of-the-art performance in many ML tasks\n",
    "    - Uses CART (Classification and Regression Tree)\n",
    "    - individual decision trees contain data as decision value at leaves, that is why these trees tend to overfit\n",
    "    - XGBoost tree contains real-valued score at leaves which are generalized numeric values than can be used as threshold that can even help for classification\n",
    "- When to use:\n",
    "    - For > 1000 samples\n",
    "    - For < 100 features\n",
    "    - just numeric features or mixture of numeric and categorical features\n",
    "- When not to use\n",
    "    - image processing\n",
    "    - natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "# Simple fit-predict\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "xg_cl.fit(X_train, y_train)\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Cross validation (Method 1)\n",
    "dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
    "params={\"objective\":\"binary:logistic\",\"max_depth\":4}\n",
    "cv_results = xgb.cv(dtrain=dmatrix, params=params, nfold=4, num_boost_round=10, \n",
    "        metrics=\"error\", as_pandas=True, stratified=True, early_stopping_rounds=10, verbose_eval=1)\n",
    "# accuracy_cv = 1 - cv_results['test-error-mean'].iloc[-1]\n",
    "# Train the final model with the best number of boosting rounds\n",
    "best_num_boost_round = len(cv_results)\n",
    "final_model = xgb.train(params, dmatrix, num_boost_round=best_num_boost_round)\n",
    "# Make predictions on the testing dataset\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "y_pred_prob = final_model.predict(dtest)\n",
    "y_pred_binary = np.round(y_pred_prob)  # Convert probabilities to binary predictions\n",
    "accuracy_final = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "# Cross validation (Method 2)\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_pred_cv = cross_val_predict(xgb_model, X_test, y=None, cv=cv)\n",
    "accuracy_final = accuracy_score(y_test, y_pred_cv)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
