{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 & Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comes after data cleaning and Exploratory Data Analysis (EDA)\n",
    "- pre-requisite for modeling\n",
    "- Helps to:\n",
    "    - produce more reliable results\n",
    "    - Improve model performance\n",
    "- inspect dataset\n",
    "- See summary statistics\n",
    "- Deal with missing values\n",
    "- Convert to specified column types\n",
    "- Split into training and testing set (Take class imbalance into account)\n",
    "    - Data leakage : non-training data is used to train the model\n",
    "- Standardize data : Transform numeric data to make it normally distributed\n",
    "    - Non-normal data introduce bias for some features due to its high variance \n",
    "    - Non-normal data introduce model underfitting due to difference in scales among different features\n",
    "    - Log-normalization, standard scaling\n",
    "    - Tree-based models can be trained without standardization\n",
    "    - The other models like linear models or dataset with high dimensions requires standardization\n",
    "- Feature Engineering:\n",
    "    - eg : vector of text\n",
    "    - eg : resampling time data (changing time granularity : from second to week, month etc)\n",
    "    - eg : one-hot encoding\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Inspect dataset\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe() # Summary stats\n",
    "\n",
    "# DEAL WITH MISSING VALUES\n",
    "df.drop([1, 2, 3]) # Drop specific rows\n",
    "df.dropna(thresh=2) # keep at least 2 non-missing values in each row\n",
    "df.dropna(subset=['C']) # Drop missing values of specified column\n",
    "\n",
    "# Convert column types\n",
    "df[\"C\"] = df[\"C\"].astype(\"float\")\n",
    "\n",
    "# Verify class imbalance\n",
    "y.value_counts()\n",
    "\n",
    "# Split into training and testing data (Consider class imbalance)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# STANDARDIZE DATASET\n",
    "df.var() # Detect high variance difference in columns are candidates of log normalization\n",
    "\n",
    "# FEATURE ENGINEERING\n",
    "# TEXT PROCESSING (cleaning / vectorizing / regular expression )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Check missing data\n",
    "df.isna().any()\n",
    "df.isna().sum()\n",
    "# Visualize missing data information\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "msno.matrix(df)\n",
    "plt.show()\n",
    "\n",
    "# Drop missing data column\n",
    "df_dropped = df.dropna(subset = ['col'], axis = 1) # 0 for row\n",
    "df.dropna(axis = 0) # Drop entire row for missing value (default)\n",
    "df.dropna(axis = 1) # Drop entire column for missing value\n",
    "\n",
    "# Replace/impute missing data with single value\n",
    "col_mean = df['col'].mean()\n",
    "df_imputed = df.fillna({'col': col_mean})\n",
    "df['col'].replace(to_replace=np.nan, value = some_mean,inplace = True) # Alternative\n",
    "# Replace/impute missing data with series\n",
    "series_imp = df['col1'] * 5\n",
    "df_imputed = df.fillna({'col2':series_imp})\n",
    "\n",
    "df[\"col\"].value_counts() # Look out for suspicious values\n",
    "\n",
    "##### Strategic dropping example ########\n",
    "# Drop missing values where <= 5% of data in column are missing , otherwise impute values\n",
    "threshold = len(df) * 0.05\n",
    "cols_to_drop = df.columns[df.isna().sum() <= threshold]\n",
    "df.dropna(subset=cols_to_drop, inplace=True)\n",
    "cols_with_missing_values = df.columns[salaries.isna().sum() > 0]\n",
    "for col in cols_with_missing_values[:-1]:\n",
    "    df[col].fillna(df[col].mode()[0])\n",
    "subgroup_dict = df.groupby(\"cat_col\")[\"num_col\"].median().to_dict()\n",
    "df[\"num_col\"] = df[\"num_col\"].fillna(df[\"cat_col\"].map(subgroup_dict))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Feature Scaling\n",
    "df[\"feature_scaled\"] = df[\"col\"]/ (df[\"col\"].max())\n",
    "# Min-max Scaling\n",
    "df[\"minmax_scaled\"] = (df[\"col\"] - df[\"col\"].min()) / (df[\"col\"].max() - df[\"col\"].min())\n",
    "# Z-score\n",
    "df[\"z_scaled\"] = (df[\"col\"] - df[\"col\"].mean()) / df[\"col\"].std() \n",
    "\n",
    "# Alternative : Using scikit learn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "log_scaler = PowerTransformer()\n",
    "your_scaler.fit(df[['col']])\n",
    "df['scaled_col'] = your_scaler.transform(df[['col']])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Visualize distribution\n",
    "sns.pairplot(df)\n",
    "df[['column_1']].boxplot()\n",
    "plt.show()\n",
    "\n",
    "# One-hot encoding\n",
    "pd.get_dummies(df, columns=['cat'], prefix='C')\n",
    "# Dummy encoding\n",
    "pd.get_dummies(df, columns=['cat'], drop_first=True, prefix='C')\n",
    "\n",
    "# Merging low frequency categorical counts\n",
    "counts = df['cat'].value_counts()\n",
    "mask = df['cat'].isin(counts[counts < 5].index) \n",
    "df['cat'][mask] = 'Other'\n",
    "\n",
    "# Binarizing numeric variables\n",
    "df['Binary_col'] = 0 \n",
    "df.loc[df['Number_col'] > 0, 'Binary_col'] = 1\n",
    "import numpy as np\n",
    "df['Binned_Group'] = pd.cut( df['Number_col'], bins=[-np.inf, 0, 2, np.inf], labels=[1, 2, 3])\n",
    "\n",
    "# SCALE / STANDARDIZE DATA\n",
    "# DEAL WITH MISSING VALUES.....\n",
    "# DEAL WITH OUTLIERS\n",
    "\n",
    "# Validate numeric columns\n",
    "df['RawSalary'] = df['RawSalary'].str.replace(',', '').astype('float')\n",
    "coerced_vals = pd.to_numeric(df['RawSalary'], errors='coerce')\n",
    "print(df[coerced_vals.isna()].head()) # Sanity check which values still show errors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Binary Encoding\n",
    "df[\"cat_col\"] = df[\"cat_col\"].apply(lambda val: 1 if val == \"y\" else 0)\n",
    "\n",
    "# One-hot-encoding on categorical variable\n",
    "df_onehot = pd.get_dummies(df, columns=['cat'], prefix='C')\n",
    "df_dummy = pd.get_dummies(df, columns=['cat'], drop_first=True, prefix='C')\n",
    "\n",
    "# Alternative approach-2\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "onehot_transformed = encoder.fit_transform(df['cat_col'].values.reshape(-1,1))\n",
    "# Convert into dataframe\n",
    "onehot_df = pd.DataFrame(onehot_transformed.toarray())\n",
    "# Add the encoded columns with original dataset, \n",
    "df = pd.concat([df, onehot_df], axis=1)\n",
    "# Drop the original column that you used for encoding \n",
    "df = df.drop('cat_col', axis=1)\n",
    "\n",
    "# Label encoding : Turning string labels into numeric values\n",
    "from sklearn import preprocessing\n",
    "encoder_lvl = preprocessing.LabelEncoder()\n",
    "# Specify the unique categories in the column to apply one-hot encoding\n",
    "encoder_lvl.fit([ 'LOW', 'NORMAL', 'HIGH'])\n",
    "# Apply one hot encoding on the third column of the dataset\n",
    "df[:,2] = encoder_lvl.transform(df[:,2]) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Remove non-letter characters\n",
    "speech_df['text'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ', regex=True)\n",
    "# Standardize case\n",
    "speech_df['text'] = speech_df['text'].str.lower()\n",
    "# Generate Feature : Average length of word\n",
    "speech_df['char_cnt'] = speech_df['text'].str.len()\n",
    "speech_df['word_cnt'] = speech_df['text'].str.split().apply(len)\n",
    "speech_df['avg_word_len'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "\n",
    "# Generate Feature : tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import stopwords\n",
    "vec = TfidfVectorizer(max_df=0.9, min_df=0.1, max_features=100, stop_words=stop_words) \n",
    "\n",
    "# Generate Feature : Bag of words / Word Count Vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(max_features=100, stop_words='english', min_df=0.1, max_df=0.9)\n",
    "\n",
    "# Generate Feature : Introduce context with n-grams\n",
    "vec = TfidfVectorizer(max_df=0.9, min_df=0.1, max_features=100, stop_words=stop_words, ngram_range = (2,2)) # Find context in 2 consecutive words\n",
    "vec.fit(speech_df['text'])\n",
    "transformed = vec.transform(speech_df['text'])\n",
    "vec_df = pd.DataFrame(transformed.toarray(), columns=vec.get_feature_names_out()).add_prefix('Counts_')\n",
    "\n",
    "# Sanity check : Find common words / patterns\n",
    "vec_df.iloc[0].sort_values(ascending=False).head()\n",
    "vec_df.sum().sort_values(ascending=False).head()\n",
    "\n",
    "speech_df = pd.concat([speech_df, vec_df], axis=1, sort=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
