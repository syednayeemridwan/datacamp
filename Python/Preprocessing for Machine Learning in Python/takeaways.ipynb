{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 & Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comes after data cleaning and Exploratory Data Analysis (EDA)\n",
    "- pre-requisite for modeling\n",
    "- Helps to:\n",
    "    - produce more reliable results\n",
    "    - Improve model performance\n",
    "- inspect dataset\n",
    "- See summary statistics\n",
    "- Deal with missing values\n",
    "- Convert to specified column types\n",
    "- Split into training and testing set (Take class imbalance into account)\n",
    "    - Data leakage : non-training data is used to train the model\n",
    "- Standardize data : Transform numeric data to make it normally distributed\n",
    "    - Non-normal data introduce bias for some features due to its high variance \n",
    "    - Non-normal data introduce model underfitting due to difference in scales among different features\n",
    "    - Log-normalization, standard scaling\n",
    "    - Tree-based models can be trained without standardization\n",
    "    - The other models like linear models or dataset with high dimensions requires standardization\n",
    "- Feature Engineering (Creating new features):\n",
    "    - eg : averaging similar features\n",
    "    - eg : vector of text\n",
    "    - eg : resampling time data (changing time granularity : from second to week, month etc)\n",
    "    - eg : one-hot encoding\n",
    "    - eg: regular expression\n",
    "- Feature Selection \n",
    "    - Remove duplicate features (They add bias into the model)\n",
    "    - Remove features with strong correlation (They add bias into the model)\n",
    "    - Remove noisy features (irrelevant feature)\n",
    "    - dimension reduction (eg: PCA, LDA, RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Inspect dataset\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe() # Summary stats\n",
    "\n",
    "# DEAL WITH MISSING VALUES\n",
    "df.drop([1, 2, 3]) # Drop specific rows\n",
    "df.dropna(thresh=2) # keep at least 2 non-missing values in each row\n",
    "df.dropna(subset=['C']) # Drop missing values of specified column\n",
    "\n",
    "# Convert column types\n",
    "df[\"C\"] = df[\"C\"].astype(\"float\")\n",
    "\n",
    "# Verify class imbalance\n",
    "y.value_counts()\n",
    "\n",
    "# Split into training and testing data (Consider class imbalance)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# STANDARDIZE DATASET\n",
    "df.var() # Detect high variance difference in columns are candidates of log normalization\n",
    "\n",
    "# FEATURE ENGINEERING\n",
    "# TEXT PROCESSING (cleaning / vectorizing / regular expression )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Check missing data\n",
    "df.isna().any()\n",
    "df.isna().sum()\n",
    "# Visualize missing data information\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "msno.matrix(df)\n",
    "plt.show()\n",
    "\n",
    "# Drop missing data column\n",
    "df_dropped = df.dropna(subset = ['col'], axis = 1) # 0 for row\n",
    "df.dropna(axis = 0) # Drop entire row for missing value (default)\n",
    "df.dropna(axis = 1) # Drop entire column for missing value\n",
    "\n",
    "# Replace/impute missing data with single value\n",
    "col_mean = df['col'].mean()\n",
    "df_imputed = df.fillna({'col': col_mean})\n",
    "df['col'].replace(to_replace=np.nan, value = some_mean,inplace = True) # Alternative\n",
    "# Replace/impute missing data with series\n",
    "series_imp = df['col1'] * 5\n",
    "df_imputed = df.fillna({'col2':series_imp})\n",
    "\n",
    "df[\"col\"].value_counts() # Look out for suspicious values\n",
    "\n",
    "##### Strategic dropping example ########\n",
    "# Drop missing values where <= 5% of data in column are missing , otherwise impute values\n",
    "threshold = len(df) * 0.05\n",
    "cols_to_drop = df.columns[df.isna().sum() <= threshold]\n",
    "df.dropna(subset=cols_to_drop, inplace=True)\n",
    "cols_with_missing_values = df.columns[salaries.isna().sum() > 0]\n",
    "for col in cols_with_missing_values[:-1]:\n",
    "    df[col].fillna(df[col].mode()[0])\n",
    "subgroup_dict = df.groupby(\"cat_col\")[\"num_col\"].median().to_dict()\n",
    "df[\"num_col\"] = df[\"num_col\"].fillna(df[\"cat_col\"].map(subgroup_dict))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Feature Scaling\n",
    "df[\"feature_scaled\"] = df[\"col\"]/ (df[\"col\"].max())\n",
    "# Min-max Scaling\n",
    "df[\"minmax_scaled\"] = (df[\"col\"] - df[\"col\"].min()) / (df[\"col\"].max() - df[\"col\"].min())\n",
    "# Z-score\n",
    "df[\"z_scaled\"] = (df[\"col\"] - df[\"col\"].mean()) / df[\"col\"].std() \n",
    "\n",
    "# Alternative : Using scikit learn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "log_scaler = PowerTransformer()\n",
    "your_scaler.fit(df[['col']])\n",
    "df['scaled_col'] = your_scaler.transform(df[['col']])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Visualize distribution\n",
    "sns.pairplot(df)\n",
    "df[['column_1']].boxplot()\n",
    "plt.show()\n",
    "\n",
    "# One-hot encoding\n",
    "pd.get_dummies(df, columns=['cat'], prefix='C')\n",
    "# Dummy encoding\n",
    "pd.get_dummies(df, columns=['cat'], drop_first=True, prefix='C')\n",
    "\n",
    "# Merging low frequency categorical counts\n",
    "counts = df['cat'].value_counts()\n",
    "mask = df['cat'].isin(counts[counts < 5].index) \n",
    "df['cat'][mask] = 'Other'\n",
    "\n",
    "# Binarizing numeric variables\n",
    "df['Binary_col'] = 0 \n",
    "df.loc[df['Number_col'] > 0, 'Binary_col'] = 1\n",
    "import numpy as np\n",
    "df['Binned_Group'] = pd.cut( df['Number_col'], bins=[-np.inf, 0, 2, np.inf], labels=[1, 2, 3])\n",
    "\n",
    "# SCALE / STANDARDIZE DATA\n",
    "# DEAL WITH MISSING VALUES.....\n",
    "# DEAL WITH OUTLIERS\n",
    "\n",
    "# Validate numeric columns\n",
    "df['RawSalary'] = df['RawSalary'].str.replace(',', '').astype('float')\n",
    "coerced_vals = pd.to_numeric(df['RawSalary'], errors='coerce')\n",
    "print(df[coerced_vals.isna()].head()) # Sanity check which values still show errors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Binary Encoding\n",
    "df[\"cat_col\"] = df[\"cat_col\"].apply(lambda val: 1 if val == \"y\" else 0)\n",
    "\n",
    "# One-hot-encoding on categorical variable\n",
    "df_onehot = pd.get_dummies(df, columns=['cat'], prefix='C')\n",
    "df_dummy = pd.get_dummies(df, columns=['cat'], drop_first=True, prefix='C')\n",
    "\n",
    "# Alternative approach-2\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "onehot_transformed = encoder.fit_transform(df['cat_col'].values.reshape(-1,1))\n",
    "# Convert into dataframe\n",
    "onehot_df = pd.DataFrame(onehot_transformed.toarray())\n",
    "# Add the encoded columns with original dataset, \n",
    "df = pd.concat([df, onehot_df], axis=1)\n",
    "# Drop the original column that you used for encoding \n",
    "df = df.drop('cat_col', axis=1)\n",
    "\n",
    "# Label encoding : Turning string labels into numeric values\n",
    "from sklearn import preprocessing\n",
    "encoder_lvl = preprocessing.LabelEncoder()\n",
    "# Specify the unique categories in the column to apply one-hot encoding\n",
    "encoder_lvl.fit([ 'LOW', 'NORMAL', 'HIGH'])\n",
    "# Apply one hot encoding on the third column of the dataset\n",
    "df[:,2] = encoder_lvl.transform(df[:,2]) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Remove non-letter characters\n",
    "speech_df['text'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ', regex=True)\n",
    "# Standardize case\n",
    "speech_df['text'] = speech_df['text'].str.lower()\n",
    "# Generate Feature : Average length of word\n",
    "speech_df['char_cnt'] = speech_df['text'].str.len()\n",
    "speech_df['word_cnt'] = speech_df['text'].str.split().apply(len)\n",
    "speech_df['avg_word_len'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "\n",
    "# Generate Feature : tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import stopwords\n",
    "vec = TfidfVectorizer(max_df=0.9, min_df=0.1, max_features=100, stop_words=stop_words) \n",
    "\n",
    "# Generate Feature : Bag of words / Word Count Vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(max_features=100, stop_words='english', min_df=0.1, max_df=0.9)\n",
    "\n",
    "# Generate Feature : Introduce context with n-grams\n",
    "vec = TfidfVectorizer(max_df=0.9, min_df=0.1, max_features=100, stop_words=stop_words, ngram_range = (2,2)) # Find context in 2 consecutive words\n",
    "vec.fit(speech_df['text'])\n",
    "transformed = vec.transform(speech_df['text'])\n",
    "vec_df = pd.DataFrame(transformed.toarray(), columns=vec.get_feature_names_out()).add_prefix('Counts_')\n",
    "\n",
    "# Sanity check : Find common words / patterns\n",
    "vec_df.iloc[0].sort_values(ascending=False).head()\n",
    "vec_df.sum().sort_values(ascending=False).head()\n",
    "\n",
    "speech_df = pd.concat([speech_df, vec_df], axis=1, sort=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "documents = ['cats say meow', 'dogs say woof', 'dogs chase cats']\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create a TfidfVectorizer: tfidf\n",
    "# maximum and minimum fraction a word should occur in is 20% to 80%,  keep top 50 termms\n",
    "tfidf = TfidfVectorizer(max_df=0.8, min_df=0.2, max_features=50, stop_words=stop_words, ngram_range = (1,1)) # ngram = sequence of words\n",
    "# Apply fit_transform to document: csr_mat\n",
    "csr_mat = tfidf.fit_transform(documents)\n",
    "# Print result of toarray() method\n",
    "print(csr_mat.toarray())\n",
    "# Get the words: words\n",
    "words = tfidf.get_feature_names() # tfidf.get_feature_names_out()\n",
    "# Print words\n",
    "print(words)\n",
    "# Create a dataframe from this sparse matrix representation\n",
    "df = pd.DataFrame(data=csr_mat.toarray(), columns=words)\n",
    "# From dataframe to sparse dataframe\n",
    "sparse_df = some_df.sparse.to_coo()\n",
    "# From sparse to dense dataframe\n",
    "dense_df = sparse_df.to_dense()\n",
    "\n",
    "# see the words and weights in the model\n",
    "tfidf.vocabulary_ # words as keys and location of the words in the text as value\n",
    "tfidf[3].indices # word index no from vocabulary that exist on the 4th row\n",
    "tfidf[3].data # weight of words on the 4th row\n",
    "# Create a dictionary for each row of data where location of the word in text is key and value is weights\n",
    "def return_weights(vocab, vector, index):\n",
    "    zipped = dict(zip(vector[index].indices, vector[index].data))\n",
    "    return {vocab[i]:zipped[i] for i in vector[index].indices}\n",
    "print(return_weights(tfidf.vocabulary_, tfidf, 3))\n",
    "\n",
    "# Clustering on tf-idf\n",
    "num_clusters = 2  # You can adjust this as needed\n",
    "cluster_centers, distortion = kmeans(csr_mat.todense(), num_clusters)\n",
    "df['cluster_labels'], _ = vq(csr_mat.todense(), cluster_centers)\n",
    "# Display top terms for each cluster\n",
    "for i in range(num_clusters):\n",
    "    cluster_df = df[df['cluster_labels'] == i]\n",
    "    top_terms = cluster_df.mean().sort_values(ascending=False).head().index\n",
    "    print(top_terms)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dimension reduction technique\n",
    "- Unsupervised learning method\n",
    "- Combines/decomposes a feature space\n",
    "- Linear transformation to uncorrelated space\n",
    "    - Mean of dataset = center of data, put on 0,0\n",
    "    - The direction of first principal component is the direction of major variance : X-axis\n",
    "    - The direction on next major variance on perpendicular of the last calculated principal component : Y- axis\n",
    "    - So on... \n",
    "    - Thus correlation becomes 0\n",
    "- Captures as much variance as possible in each component (The most significant principal component has the most variance)\n",
    "- Blackbox technique : Difficult to interpret components\n",
    "- Done at the end of preprocessing journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "std_df = scaler.fit_transform(df)\n",
    "pca = PCA()\n",
    "print(pca.fit_transform(std_df))\n",
    "# See which principal components explains the variance more\n",
    "print(pca.explained_variance_ratio_)\n",
    "# See equation of principal components : PC 1 = 0.71 x Feature 1 + 0.71 x Feature 2\n",
    "print(pca.components_)\n",
    "\n",
    "### Alternative approach: Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                ('reducer', PCA(n_components=3)), # n_components=0.9 for capturing 90% variance\n",
    "                ('classifier', RandomForestClassifier())])\n",
    "# principal components\n",
    "pipe['reducer'].components_\n",
    "# No of principal components\n",
    "pipe['reducer'].n_components_\n",
    "# See PCA explained variance\n",
    "pipe['reducer'].explained_variance_ratio_\n",
    "# Visualize elbow plot for PCA tuning\n",
    "plt.plot(pipe['reducer'].explained_variance_ratio_)\n",
    "# Visualize PCA plot\n",
    "pc = pipe['reducer'].fit_transform(df)\n",
    "df['PC 1'] = pc[:,0]\n",
    "df['PC 2'] = pc[:,1]\n",
    "sns.scatterplot(data=df, x='PC 1', y='PC 2', hue='cat_col', alpha=0.4)\n",
    "# Rebuilding back to original Data\n",
    "pc = pipe['reducer'].transform(X)\n",
    "X_rebuilt = pipe['reducer'].inverse_transform(pc)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
