{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which MLlib Module?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark MLlib has many modules and algorithms available for machine learning. Each of them solves a specific subset of problem types. Given the problem we are trying to solve, which module is the correct one for continuous value prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/04.01.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ml.regression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Time Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often with time series, you acquire new data as it is made available and you will want to retrain your model using the newest data. In the video, we showed how to do a percentage split for test and training sets but suppose you wish to train on all available data except for the last 45days which you want to use for a test set.\n",
    "\n",
    "In this exercise, we will create a function to find the split date for using the last 45 days of data for testing and the rest for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------------+--------------------+----------+----------+---------------+----------------+---------------+-------------------+---------+---------------+-----------------+------------+--------------+-----+---------+-----------------+--------------------+------------+-------------------+----------+---------+--------------------+--------------------+----------+------------------+---------------+----+--------------------+---------------+------+----------+---------+------------------+-------+----------+----------+---------+------------------+--------------------+-----+------------------+--------------------+----------------+--------------+---------+------------+----------+----------+---------+---------------------+--------------------+---------+---------+-----------+-----------------+-----+----------+--------------------+---------+----------+---------+----------+---------+----------+---------+----------+--------+---------------+-----------------+--------------+-----------------+-----------------+\n",
      "|No.|MLSID|StreetNumberNumeric|       streetaddress|STREETNAME|PostalCode|StateOrProvince|            City|SalesClosePrice|           LISTDATE|LISTPRICE|       LISTTYPE|OriginalListPrice|PricePerTSFT|FOUNDATIONSIZE|FENCE|MapLetter|LotSizeDimensions|SchoolDistrictNumber|DAYSONMARKET|      offmarketdate|Fireplaces|RoomArea4|            roomtype|                ROOF|RoomFloor4|PotentialShortSale|PoolDescription|PDOM|   GarageDescription|SQFTABOVEGROUND| Taxes|RoomFloor1|RoomArea1|TAXWITHASSESSMENTS|TAXYEAR|LivingArea|UNITNUMBER|YEARBUILT|            ZONING|               STYLE|ACRES|CoolingDescription|          APPLIANCES|backonmarketdate|ROOMFAMILYCHAR|RoomArea3|    EXTERIOR|RoomFloor3|RoomFloor2|RoomArea2|DiningRoomDescription|            BASEMENT|BathsFull|BathsHalf|BATHQUARTER|BATHSTHREEQUARTER|Class|BATHSTOTAL|            BATHDESC|RoomArea5|RoomFloor5|RoomArea6|RoomFloor6|RoomArea7|RoomFloor7|RoomArea8|RoomFloor8|Bedrooms|SQFTBELOWGROUND|AssumableMortgage|AssociationFee|ASSESSMENTPENDING|AssessedValuation|\n",
      "+---+-----+-------------------+--------------------+----------+----------+---------------+----------------+---------------+-------------------+---------+---------------+-----------------+------------+--------------+-----+---------+-----------------+--------------------+------------+-------------------+----------+---------+--------------------+--------------------+----------+------------------+---------------+----+--------------------+---------------+------+----------+---------+------------------+-------+----------+----------+---------+------------------+--------------------+-----+------------------+--------------------+----------------+--------------+---------+------------+----------+----------+---------+---------------------+--------------------+---------+---------+-----------+-----------------+-----+----------+--------------------+---------+----------+---------+----------+---------+----------+---------+----------+--------+---------------+-----------------+--------------+-----------------+-----------------+\n",
      "|  1| RMLS|              11511|11511 Stillwater ...|Stillwater|     55042|             MN|LELM - Lake Elmo|       143000.0|2017-07-15 00:00:00| 139900.0|Exclusive Right|           139900|    145.9184|         980.0|Other|       C4|          279X200|    834 - Stillwater|          10|2017-07-30 00:00:00|         0|  12 x  9|Living Room, Dini...|                NULL|      Main|                No|           NULL|  10|     Attached Garage|          980.0|1858.0|      Main|  16 x 13|              1858|   2017|       980|      NULL|     1950|Residential-Single|      (SF) One Story| 1.28|           Central|Range, Dishwasher...|            NULL|          NULL|     NULL|       Vinyl|      NULL|      Main|   9 x  7|       Eat In Kitchen|                Full|        1|        1|          0|                0|   SF|       2.0|Main Floor 3/4 Ba...|  13 x 11|      Main|  10 x 10|      Main|     NULL|      NULL|     NULL|      NULL|     3.0|            0.0|             NULL|             0|          Unknown|              0.0|\n",
      "|  2| RMLS|              11200|     11200 31st St N|      31st|     55042|             MN|LELM - Lake Elmo|       190000.0|2017-10-09 00:00:00| 210000.0|Exclusive Right|           210000|     85.2783|        1144.0| NULL|       C1|          100x140|    834 - Stillwater|           4|2017-10-13 00:00:00|         0|    11x11|Living Room, Dini...|Asphalt Shingles,...|      Main|                No|           NULL|   4|Attached Garage, ...|         1268.0|1640.0|      Main|    22x14|              1640|   2017|      2228|      NULL|     1971|Residential-Single|(SF) Split Entry ...| 0.32|           Central|Range, Microwave,...|            NULL|   Lower Level|    22x14|       Vinyl|     Lower|      Main|    11x12| Informal Dining R...|Full, Partial Fin...|        1|        0|          0|                2|   SF|       3.0|Main Floor Full B...|    15x11|      Main|    14x11|      Main|    10x11|      Main|    11x11|     Lower|     4.0|          960.0|             NULL|             0|          Unknown|              0.0|\n",
      "|  3| RMLS|               8583|8583 Stillwater B...|Stillwater|     55042|             MN|LELM - Lake Elmo|       225000.0|2017-06-26 00:00:00| 225000.0|Exclusive Right|           225000|    204.1742|        1102.0| None|       E1|          120x296|622 - North St Pa...|          28|2017-07-24 00:00:00|         0|    14x12|Living Room, Dini...|                NULL|      Main|                No|           None|  28|     Attached Garage|         1102.0|2390.0|      Main|    20x13|              2390|   2016|      1102|      NULL|     1949|Residential-Single|      (SF) One Story|0.822|            Window|Range, Microwave,...|            NULL|          NULL|     NULL|Cement Board|      NULL|      Main|    18x11| Informal Dining Room|Full, Crawl Space...|        1|        0|          0|                0|   SF|       1.0|Main Floor Full Bath|    12x12|      Main|    12x12|      Main|     NULL|      NULL|     NULL|      NULL|     2.0|            0.0|    Not Assumable|             0|               No|              0.0|\n",
      "+---+-----+-------------------+--------------------+----------+----------+---------------+----------------+---------------+-------------------+---------+---------------+-----------------+------------+--------------+-----+---------+-----------------+--------------------+------------+-------------------+----------+---------+--------------------+--------------------+----------+------------------+---------------+----+--------------------+---------------+------+----------+---------+------------------+-------+----------+----------+---------+------------------+--------------------+-----+------------------+--------------------+----------------+--------------+---------+------------+----------+----------+---------+---------------------+--------------------+---------+---------+-----------+-----------------+-----+----------+--------------------+---------+----------+---------+----------+---------+----------+---------+----------+--------+---------------+-----------------+--------------+-----------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"dataset/2017_StPaul_MN_Real_Estate.csv\", header=True)\n",
    "df = df.withColumn('ACRES', df.ACRES.cast('double'))\n",
    "df = df.withColumn('SalesClosePrice', df.SalesClosePrice.cast('double'))\n",
    "df = df.withColumn('FOUNDATIONSIZE', df.FOUNDATIONSIZE.cast('double'))\n",
    "df = df.withColumn('LISTPRICE', df.LISTPRICE.cast('double'))\n",
    "df = df.withColumn('AssessedValuation', df.AssessedValuation.cast('double'))\n",
    "df = df.withColumn('Taxes', df.Taxes.cast('double'))\n",
    "df = df.withColumn('Bedrooms', df.Bedrooms.cast('double'))\n",
    "df = df.withColumn('BATHSTOTAL', df.BATHSTOTAL.cast('double'))\n",
    "df = df.withColumn('SQFTBELOWGROUND', df.SQFTBELOWGROUND.cast('double'))\n",
    "df = df.withColumn('SQFTABOVEGROUND', df.SQFTABOVEGROUND.cast('double'))\n",
    "\n",
    "# Import needed functions\n",
    "from pyspark.sql.functions import to_date, dayofweek, to_timestamp,col\n",
    "df = df.withColumn(\"LISTDATE\", to_timestamp(\"LISTDATE\", 'M/d/yyyy H:mm'))\n",
    "df = df.withColumn(\"offmarketdate\", to_timestamp(\"offmarketdate\", 'M/d/yyyy H:mm'))\n",
    "# sorted(df.columns)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "def train_test_split_date(df, split_col, test_days=45):\n",
    "  \"\"\"Calculate the date to split test and training sets\"\"\"\n",
    "  # Find how many days our data spans\n",
    "  max_date = df.agg({split_col: 'max'}).collect()[0][0]\n",
    "  min_date = df.agg({split_col: 'min'}).collect()[0][0]\n",
    "  # Subtract an integer number of days from the last date in dataset\n",
    "  split_date = max_date - timedelta(days=test_days)\n",
    "  return split_date\n",
    "\n",
    "# Find the date to use in spitting test and train\n",
    "split_date = train_test_split_date(df, 'offmarketdate')\n",
    "\n",
    "# Create Sequential Test and Training Sets\n",
    "train_df = df.where(df['offmarketdate'] < split_date) \n",
    "test_df = df.where(df['offmarketdate'] >= split_date).where(df['LISTDATE'] <= split_date) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Time Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have mentioned throughout this course some of the dangers of leaking information to your model during training. Data leakage will cause your model to have very optimistic metrics for accuracy but once real data is run through it the results are often very disappointing.\n",
    "\n",
    "In this exercise, we are going to ensure that DAYSONMARKET only reflects what information we have at the time of predicting the value. I.e., if the house is still on the market, we don't know how many more days it will stay on the market. We need to adjust our test_df to reflect what information we currently have as of 2017-12-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------------+------------+\n",
      "|           LISTDATE|      offmarketdate|DAYSONMARKET_Original|DAYSONMARKET|\n",
      "+-------------------+-------------------+---------------------+------------+\n",
      "|2017-10-06 00:00:00|2018-01-24 00:00:00|                  110|          65|\n",
      "|2017-09-18 00:00:00|2017-12-12 00:00:00|                   82|          83|\n",
      "|2017-11-07 00:00:00|2017-12-12 00:00:00|                   35|          33|\n",
      "|2017-10-30 00:00:00|2017-12-11 00:00:00|                   42|          41|\n",
      "|2017-07-14 00:00:00|2017-12-19 00:00:00|                  158|         149|\n",
      "+-------------------+-------------------+---------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, to_date, lit\n",
    "\n",
    "split_date = to_date(lit('2017-12-10'))\n",
    "# Create Sequential Test set\n",
    "test_df = df.where(df['offmarketdate'] >= split_date).where(df['LISTDATE'] <= split_date)\n",
    "\n",
    "# Create a copy of DAYSONMARKET to review later\n",
    "test_df = test_df.withColumn('DAYSONMARKET_Original', test_df['DAYSONMARKET'])\n",
    "\n",
    "# Recalculate DAYSONMARKET from what we know on our split date\n",
    "test_df = test_df.withColumn('DAYSONMARKET', datediff(split_date, 'LISTDATE'))\n",
    "\n",
    "# Review the difference\n",
    "test_df[['LISTDATE', 'offmarketdate', 'DAYSONMARKET_Original', 'DAYSONMARKET']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering For Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering what steps you'll need to take to preprocess your data before running a machine learning algorithm is important or you could get invalid results. Which of the following preprocessing techniques are needed for Random Forest Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform value replacement for missing values and encode categorical text features to numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Columns with Low Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing a lot of feature engineering it's a good idea to take a step back and look at what you've created. If you've used some automation techniques on your categorical features like exploding or OneHot Encoding you may find that you now have hundreds of new binary features. While the subject of feature selection is material for a whole other course but there are some quick steps you can take to reduce the dimensionality of your data set.\n",
    "\n",
    "In this exercise, we are going to remove columns that have less than 30 observations. 30 is a common minimum number of observations for statistical significance. Any less than that and the relationships cause overfitting because of a sheer coincidence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PotentialShortSale\n",
    "# obs_threshold = 30\n",
    "# cols_to_remove = list()\n",
    "# # Inspect first 10 binary columns in list\n",
    "# for col in binary_cols[0:10]:\n",
    "#   # Count the number of 1 values in the binary column\n",
    "#   obs_count = df.agg({col:'sum'}).collect()[0][0]\n",
    "#   # If less than our observation threshold, remove\n",
    "#   if obs_count < obs_threshold:\n",
    "#     cols_to_remove.append(col)\n",
    "    \n",
    "# # Drop columns and print starting and ending dataframe shapes\n",
    "# new_df = df.drop(*cols_to_remove)\n",
    "\n",
    "# print('Rows: ' + str(df.count()) + ' Columns: ' + str(len(df.columns)))\n",
    "# print('Rows: ' + str(new_df.count()) + ' Columns: ' + str(len(new_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naively Handling Missing and Categorical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regression is robust enough to allow us to ignore many of the more time consuming and tedious data preparation steps. While some implementations of Random Forest handle missing and categorical values automatically, PySpark's does not. The math remains the same however so we can get away with some naive value replacements.\n",
    "\n",
    "For missing values since our data is strictly positive, we will assign -1. The random forest will split on this value and handle it differently than the rest of the values in the same feature.\n",
    "\n",
    "For categorical values, we can just map the text values to numbers and again the random forest will appropriately handle them by splitting on them. In this example, we will dust off pipelines from Introduction to PySpark to write our code more concisely. Please note that the exercise will start by displaying the dtypes of the columns in the dataframe, compare them to the results at the end of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "# # Replace missing values\n",
    "# df = df.fillna(-1, subset=['WALKSCORE' , 'BIKESCORE'])\n",
    "\n",
    "# # Create list of StringIndexers using list comprehension\n",
    "# indexers = [StringIndexer(inputCol=col, outputCol=col+\"_IDX\")\\\n",
    "#             .setHandleInvalid(\"keep\") for col in categorical_cols]\n",
    "# # Create pipeline of indexers\n",
    "# indexer_pipeline = Pipeline(stages=indexers)\n",
    "# # Fit and Transform the pipeline to the original data\n",
    "# df_indexed = indexer_pipeline.fit(df).transform(df)\n",
    "\n",
    "# # Clean up redundant columns\n",
    "# df_indexed = df_indexed.drop(*categorical_cols)\n",
    "# # Inspect data transformations\n",
    "# print(df_indexed.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great things about PySpark ML module is that most algorithms can be tried and tested without changing much code. Random Forest Regression is a fairly simple ensemble model, using bagging to fit. Another tree based ensemble model is Gradient Boosted Trees which uses a different approach called boosting to fit. In this exercise let's train a `GBTRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns\n",
    "features = features.remove('SalesClosePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# # Train a Gradient Boosted Trees (GBT) model.\n",
    "# gbt = GBTRegressor(featuresCol='features',\n",
    "#                            labelCol='SalesClosePrice',\n",
    "#                            predictionCol=\"Prediction_Price\",\n",
    "#                            seed=42\n",
    "#                            )\n",
    "\n",
    "# # Train model.\n",
    "# model = gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating & Comparing Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created a new model with GBTRegressor its time to compare it against our baseline of RandomForestRegressor. To do this we will compare the predictions of both models to the actual data and calculate RMSE and R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# # Select columns to compute test error\n",
    "# evaluator = RegressionEvaluator(labelCol='SalesClosePrice', \n",
    "#                                 predictionCol='Prediction_Price')\n",
    "# # Dictionary of model predictions to loop over\n",
    "# models = {'Gradient Boosted Trees': gbt_predictions, 'Random Forest Regression': rfr_predictions}\n",
    "# for key, preds in models.items():\n",
    "#   # Create evaluation metrics\n",
    "#   rmse = evaluator.evaluate(preds, {evaluator.metricName: \"rmse\"})\n",
    "#   r2 = evaluator.evaluate(preds, {evaluator.metricName: \"r2\"})\n",
    "  \n",
    "#   # Print Model Metrics\n",
    "#   print(key + ' RMSE: ' + str(rmse))\n",
    "#   print(key + ' R^2: ' + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that R^2 and RMSE are both metrics to evaluate the performance of regression models. Both provide a different way to interpret the fit of our model. Which of the following statements is TRUE regarding R^2 or RMSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- RMSE is comparable across predictions looking at the same dependent variable.\n",
    "- R^2 is comparable across predictions regardless of dependent variable.\n",
    "- RMSE is a measure of unexplained variance in the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is almost always important to know which features are influencing your prediction the most. Perhaps its counterintuitive and that's an insight? Perhaps a hand full of features account for most of the accuracy of your model and you don't need to perform time acquiring or massaging other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert feature importances to a pandas column\n",
    "# fi_df = pd.DataFrame(importances, columns=['importances'])\n",
    "\n",
    "# # Convert list of feature names to pandas column\n",
    "# fi_df['feature'] = pd.Series(feature_cols)\n",
    "\n",
    "# # Sort the data based on feature importance\n",
    "# fi_df.sort_values(by=['importances'], ascending=False, inplace=True)\n",
    "\n",
    "# # Inspect Results\n",
    "# fi_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving & Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times you may find yourself going back to a previous model to see what assumptions or settings were used when diagnosing where your prediction errors were coming from. Perhaps there was something wrong with the data? Maybe you need to incorporate a new feature to capture an unusual event that occurred?\n",
    "\n",
    "In this example, you will practice saving and loading a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "# # Save model\n",
    "# model.save('rfr_no_listprice')\n",
    "\n",
    "# # Load model\n",
    "# loaded_model = RandomForestRegressionModel.load('rfr_no_listprice')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
