{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create a pandas dataframe from pyspark dataframe for visualization\n",
    "sample_df = large_df.sample(withReplacement = False, fraction=0.3, seed=42) # randomly sample from pyspark dataframe\n",
    "sample_pandas_df = sample_df.toPandas() # Convert to pandas dataframe\n",
    "sns.histplot(sample_pandas_df[numeric_column], kde=True, color='skyblue') # See distribution plot\n",
    "\n",
    "# Visualization : Pyspark_dist_explore, pandas (NOT RECOMMENDED), HandySpark(RECOMMENDED)\n",
    "pandas_df = spark_df.toPandas()\n",
    "handy_df = spark_df.toHandy() # Convert to handyspark dataframe\n",
    "handy_df.cols[\"col_name\"].hist()\n",
    "spark_df = handy_df.to_spark() # Convert to pyspark dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# No of missing values\n",
    "df.where(df['col_name'].isNull()).count()\n",
    "# Visualise missing values with heatmap\n",
    "pandas_df = spark_df.toPandas()\n",
    "sns.heatmap(data=pandas_df.isnull())\n",
    "# Drop any records with NULL values\n",
    "df = df.dropna()\n",
    "# drop records if both LISTPRICE and SALESCLOSEPRICE are NULL\n",
    "df = df.dropna(how='all', subset['col1', 'col2 '])\n",
    "# Drop records where at least two columns have NULL values\n",
    "df = df.dropna(thresh=2)\n",
    "# Drop columns with >30% missing values\n",
    "df = df.drop(*col_list)\n",
    "# Replace missing values\n",
    "col_mean = df.agg({'col_name': 'mean'}).collect()[0][0]\n",
    "df.fillna(col_mean, subset=['col_name'])\n",
    "# Drop duplicates\n",
    "df.dropDuplicates(['col_name'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "max_val = df.agg({'col_name': 'max'}).collect()[0][0]\n",
    "min_val = df.agg({'col_name': 'min'}).collect()[0][0]\n",
    "mean_val = df.agg({'col_name': 'mean'}).collect()[0][0]\n",
    "std_val = df.agg({'col_name': 'stddev'}).collect()[0][0]\n",
    "df = df.withColumn(\"min_max_scaled_col\", (df['col_name'] - min_val) / (max_val - min_val))\n",
    "df = df.withColumn(\"standard_scaled_col\", (df['col_name'] - mean_val) / std_val)\n",
    "df = df.withColumn(\"feature_scaled_col\", df['col_name']  / max_val)\n",
    "# Log transformation\n",
    "from pyspark.sql.functions import log\n",
    "df = df.withColumn('log_col', log(df['col_name']))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pySpark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create dataframe from RDD\n",
    "spark_df = spark.createDataFrame(RDD, schema=colname_list)\n",
    "\n",
    "# Loading file (folder name will make the spark load all files in that folder in parallel mode)\n",
    "from pyspark.sql.types import *\n",
    "dataSchema = StructType([ StructField('col1', StringType(), , nullable=True),\n",
    "                            StructField('col2', StringType(), , nullable=False)])\n",
    "df = spark.read.csv(\"file.csv\", header=True, schema=dataSchema, comment='#', sep=',') # .json, .txt, .load for parquet\n",
    "df = spark.read.format('csv').options(Header=True).load(name='filename.csv') # schema=dataSchema\n",
    "df.write.parquet('filename.parquet', mode='overwrite') # Save file (parquet is more efficient, binary format for big data)\n",
    "df.write.format('parquet').save('filename.parquet')\n",
    "df.show(3) # Show first 3 rows\n",
    "df.collect() # Store result as list of tuples\n",
    "df.limit(3) # Same as show\n",
    "df.dtypes # See datatype of each column\n",
    "df.printSchema() # See schema information\n",
    "result.columns # See result table columns\n",
    "df.filter(~ col('col').isNull()) # Check for nulls\n",
    "df = df.na.drop(subset=[\"col_name\"]) # Drop nulls\n",
    "df = df.drop(subset=[\"col_name\"]) # Drop column\n",
    "df = df.dropDuplicates() # Drop duplicates\n",
    "df = df.withColumn(\"col_name\", col(\"col_name\").cast(\"float\"))  # Way 1 : Casting a column to another data type\n",
    "df = df.withColumn(\"col_name\", df.col_name.cast(\"float\")) # Way 2 : Casting a column to another data type\n",
    "from pyspark.sql.types import IntegerType # Remember: ArrayType is homogeneous, use StructType([]) for heterogeneity\n",
    "df = df.withColumn('casted_col', df['col_name'].cast(IntegerType())) # Way 3 : Casting \n",
    "df.describe().show() # Summary stats\n",
    "df.agg({'col_name':'max'}).first()[0] # Maximum value of a column\n",
    "df = df.repartition(4, 'some_col') # create 4 partitions using same column values of specified column\n",
    "print(df.rdd.getNumPartitions()) # See no of partitions of the dataset\n",
    "df = df.coalesce(num_partitions) # Reduce the number of partitions = reduce shuffling (distribution of data to the nodes)\n",
    "df_split = df.withColumn(\"DataSplit\", split(df[\"Data\"], \",\")) # Split values and store as list in a column\n",
    "df_explode = df_split.withColumn(\"ExplodedData\", explode(df_split[\"DataSplit\"])) # Store each element of list in separate rows\n",
    "df_pivot = df_explode.groupBy(\"Key\").pivot(\"ExplodedData\").count().fillna(0) # pivot count of each element in separate columns \n",
    "df = df.select(df.col1, df.col2, df.col3) # way1 : select column from dataframe\n",
    "df = df.select(\"col1\", \"col2\") # way2 : select column from dataframe\n",
    "df.select(col('col1'), col('col2')) # way3 : select column from dataframe,  import col from sql.functions\n",
    "df = df.withColumn(\"new_col\",df.old_col+10) # Add a new result column\n",
    "df = df.withColumnRenamed(\"old_col_name\", \"new_col_name\") # Rename column\n",
    "df = df.select(col('col1').alias('col1_renamed'), 'col2')\n",
    "df = df.selectExpr(\"col1\", \"col2\", \"col3\", \"col1/(col2/60) as another_col\")\n",
    "df = df.withColumn(\"idx\", monotonically_increasing_id()) # Creating id column\n",
    "df.where(array_contains('col', 'abc')) # Check if an element is inside an array\n",
    "df1 = df1.withColumn(\"source\", lit(\"df1\")) # Adding constants in a column\n",
    "\n",
    "df_vertical = df1.union(df2) # Vertical join (append rows vertically)\n",
    "df_horizontal = df1.join(df1, on=['common_col1', 'common_col2'], how=\"left\") (append columns horizontally with join)\n",
    "joined_df = df1.join(df2, df1[\"colx\"] == df2[\"coly\"] , how=\"inner\") # Alternative way : Join \n",
    "combined_df = df_1.join(broadcast(df_2)) # Prevents undue / excess communication between nodes by giving a nroadcasted copy to each\n",
    "df_cross = df1.crossJoin(df2) # Cross Join (Horizontally appending columns of possible combinations)\n",
    "\n",
    "# Filtering (Both produces same results)\n",
    "df = df.filter(\"col_name > 120\").show()\n",
    "df = df.where(\"Value > 120\")\n",
    "df = df.filter(df.col_name > 120).show()\n",
    "df = df.where(df.Value > 120)\n",
    "filterA = df.col1 == \"SEA\"\n",
    "result = temp.filter(filterA).filter(filterB) # Chaining filters\n",
    "df.groupBy(\"col_name\").count() # Group by and count\n",
    "df.orderBy(\"col_name\") # order by \n",
    "df.filter(df.col == 'value').groupBy().max(\"another_col\") # Multiple chaining aggregation\n",
    "\n",
    "df.createOrReplaceTempView(\"table_name\") # Register DataFrame as a temporary talbe in catalog\n",
    "spark.catalog.listTables() # See all table information in the catalog\n",
    "spark.catalog.dropTempView('table_name') # Remove temp table from catalog\n",
    "spark_df = spark.table(\"table_name\") # start using a spark table as spark dataframe\n",
    "result = spark.sql(\"SELECT * FROM table_name\") # Run query on table\n",
    "\n",
    "# Using PYSPARK CUSTOM FUNCTION\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "def double_val(col):\n",
    "    return col * 2 # Make sure any new data is casted to proper type\n",
    "double_val_udf = udf(double_val, IntegerType()) # Register UDF with custom function and return type\n",
    "df = df.withColumn(\"DoubledCol\", custom_func(df[\"col\"]))\n",
    "\n",
    "## Visualization : Pyspark_dist_explore, pandas (NOT RECOMMENDED), HandySpark(RECOMMENDED)\n",
    "pandas_df = spark_df.toPandas()\n",
    "handy_df = spark_df.toHandy() # Convert to handyspark dataframe\n",
    "handy_df.cols[\"col_name\"].hist()\n",
    "spark_df = handy_df.to_spark() # Convert to pyspark dataframe\n",
    "\n",
    "## NOTE\n",
    "# Array: [1.0, 0.0, 0.0, 3.0]\n",
    "# Sparse vector: (4, [0, 3], [1.0, 3.0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Binarizing (create column with value to 0 or 1)\n",
    "from pyspark.ml.feature import Binarizer\n",
    "df = df.withColumn('val', df['val'].cast('double'))\n",
    "bin = Binarizer(threshold=0.0, inputCol='val', outputCol='binary_col')\n",
    "df = bin.transform(df)\n",
    "\n",
    "# Bucketing \n",
    "from pyspark.ml.feature import Bucketizer\n",
    "splits = [0, 1, 2, 3, 4, float('Inf')]\n",
    "# Create bucketing transformer\n",
    "buck = Bucketizer(splits=splits, inputCol='BATHSTOTAL', outputCol='baths')\n",
    "# Apply transformer\n",
    "df = buck.transform(df)\n",
    "\n",
    "# One-hot encoding (Can be used with PYSPARK PIPELINE and PYSPARK MACHINE LEARNING model)\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "string_indexer = StringIndexer(inputCol='cat_col', outputCol='Cat_Index') # Map strings to numbers with string indexer\n",
    "indexed_df = string_indexer.fit(df).transform(df)\n",
    "encoder = OneHotEncoder(inputCol='Cat_Index', outputCol='Onehot_feature') # Onehot encode indexed values\n",
    "encoded_df = encoder.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "# Using Pipeline to do many steps at once\n",
    "from pyspark.ml import Pipeline\n",
    "features_cols = list(df.columns) # Check for non-null columns\n",
    "features_cols.remove('some_null_col') # Remove the dependent variable from the list\n",
    "df = df.fillna(-1) # Vector Assembler should not take in any nulls\n",
    "vec_assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"Onehot_feature\"], outputCol=\"features\") # features_cols\n",
    "pipeline = Pipeline(stages=[string_indexer, encoder, vec_assembler]) # Last stage is model: eg : stages=[.., model]\n",
    "pipeline_model = pipeline.fit(df)\n",
    "transformed_df = pipeline_model.transform(df)\n",
    "\n",
    "# SPLIT DATA\n",
    "\n",
    "# Create Model\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"SALESCLOSEPRICE\",\n",
    "                    predictionCol=\"Prediction_Price\", seed=42 )\n",
    "model = rf.fit(train_df) # Train model\n",
    "predictions = model.transform(test_df)\n",
    "model.save('rfr_model') # Save model\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "model2 = RandomForestRegressionModel.load('rfr_model') # Load the model\n",
    "# Evaluate Model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"SALESCLOSEPRICE\", predictionCol=\"Prediction_Price\")\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Feature importance\n",
    "import pandas as pd\n",
    "# Convert feature importances to a pandas column\n",
    "importance_df = pd.DataFrame(model.featureImportances.toArray(), columns=['importance'])\n",
    "importance_df['features'] = pd.Series(feature_cols) # Create a new column to hold feature names\n",
    "importance_df.sort_values(by=['importance'], ascending=False, inplace=True) # Sort the data based on feature importance\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "########### Splitting non-sequence data\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "########### Splitting sequence data with inherent sequence (eg : Time Series)\n",
    "# Find how many days our data spans\n",
    "from pyspark.sql.functions import datediff\n",
    "range_in_days = datediff(max_date, min_date) # Find the no of days beteen minimum and maximum date\n",
    "# Find the date to split the dataset on\n",
    "from pyspark.sql.functions import date_add\n",
    "split_in_days = round(range_in_days * 0.8) # Find 80% date split point\n",
    "split_date = date_add(min_date, split_in_days) # Add split point with minimum date to get the split date\n",
    "# Split the data into 80% train, 20% test\n",
    "train_df = df.where(df['DATE'] < split_date) # Use filtering with split date to take only training data\n",
    "test_df = df.where(df['DATE'] >= split_date) # Use filtering with split date to take only testing data\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
