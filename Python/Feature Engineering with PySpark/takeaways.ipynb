{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create a pandas dataframe from pyspark dataframe for visualization\n",
    "sample_df = large_df.sample(withReplacement = False, fraction=0.3, seed=42) # randomly sample from pyspark dataframe\n",
    "sample_pandas_df = sample_df.toPandas() # Convert to pandas dataframe\n",
    "sns.histplot(sample_pandas_df[numeric_column], kde=True, color='skyblue') # See distribution plot\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# No of missing values\n",
    "df.where(df['col_name'].isNull()).count()\n",
    "# Visualise missing values with heatmap\n",
    "pandas_df = spark_df.toPandas()\n",
    "sns.heatmap(data=pandas_df.isnull())\n",
    "# Drop any records with NULL values\n",
    "df = df.dropna()\n",
    "# drop records if both LISTPRICE and SALESCLOSEPRICE are NULL\n",
    "df = df.dropna(how='all', subset['col1', 'col2 '])\n",
    "# Drop records where at least two columns have NULL values\n",
    "df = df.dropna(thresh=2)\n",
    "# Drop columns with >30% missing values\n",
    "df = df.drop(*col_list)\n",
    "# Replace missing values\n",
    "col_mean = df.agg({'col_name': 'mean'}).collect()[0][0]\n",
    "df.fillna(col_mean, subset=['col_name'])\n",
    "# Drop duplicates\n",
    "df.dropDuplicates(['col_name'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "max_val = df.agg({'col_name': 'max'}).collect()[0][0]\n",
    "min_val = df.agg({'col_name': 'min'}).collect()[0][0]\n",
    "mean_val = df.agg({'col_name': 'mean'}).collect()[0][0]\n",
    "std_val = df.agg({'col_name': 'stddev'}).collect()[0][0]\n",
    "df = df.withColumn(\"min_max_scaled_col\", (df['col_name'] - min_val) / (max_val - min_val))\n",
    "df = df.withColumn(\"standard_scaled_col\", (df['col_name'] - mean_val) / std_val)\n",
    "df = df.withColumn(\"feature_scaled_col\", df['col_name']  / max_val)\n",
    "# Log transformation\n",
    "from pyspark.sql.functions import log\n",
    "df = df.withColumn('log_col', log(df['col_name']))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
