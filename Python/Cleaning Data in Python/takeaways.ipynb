{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See datatype of columns : `df.dtypes`\n",
    "- See dataframe information : `df.info()`\n",
    "- See column information : `df.describe()`\n",
    "- Remove character : `df['col'].str.strip('$')`\n",
    "- Drop values : `df.drop(df[df['col'] > 5].index, inplace = True)`\n",
    "- Set values :\n",
    "    1. `df.loc[df['col'] > 5, 'col2'] = 5`\n",
    "    2. `df.at[df['col'] > 5,'col2']= 5`\n",
    "- Convert type : \n",
    "    - Other data type : `df['col'].astype('int')`\n",
    "    - Catergorical : `df[\"col\"].astype('category')`\n",
    "    - Date : `df['date_column'] = pd.to_datetime(df['datetime_column']).dt.date`\n",
    "- Test : `assert df['col'].dtype == 'int'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips to filter unclean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if integer values are meant to be categorical values\n",
    "- check for unwanted characters\n",
    "- Check if numerical values are converted into strings\n",
    "- Check for out of range values\n",
    "- Check for missing values\n",
    "- Check if dates are in proper format\n",
    "- Check for range of dates (if there are impossible values)\n",
    "- Check for duplicates (both complete and partial)\n",
    "- Checking constrains\n",
    "- cross-field validation : sanity / validity check using multiple fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Duplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Drop complete duplicates\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "# Column names to check for partial duplicates\n",
    "column_names = ['A','B','C']\n",
    "duplicates = df.duplicated(subset = column_names, keep = False)\n",
    "# See partial duplicate values\n",
    "df[duplicates]\n",
    "\n",
    "# Combine result for partial duplicates\n",
    "summaries = {'D': 'max', 'E': 'mean'}\n",
    "df = df.groupby(by = column_names).agg(summaries).reset_index()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter out inconsistent categorical data by comparing them with known categories using anti-join\n",
    "- uppercase : `df['col'].str.upper()`\n",
    "- lowercase : `df['col'].str.lower()`\n",
    "- Remove character : `df['col'].str.strip('$')`\n",
    "- Replace character : \n",
    "    1. `df['col'] = df['col'].apply(lambda x: x.replace('_', '+'))`\n",
    "    2. `df['col'] = df['col'].str.replace('_', '+')`\n",
    "    3. `df['col'] = df['col'].str.replace(r'\\D+', '')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Type constrains : data type\n",
    "- Range constrains : Range of data\n",
    "- Uniqueness constrains : Unique value of row\n",
    "- Membership constain : Known member in a group (eg: month from 1 to 12, week from 1 to 7 etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import pandas as pd\n",
    "# Way 1 : Equal cut\n",
    "group_names = ['0-200K', '200K-500K', '500K+']\n",
    "df['cat'] = pd.qcut(df['range_col'], q = 3, labels = group_names)\n",
    "\n",
    "# Way 2 : More precise\n",
    "ranges = [0,200000,500000,np.inf]\n",
    "group_names = ['0-200K', '200K-500K', '500K+']\n",
    "# Create income group column\n",
    "df['cat'] = pd.cut(df['range_col'], bins=ranges, labels=group_names)\n",
    "\n",
    "# Way 3 : Mapping\n",
    "mapping = {'MALE':'M', 'Male':'M', 'FEMALE':'F', 'Female':'F'}\n",
    "df['cat'] = df['string_col'].replace(mapping)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "assert df['col'].dtype == 'int'\n",
    "assert phone['Phone number'].str.contains(\"+|-\").any() == False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Missing Completely at Random: \n",
    "\t- No systematic relationship between a column's missing values and \n",
    "    other or own values. \n",
    "    - eg : errors when inputting data\n",
    "2. Missing at Random: \n",
    "\t- There is a systematic relationship between a \n",
    "\tcolumn's missing values and other observed values.\n",
    "    - eg : missing ozone data for high temperature\n",
    "3. Missing not at Random: \n",
    "\t- There is a systematic relationship between a \n",
    "\tcolumn's missing values and unobserved values.\n",
    "    - eg : missing temperature values for high temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Show number of missing data\n",
    "df.isna().sum()\n",
    "\n",
    "# Visualize missing data information\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "msno.matrix(airquality)\n",
    "plt.show()\n",
    "\n",
    "# Drop missing data\n",
    "df_dropped = df.dropna(subset = ['col'])\n",
    "\n",
    "# Replace/impute missing data with single value\n",
    "col_mean = df['col'].mean()\n",
    "df_imputed = df.fillna({'col': col_mean})\n",
    "\n",
    "# Replace/impute missing data with series\n",
    "series_imp = df['col1'] * 5\n",
    "df_imputed = df.fillna({'col2':series_imp})\n",
    "\n",
    "# Missing values are not always \"NaN\". They can be blank, \"?\" or other symbols (rarely)\n",
    "# Check for values through manual validations first\n",
    "df[\"col\"].value_counts() # Look out for suspicious values\n",
    "# Determine number of missing values in a column\n",
    "df.isna().any()\n",
    "df['col'].isnull().sum()\n",
    "# Drop missing values\n",
    "df.dropna(axis = 0) # Drop entire row for missing value (default)\n",
    "df.dropna(axis = 1) # Drop entire column for missing value\n",
    "# Drop missing values for specific column\n",
    "df.dropna(subset = [\"col\"], axis = 0)\n",
    "# Replace missing values\n",
    "df[\"col\"].replace(np.nan, new_val)\n",
    "df.fillna(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Way 1\n",
    "df[\"date_col\"] = pd.to_datetime(df[\"date_col\"], \n",
    "                                infer_datetime_format=True,\n",
    "                                errors='coerce')\n",
    "# Way 2\n",
    "df[\"date_col\"] = df[\"date_col\"].dt.strftime(\"%d-%m-%Y\")\n",
    "# Extract month information\n",
    "dataframe[\"date_col\"].dt.month\n",
    "# Extract year information\n",
    "dataframe[\"date_col\"].dt.year\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum edit distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimum edit distance = Least amount of steps needed to transition one string to another\n",
    "- These steps can be\n",
    "    1. Insertion (a new character)\n",
    "    2. Deletion (of a character)\n",
    "    3. Substitution (Replacing a character)\n",
    "    4. Transposition (Swapping 2 character position)\n",
    "- The lower the distance, the closer the 2 strings are\n",
    "<center><img src=\"images/04.01.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "### For comparing two strings\n",
    "from thefuzz import fuzz\n",
    "# Compare reeding vs reading\n",
    "fuzz.WRatio('Reeding', 'Reading') # 86\n",
    "\n",
    "### For comparing a string with a series\n",
    "from thefuzz import process\n",
    "from fuzzywuzzy import process # Alternative\n",
    "# Define string and array of possible matches\n",
    "string = \"Houston Rockets vs Los Angeles Lakers\"\n",
    "choices = pd.Series(['Rockets vs Lakers', 'Lakers vs Rockets',\n",
    "'Houson vs Los Angeles', 'Heat vs Bulls'])\n",
    "# Return highest 2 matches in format (string_in_series, similarity_score, index)\n",
    "process.extract(string, choices, limit = 2) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record linkage : Removing duplicates before appending two different dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import recordlinkage\n",
    "\n",
    "# Create indexing object\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Generate pairs blocked on a column name common in 2 dataframes\n",
    "indexer.block('col')\n",
    "pairs = indexer.index(df1, df2)\n",
    "# See pairs\n",
    "print(pairs)\n",
    "\n",
    "# Create a Compare object\n",
    "compare_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches for pairs of col1 and col2\n",
    "compare_cl.exact('df1_col1', 'df2_col1', label='col1')\n",
    "compare_cl.exact('df1_col2', 'df2_col2', label='col2')\n",
    "\n",
    "# Find close matches for pairs of surname and address_1 using string similarity\n",
    "compare_cl.string('df1_col3', 'df2_col3', threshold=0.85, label='col3')\n",
    "compare_cl.string('df1_col4', 'df2_col4', threshold=0.85, label='col4')\n",
    "\n",
    "# Find matches\n",
    "potential_matches = compare_cl.compute(pairs, df1, df2)\n",
    "# See potential matches\n",
    "print(potential_matches)\n",
    "# Filter matches where more than 2 columns match\n",
    "matches = potential_matches[potential_matches.sum(axis = 1) => 2]\n",
    "print(matches)\n",
    "# See index\n",
    "matches.index\n",
    "# Get index of duplicates in df2\n",
    "duplicate_rows = matches.index.get_level_values(1)\n",
    "# Finding duplicates in df2\n",
    "df2_duplicates = df2[df2.index.isin(duplicate_rows)]\n",
    "# Finding rows in df2 that are not duplicates\n",
    "df2_unique = df2[~df2.index.isin(duplicate_rows)]\n",
    "# Link the DataFrames!\n",
    "full_df = df1.append(df2_unique)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
