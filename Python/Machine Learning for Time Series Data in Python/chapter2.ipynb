{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many repetitions of sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll start with perhaps the simplest classification technique: averaging across dimensions of a dataset and visually inspecting the result.\n",
    "\n",
    "You'll use the heartbeat data described in the last chapter. Some recordings are normal heartbeat activity, while others are abnormal activity. Let's see if you can spot the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_plot_and_make_titles():\n",
    "   axs[0, 0].set(title=\"Normal Heartbeats\")\n",
    "   axs[0, 1].set(title=\"Abnormal Heartbeats\")\n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True)\n",
    "\n",
    "# # Calculate the time array\n",
    "# time =  np.arange(0, len(normal)) / sfreq\n",
    "\n",
    "# # Stack the normal/abnormal audio so you can loop and plot\n",
    "# stacked_audio = np.hstack([normal, abnormal]).T\n",
    "\n",
    "# # Loop through each audio file / ax object and plot\n",
    "# # .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
    "# for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
    "#     ax.plot(time, iaudio)\n",
    "# show_plot_and_make_titles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invariance in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you should always start by visualizing your raw data, this is often uninformative when it comes to discriminating between two classes of data points. Data is usually noisy or exhibits complex patterns that aren't discoverable by the naked eye.\n",
    "\n",
    "Another common technique to find simple differences between two sets of data is to average across multiple instances of the same class. This may remove noise and reveal underlying patterns (or, it may not).\n",
    "\n",
    "In this exercise, you'll average across many instances of each class of heartbeat sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Average across the audio files of each DataFrame\n",
    "# mean_normal = np.mean(normal, axis=1)\n",
    "# mean_abnormal = np.mean(abnormal, axis=1)\n",
    "\n",
    "# # Plot each average over time\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
    "# ax1.plot(time, mean_normal)\n",
    "# ax1.set(title=\"Normal Data\")\n",
    "# ax2.plot(time, mean_abnormal)\n",
    "# ax2.set(title=\"Abnormal Data\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While eye-balling differences is a useful way to gain an intuition for the data, let's see if you can operationalize things with a model. In this exercise, you will use each repetition as a datapoint, and each moment in time as a feature to fit a classifier that attempts to predict abnormal vs. normal heartbeats using only the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "# # Initialize and fit the model\n",
    "# model = LinearSVC()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Generate predictions and score them manually\n",
    "# predictions = model.predict(X_test)\n",
    "# print(sum(predictions == y_test.squeeze()) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the envelope of sound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the ways you can improve the features available to your model is to remove some of the noise present in the data. In audio data, a common way to do this is to smooth the data and then rectify it so that the total amount of sound energy over time is more distinguishable. You'll do this in the current exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the raw data first\n",
    "# audio.plot(figsize=(10, 5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rectify the audio signal\n",
    "# audio_rectified = audio.apply(np.abs)\n",
    "\n",
    "# # Plot the result\n",
    "# audio_rectified.plot(figsize=(10, 5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Smooth by applying a rolling mean\n",
    "# audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
    "\n",
    "# # Plot the result\n",
    "# audio_rectified_smooth.plot(figsize=(10, 5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating features from the envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've removed some of the noisier fluctuations in the audio, let's see if this improves your ability to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate stats\n",
    "# means = np.mean(audio_rectified_smooth, axis=0)\n",
    "# stds = np.std(audio_rectified_smooth, axis=0)\n",
    "# maxs = np.max(audio_rectified_smooth, axis=0)\n",
    "\n",
    "# # Create the X and y arrays\n",
    "# X = np.column_stack([means, stds, maxs])\n",
    "# y = labels.reshape(-1, 1)\n",
    "\n",
    "# # Fit the model and score on testing data\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# percent_score = cross_val_score(model, X, y, cv=5)\n",
    "# print(np.mean(percent_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative features: The tempogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of cleaning up your data is that it lets you compute more sophisticated features. For example, the envelope calculation you performed is a common technique in computing tempo and rhythm features. In this exercise, you'll use librosa to compute some tempo and rhythm features for heartbeat data, and fit a model once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the tempo of the sounds\n",
    "# tempos = []\n",
    "# for col, i_audio in audio.items():\n",
    "#     tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
    "\n",
    "# # Convert the list to an array so you can manipulate it more easily\n",
    "# tempos = np.array(tempos)\n",
    "\n",
    "# # Calculate statistics of each tempo\n",
    "# tempos_mean = tempos.mean(axis=-1)\n",
    "# tempos_std = tempos.std(axis=-1)\n",
    "# tempos_max = tempos.max(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the X and y arrays\n",
    "# X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
    "# y = labels.reshape(-1, 1)\n",
    "\n",
    "# # Fit the model and score on testing data\n",
    "# percent_score = cross_val_score(model, X, y, cv=5)\n",
    "# print(np.mean(percent_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrograms of heartbeat audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral engineering is one of the most common techniques in machine learning for time series data. The first step in this process is to calculate a spectrogram of sound. This describes what spectral content (e.g., low and high pitches) are present in the sound over time. In this exercise, you'll calculate a spectrogram of a heartbeat audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the stft function\n",
    "# from librosa.core import stft\n",
    "\n",
    "# # Prepare the STFT\n",
    "# HOP_LENGTH = 2**4\n",
    "# spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from librosa.core import amplitude_to_db\n",
    "# from librosa.display import specshow\n",
    "\n",
    "# # Convert into decibels\n",
    "# spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# # Compare the raw audio to the spectrogram of the audio\n",
    "# fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "# axs[0].plot(time, audio)\n",
    "# specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH, ax=axs[1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineering spectral features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can probably tell, there is a lot more information in a spectrogram compared to a raw audio file. By computing the spectral features, you have a much better idea of what's going on. As such, there are all kinds of spectral features that you can compute using the spectrogram as a base. In this exercise, you'll look at a few of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa as lr\n",
    "\n",
    "# # Calculate the spectral centroid and bandwidth for the spectrogram\n",
    "# bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
    "# centroids = lr.feature.spectral_centroid(S=spec)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from librosa.core import amplitude_to_db\n",
    "# from librosa.display import specshow\n",
    "\n",
    "# # Convert spectrogram to decibels for visualization\n",
    "# spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# # Display these features on top of the spectrogram\n",
    "# fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH, ax=ax)\n",
    "# ax.plot(times_spec, centroids)\n",
    "# ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
    "# ax.set(ylim=[None, 6000])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining many features in a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've spent this lesson engineering many features from the audio data - some contain information about how the audio changes in time, others contain information about the spectral content that is present.\n",
    "\n",
    "The beauty of machine learning is that it can handle all of these features at the same time. If there is different information present in each feature, it should improve the classifier's ability to distinguish the types of audio. Note that this often requires more advanced techniques such as regularization, which we'll cover in the next chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through each spectrogram\n",
    "# bandwidths = []\n",
    "# centroids = []\n",
    "\n",
    "# for spec in spectrograms:\n",
    "#     # Calculate the mean spectral bandwidth\n",
    "#     this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
    "#     # Calculate the mean spectral centroid\n",
    "#     this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
    "#     # Collect the values\n",
    "#     bandwidths.append(this_mean_bandwidth)  \n",
    "#     centroids.append(this_mean_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create X and y arrays\n",
    "# X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids])\n",
    "# y = labels.reshape(-1, 1)\n",
    "\n",
    "# # Fit the model and score on testing data\n",
    "# percent_score = cross_val_score(model, X, y, cv=5)\n",
    "# print(np.mean(percent_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
