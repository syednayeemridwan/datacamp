{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Volume: Size of the data\n",
    "- Variety: Different sources and formats\n",
    "- Velocity: Speed of the data\n",
    "- Clustered computing: Collection of resources of multiple machines\n",
    "- Parallel computing: Simultaneous computation on single computer\n",
    "- Distributed computing: Collection of nodes (networked computers) that run in parallel\n",
    "- Batch processing: Breaking the job into small pieces and running them on individual machines\n",
    "- Real-time processing: Immediate processing of data\n",
    "- Big Data processing systems\n",
    "    - Hadoop/MapReduce: Scalable and fault tolerant framework written in Java (Batch processing)\n",
    "    - Apache Spark: General purpose and lightning fast cluster computing system (Both batch and real-time data processing)\n",
    "    - Note: Apache Spark is nowadays preferred over Hadoop/MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- General purpose data processing engine designed for big data.\n",
    "- Written in scala\n",
    "- Spark is a platform for cluster computing.\n",
    "- Spark lets you spread data and computations over clusters with multiple nodes (each node as a separate computer). \n",
    "- Very large datasets are split into smaller datasets and  each node only works with a small amount of data.\n",
    "- Data processing and computation are performed in parallel over the nodes in the cluster. \n",
    "- However, with greater computing power comes greater complexity.\n",
    "- Can be used for Analytics, Data Integration, Machine learning, Stream Processing.\n",
    "- Master and Worker:\n",
    "    - Master: \n",
    "        - Connected to the rest of the computers in the cluster, which are called worker\n",
    "        - sends the workers data and calculations to run\n",
    "    - Worker: \n",
    "        - They send their results back to the master.\n",
    "- Spark's core data structure is the Resilient Distributed Dataset (RDD)\n",
    "- Instead of RDDs, it is easier to work with Spark DataFrame abstraction built on top of RDDs ( Operations using DataFrames are automatically optimized.)\n",
    "- spark dataframes are immutable, you need to return a new instance after modification \n",
    "- You start working with `SparkSession` or `SparkContext` entrypoint\n",
    "- 2 modes:\n",
    "    - local mode : Single computer\n",
    "    - cluster mode : cluster computers\n",
    "    - You first build in local mode and deploy in cluster mode (no code change is required)\n",
    "- Spark shell : \n",
    "    - interactive environment for spark jobs\n",
    "    - allow interacting with data on disk or in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "func_name = lambda inputs : return_expression\n",
    "\n",
    "add = lambda a, b : a + b\n",
    "add(3,6) ## 9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#### Core python use case #####\n",
    "#map(func_name, some_list)\n",
    "\n",
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x + 2 , items))  ## [3, 4, 5, 6]\n",
    "#### Dataframe Application #####\n",
    "# Method 1\n",
    "df[\"col\"].apply(lambda x: x+1)\n",
    "# Method 2\n",
    "genders = {'James': 'Male', 'Jane': 'Female'}\n",
    "df['gender'] = df['name'].map(genders)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "## filter(boolean_func, list)\n",
    "\n",
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: (x%2 != 0), items)) ## [1, 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Create a SparkConf object to configure the SparkContext\n",
    "conf = SparkConf().setAppName(\"YourAppName\").setMaster(\"local[*]\")\n",
    "\n",
    "# Create a SparkContext with the configured SparkConf object\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "print(sc) # Verify SparkContext\n",
    "print(sc.version) # Print Spark version\n",
    "print(sc.pythonVer) # Print Python version\n",
    "print(sc.master) # Print the spark mode\n",
    "\n",
    "# Loading data\n",
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd2 = sc.textFile(\"test.txt\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
