{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defined by 5 Vs:\n",
    "    - velocity =  Data generated at extremely fast speed.\n",
    "    - volume = Amount is huge\n",
    "    - variety = Data comes from many different sources\n",
    "    - veracity = Consistency, Completeness, Integrity, Ambiguity of data (Structured, unstructured, semi-structured)\n",
    "    - value = Derive insight from data\n",
    "\n",
    "- Related concepts:\n",
    "    - Clustered computing: Collection of resources of multiple machines\n",
    "    - Parallel computing: Simultaneous computation on single computer\n",
    "    - Distributed computing: Collection of nodes (networked computers) that run in parallel\n",
    "    - Batch processing: Breaking the job into small pieces and running them on individual machines\n",
    "    - Real-time processing: Immediate processing of data\n",
    "    - Big Data processing systems\n",
    "        - Hadoop/MapReduce: Scalable and fault tolerant framework written in Java (for Distributed storage and Batch processing)\n",
    "        - Apache Spark: General purpose and lightning fast cluster computing system for Distributed real-time analytics (Both batch and real-time data processing)\n",
    "        - Apache Hive (Warehouse for query and analysis)\n",
    "        - Note: Apache Spark is nowadays preferred over Hadoop/MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- General purpose data processing engine designed for big data.\n",
    "- Written in scala\n",
    "- Spark is a platform for cluster computing.\n",
    "- Spark lets you spread data and computations over clusters with multiple nodes (each node as a separate computer). \n",
    "- Very large datasets are split into smaller datasets and  each node only works with a small amount of data.\n",
    "- Data processing and computation are performed in parallel over the nodes in the cluster. \n",
    "- However, with greater computing power comes greater complexity.\n",
    "- Can be used for Analytics, Data Integration, Machine learning, Stream Processing.\n",
    "- Master and Worker:\n",
    "    - Master: \n",
    "        - Connected to the rest of the computers in the cluster, which are called worker\n",
    "        - sends the workers data and calculations to run\n",
    "    - Worker: \n",
    "        - They send their results back to the master.\n",
    "- Spark's core data structure is the Resilient Distributed Dataset (RDD)\n",
    "- Instead of RDDs, it is easier to work with Spark DataFrame abstraction built on top of RDDs ( Operations using DataFrames are automatically optimized.)\n",
    "- spark dataframes are immutable, you need to return a new instance after modification \n",
    "- You start working with `SparkSession` or `SparkContext` entrypoint\n",
    "- 2 modes:\n",
    "    - local mode : Single computer\n",
    "    - cluster mode : cluster computers\n",
    "    - You first build in local mode and deploy in cluster mode (no code change is required)\n",
    "- Spark shell : \n",
    "    - interactive environment for spark jobs\n",
    "    - allow interacting with data on disk or in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "func_name = lambda inputs : return_expression\n",
    "\n",
    "add = lambda a, b : a + b\n",
    "add(3,6) ## 9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#### Core python use case #####\n",
    "#map(func_name, some_list)\n",
    "\n",
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x + 2 , items))  ## [3, 4, 5, 6]\n",
    "#### Dataframe Application #####\n",
    "# Method 1\n",
    "df[\"col\"].apply(lambda x: x+1)\n",
    "# Method 2\n",
    "genders = {'James': 'Male', 'Jane': 'Female'}\n",
    "df['gender'] = df['name'].map(genders)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "## filter(boolean_func, list)\n",
    "\n",
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: (x%2 != 0), items)) ## [1, 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from functools import reduce\n",
    "\n",
    "some_list = [1, 2, 3, 4, 5]\n",
    "total_sum = reduce(lambda x, y: x + y, some_list) # 15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load and Query CSV with SQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Register the DataFrame as a temporary table or view\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Run SQL queries on the DataFrame\n",
    "query_result = spark.sql(\"SELECT * FROM my_table WHERE column_name = 'value'\")\n",
    "\n",
    "# Show the query result\n",
    "query_result.show()\n",
    "\n",
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Access the SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "spark = SparkSession(sc) # Create a SparkSession from SparkContext\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create a context from SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Alternative : create spark context explicitly\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"YourAppName\").setMaster(\"local[*]\") # Set configuration for SparkContext\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "print(sc) # Verify SparkContext\n",
    "print(sc.version) # Print Spark version\n",
    "print(sc.pythonVer) # Print Python version\n",
    "print(sc.master) # Print the spark mode\n",
    "\n",
    "# Loading data (With specified number of partitions)\n",
    "numRDD = sc.parallelize(range(10), minPartitions = 6)\n",
    "fileRDD = sc.textFile(\"README.md\", minPartitions = 6)\n",
    "fileRDD.getNumPartitions() # See number of broken parts\n",
    "\n",
    "# Create a SparkSession from SparkContext\n",
    "spark = SparkSession(sc) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True) # load file\n",
    "df.printSchema() # Show the DataFrame schema\n",
    "df.show(5) # Show the first few rows of the DataFrame\n",
    "df.createOrReplaceTempView(\"table_name\") # Register DataFrame as a temporary view\n",
    "result = spark.sql(\"SELECT * FROM table_name\") # Run query on table\n",
    "result.show() # Show result\n",
    "spark_df = spark.table(\"table_name\") # start using a spark table as spark dataframe\n",
    "\n",
    "df_pandas = df.toPandas() # Convert from spark dataframe to pandas dataframe\n",
    "df_spark = spark.createDataFrame(df_pandas) # Convert from pandas dataframe to spark dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "RDD = sc.textFile(\"README.md\", minPartitions = 5)\n",
    "RDD.getNumPartitions() # See number of partitions\n",
    "RDD = sc.parallelize([1,2,3,4])\n",
    "RDD_map = RDD.map(lambda x: x * x) # using map with an RDD\n",
    "RDD_filter = RDD.filter(lambda x: x > 2) # using filter with an RDD\n",
    "RDD_reduce = RDD.reduce(lambda x, y : x + y) # 10\n",
    "\n",
    "RDD.flatMap(lambda x: x.split(\" \")) # flatMap returns multiple values for each element in the original RDD\n",
    "combinedRDD = RDD1.union(RDD2) # Combining 2 RDDs\n",
    "RDD.collect() # Return all elements of dataset as an array\n",
    "RDD.take(2)  # Return first n elements of dataset\n",
    "RDD.first() # Return first element of dataset\n",
    "RDD.count() # Return no of elements in the RDD\n",
    "\n",
    "RDD.saveAsTextFile(\"tempFile\") # Save text file as multiple partition files\n",
    "RDD.coalesce(1).saveAsTextFile(\"tempFile\")  # Save text file as a single file\n",
    "\n",
    "# Working with paired data\n",
    "my_tuple = [(\"Messi\", 23), (\"Ronaldo\", 34), (\"Neymar\", 22), (\"Messi\", 24)]\n",
    "pairRDD = sc.parallelize(my_tuple)\n",
    "pairRDD.reduceByKey(lambda x,y : x + y).collect() # [('Neymar', 22), ('Ronaldo', 34), ('Messi', 47)]\n",
    "pairRDD = pairRDD.map(lambda x: (x[1], x[0])) # keys and values swap places\n",
    "pairRDD.sortByKey(ascending=False).collect()#  [(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]\n",
    "RDD1.join(RDD2).collect() # Joining two RDDs\n",
    "# Groupby operation\n",
    "grouped_RDD = pairRDD.groupByKey().collect() \n",
    "for key, val in grouped_RDD:\n",
    "    print(key, list(val))\n",
    "\n",
    "# Countby operation\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "countby_rdd = rdd.countByKey()\n",
    "for key, val in countby_rdd.items():\n",
    "    print(key, val) # ('a', 2) , ('b', 1)\n",
    "\n",
    "# Turning into dictionary\n",
    "pairRDD.collectAsMap()\n",
    "\n",
    "# Turning into dataframe\n",
    "RDD = sc.parallelize([(\"X10\", 2017, 5.65, 2.79, 6.13),\n",
    "                    (\"8Plus\", 2017, 6.23, 3.07, 7.12)])\n",
    "names = ['Model', 'Year', 'Height', 'Width', 'Weight']\n",
    "spark_df = spark.createDataFrame(iphones_RDD, schema=names) # spark is sparksession object\n",
    "df_pandas = spark_df.toPandas() # Convert from spark dataframe to pandas dataframe\n",
    "handy_df = spark_df.toHandy() # Convert to handyspark dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create dataframe from RDD\n",
    "spark_df = spark.createDataFrame(RDD, schema=colname_list)\n",
    "# Loading file\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True) # .json, .txt\n",
    "df.show(3)\n",
    "df.printSchema() # See schema information\n",
    "df.describe().show() # Summary stats\n",
    "df.createOrReplaceTempView(\"table_name\") # Register DataFrame as a temporary view\n",
    "result = spark.sql(\"SELECT * FROM table_name\") # Run query on table\n",
    "spark_df = spark.table(\"table_name\") # start using a spark table as spark dataframe\n",
    "# Add a new result column\n",
    "df = df.withColumn(\"new_col\",df.old_col+10)\n",
    "# Selecting column\n",
    "df = df.select(df.col1, df.col2, df.col3)\n",
    "calculated_col = (df.col1/(df.col2/60)).alias(\"another_col\")\n",
    "df = df.select(\"col1\", \"col2\", \"col3\", calculated_col)\n",
    "df = df.selectExpr(\"col1\", \"col2\", \"col3\", \"col1/(col2/60) as another_col\")\n",
    "# Filtering (Both produces same results)\n",
    "df.filter(\"col_name > 120\").show()\n",
    "df.filter(df.col_name > 120).show()\n",
    "# Chaining filters\n",
    "filterA = df.col1 == \"SEA\"\n",
    "filterB = df.col2 == \"PDX\"\n",
    "result = temp.filter(filterA).filter(filterB)\n",
    "\n",
    "df.groupBy(\"col_name\").count().show() # Group by and count\n",
    "df.orderBy(\"col_name\").show(3) # order by and count\n",
    "# Aggregation\n",
    "df.filter(df.col == 'value').groupBy().max(\"another_col\").show()\n",
    "df = df.na.drop(subset=[\"col_name\"]) # Drop nulls\n",
    "df = df.dropDuplicates() # Drop duplicates\n",
    "# Rename column\n",
    "df = df.withColumnRenamed(\"old_col_name\", \"new_col_name\")\n",
    "\n",
    "# Casting / Converting column type\n",
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"col_name\", col(\"col_name\").cast(\"float\"))\n",
    "df = df.withColumn(\"col_name\", df.col_name.cast(\"float\"))\n",
    "\n",
    "# SQL with dataframe\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "df2 = spark.sql(\"SELECT * FROM table_name\")\n",
    "result = df2.collect() # Dataframe as list of rows tha you can iterate over\n",
    "\n",
    "## Visualization : Pyspark_dist_explore, pandas (NOT RECOMMENDED), HandySpark(RECOMMENDED)\n",
    "pandas_df = spark_df.toPandas()\n",
    "handy_df = spark_df.toHandy() # Convert to handyspark dataframe\n",
    "handy_df.cols[\"col_name\"].hist()\n",
    "spark_df = handy_df.to_spark() # Convert to pyspark dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning with dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# One-hot encoding\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "# StringIndexer does indexing for each category. this step allows handling unseen category in testing set\n",
    "string_indexer1 = StringIndexer(inputCol=\"cat_col\",outputCol=\"string_index\") \n",
    "one_hot_encoder1 = OneHotEncoder(inputCol=\"string_index\",outputCol=\"onehot_feature\") # One-hot encoding using the category indices\n",
    "# Combine all features\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\", \"onehot_feature1\", \"onehot_feature2\"], outputCol=\"features\")\n",
    "\n",
    "# Define the model\n",
    "model_rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10) # from pyspark.ml.classification import RandomForestClassifier\n",
    "model_lr1 = LogisticRegression(featuresCol=\"features\", labelCol=\"label\") # from pyspark.ml.classification import LogisticRegression\n",
    "model_lr2 = LinearRegression(featuresCol=\"features\", labelCol=\"label\") # from pyspark.ml.regression import LinearRegression\n",
    "model_kmeans = KMeans(featuresCol=\"features\", predictionCol=\"kmeans_prediction\", k=3) # from pyspark.ml.clustering import KMeans\n",
    "# deep learninng\n",
    "layers = [len(feature_cols) + 2, 5, 2]  # Input layer size, hidden layer sizes, output layer size\n",
    "model_dl = MultilayerPerceptronClassifier(layers=layers, labelCol=\"label\", featuresCol=\"features\", seed=123)\n",
    "# Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[string_indexer1, one_hot_encoder1, string_indexer2, one_hot_encoder2, vec_assembler, model_xx])\n",
    "\n",
    "# Create the parameter grid\n",
    "import pyspark.ml.tuning as tune\n",
    "paramGrid = tune.ParamGridBuilder()\\\n",
    "        .addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "        .addGrid(lr.elasticNetParam, [0, 1])\n",
    "        .build()\n",
    "\n",
    "# Evaluation metric\n",
    "import pyspark.ml.evaluation as evals\n",
    "evaluator_logistic = evals.BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "evaluator_reg = evals.RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_rf_dl = evals.MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\") \n",
    "# Create cross-validator\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3) \n",
    "\n",
    "# Split the data into training and test sets\n",
    "training, test = transformed_df.randomSplit([.6, .4])\n",
    "\n",
    "cvModel = cv.fit(training) # Fit the dataframe\n",
    "bestModel = cvModel.bestModel # Best model\n",
    "bestParams = bestModel.stages[-1].extractParamMap() # See best parameters\n",
    "test_results = bestModel.transform(test) # Use the model to predict the test set\n",
    "predictions = cvModel.transform(test) # Predict using testing set\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "feature_importances = bestModel.stages[-1].featureImportances\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning with RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "data = [\n",
    "    ('A', 0.7, 'm', 1.0),\n",
    "    ('B', 0.1, 'f', 0.3),\n",
    "    ('A', 0.8, 'm', 0.2),\n",
    "    ('C', 0.2, 'f', 0.5),\n",
    "    ('C', 0.5, 'f', 0.6)\n",
    "]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Convert label to numerical values\n",
    "label_mapping = {'A': 0, 'B': 1, 'C': 2}\n",
    "rdd = rdd.map(lambda x: (label_mapping[x[0]], x[1:]))\n",
    "# Convert data to LabeledPoint (helps to identify the labels as rdd.label and features as rdd.features)\n",
    "labeled_rdd = rdd.map(lambda x: LabeledPoint(x[0], Vectors.dense(x[1])))\n",
    "\n",
    "# One-hot encoding ( using Pipeline and pyspark dataframe construct)\n",
    "pipeline = Pipeline(stages=[\n",
    "    StringIndexer(inputCol='_2', outputCol='gender_index'),\n",
    "    OneHotEncoder(inputCol='gender_index', outputCol='gender_encoded')\n",
    "])\n",
    "df = spark.createDataFrame(labeled_rdd, [\"label\", \"features\"])\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df = pipeline_model.transform(df)\n",
    "# Convert DataFrame back to RDD\n",
    "rdd = df.rdd.map(lambda row: (row.label, row.features, row.gender_encoded))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(trainingData, testData) = rdd.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train the model \n",
    "model_lr = LogisticRegressionWithLBFGS.train(trainingData)\n",
    "model_lin = LinearRegressionWithSGD.train(trainingData, iterations=100, step=0.1)\n",
    "model_rf = RandomForest.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={},\n",
    "                                     numTrees=10, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "kmeans_model = KMeans.train(trainingData.map(lambda x: x[1]), k=3, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# Train the model with deep learning \n",
    "input_size = len(trainingData.first()[1])\n",
    "output_size = 3  # number of classes\n",
    "hidden_layers = [input_size, 5, output_size]  # input layer size, hidden layer sizes, output layer size\n",
    "model_mlp = MultilayerPerceptronClassifier.train(trainingData, iterations=100, stepSize=0.1, layers=hidden_layers)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = testData.map(lambda x: (model_.predict(x[1]), x[0]))\n",
    "predictions_kmeans = kmeans_model.predict(testData.map(lambda x: x[1]))\n",
    "\n",
    "# Evaluate MLP, Random Forest model\n",
    "metrics = MulticlassMetrics(predictions)\n",
    "accuracy = metrics_mlp.accuracy\n",
    "\n",
    "# Evaluate Logistic Regression model\n",
    "metrics_lr = BinaryClassificationMetrics(predictions_lr)\n",
    "auc_roc_lr = metrics_lr.areaUnderROC\n",
    "\n",
    "# Evaluate Linear Regression model\n",
    "metrics_lin = RegressionMetrics(predictions_lin)\n",
    "rmse_lin = metrics_lin.rootMeanSquaredError\n",
    "\n",
    "# Compute R-squared for KMeans model\n",
    "def calculate_rmse(predictions):\n",
    "    return np.sqrt(predictions.map(lambda x: (x[0] - x[1]) ** 2).mean())\n",
    "\n",
    "# Calculate RMSE for KMeans model\n",
    "rmse_kmeans = calculate_rmse(testData.map(lambda x: (predictions_kmeans.predict(x[1]), x[0])))\n",
    "\n",
    "sc.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colaborative Filtering pyspark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ALS Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data (User ID, Item ID, Rating, Additional Column1, Additional Column2)\n",
    "data = [\n",
    "    (1, 1, 5, \"A\", \"X\"),\n",
    "    (1, 2, 4, \"B\", \"Y\"),\n",
    "    (2, 1, 3, \"C\", \"Z\"),\n",
    "    (2, 2, 5, \"D\", \"W\"),\n",
    "    (3, 1, 4, \"E\", \"V\"),\n",
    "    (3, 2, 2, \"F\", \"U\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"user\", \"item\", \"rating\", \"additional_col1\", \"additional_col2\"])\n",
    "\n",
    "# Split data into training and test sets\n",
    "(training_data, test_data) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train ALS model\n",
    "als = ALS(rank=10, maxIter=10, regParam=0.01, userCol=\"user\", itemCol=\"item\", ratingCol=\"rating\")\n",
    "model = als.fit(training_data)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate predictions using RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) = \" + str(rmse))\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, Rating\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(\"local\", \"ALS Example\")\n",
    "\n",
    "# Sample data (User ID, Item ID, Rating, Additional Column1, Additional Column2)\n",
    "data = [\n",
    "    (1, 1, 5, \"A\", \"X\"),\n",
    "    (1, 2, 4, \"B\", \"Y\"),\n",
    "    (2, 1, 3, \"C\", \"Z\"),\n",
    "    (2, 2, 5, \"D\", \"W\"), # <--- say this data is for testing\n",
    "    (3, 1, 4, \"E\", \"V\"),\n",
    "    (3, 2, 2, \"F\", \"U\")\n",
    "]\n",
    "\n",
    "# Create RDD\n",
    "ratings_rdd = sc.parallelize(data)\n",
    "\n",
    "# Map the data to Rating objects (User ID, Item ID, Rating)\n",
    "ratings = ratings_rdd.map(lambda x: Rating(x[0], x[1], x[2])) # Rating(user=1, product=1, rating=5.0)\n",
    "\n",
    "# Split data into training and test sets\n",
    "training_data, test_data = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train ALS model\n",
    "rank = 10  # Number of latent factors\n",
    "num_iterations = 10  # Number of iterations\n",
    "model = ALS.train(training_data, rank, num_iterations)\n",
    "\n",
    "# Make predictions on test data\n",
    "test_user_item_pairs = test_data.map(lambda x: (x[0], x[1]))\n",
    "predictions = model.predictAll(test_user_item_pairs)\n",
    "predictions = predictions.map(lambda r: ((r[0], r[1]), r[2])) # ((2, 2), 5.008601768134059)\n",
    "\n",
    "# Join predicted ratings with actual ratings\n",
    "rates_and_preds = test_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions) # ((2, 2), (5.0, 5.008601768134059))\n",
    "\n",
    "# Calculate RMSE\n",
    "RMSE = (rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())**0.5\n",
    "print(\"Root Mean Squared Error (RMSE) = \" + str(RMSE))\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
