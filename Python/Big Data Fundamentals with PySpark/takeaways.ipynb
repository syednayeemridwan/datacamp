{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Volume: Size of the data\n",
    "- Variety: Different sources and formats\n",
    "- Velocity: Speed of the data\n",
    "- Clustered computing: Collection of resources of multiple machines\n",
    "- Parallel computing: Simultaneous computation on single computer\n",
    "- Distributed computing: Collection of nodes (networked computers) that run in parallel\n",
    "- Batch processing: Breaking the job into small pieces and running them on individual machines\n",
    "- Real-time processing: Immediate processing of data\n",
    "- Big Data processing systems\n",
    "    - Hadoop/MapReduce: Scalable and fault tolerant framework written in Java (Batch processing)\n",
    "    - Apache Spark: General purpose and lightning fast cluster computing system (Both batch and real-time data processing)\n",
    "    - Note: Apache Spark is nowadays preferred over Hadoop/MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- General purpose data processing engine designed for big data.\n",
    "- Written in scala\n",
    "- Spark is a platform for cluster computing.\n",
    "- Spark lets you spread data and computations over clusters with multiple nodes (each node as a separate computer). \n",
    "- Very large datasets are split into smaller datasets and  each node only works with a small amount of data.\n",
    "- Data processing and computation are performed in parallel over the nodes in the cluster. \n",
    "- However, with greater computing power comes greater complexity.\n",
    "- Can be used for Analytics, Data Integration, Machine learning, Stream Processing.\n",
    "- Master and Worker:\n",
    "    - Master: \n",
    "        - Connected to the rest of the computers in the cluster, which are called worker\n",
    "        - sends the workers data and calculations to run\n",
    "    - Worker: \n",
    "        - They send their results back to the master.\n",
    "- Spark's core data structure is the Resilient Distributed Dataset (RDD)\n",
    "- Instead of RDDs, it is easier to work with Spark DataFrame abstraction built on top of RDDs ( Operations using DataFrames are automatically optimized.)\n",
    "- spark dataframes are immutable, you need to return a new instance after modification \n",
    "- You start working with `SparkSession` or `SparkContext` entrypoint\n",
    "- 2 modes:\n",
    "    - local mode : Single computer\n",
    "    - cluster mode : cluster computers\n",
    "    - You first build in local mode and deploy in cluster mode (no code change is required)\n",
    "- Spark shell : \n",
    "    - interactive environment for spark jobs\n",
    "    - allow interacting with data on disk or in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "func_name = lambda inputs : return_expression\n",
    "\n",
    "add = lambda a, b : a + b\n",
    "add(3,6) ## 9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#### Core python use case #####\n",
    "#map(func_name, some_list)\n",
    "\n",
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x + 2 , items))  ## [3, 4, 5, 6]\n",
    "#### Dataframe Application #####\n",
    "# Method 1\n",
    "df[\"col\"].apply(lambda x: x+1)\n",
    "# Method 2\n",
    "genders = {'James': 'Male', 'Jane': 'Female'}\n",
    "df['gender'] = df['name'].map(genders)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "## filter(boolean_func, list)\n",
    "\n",
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: (x%2 != 0), items)) ## [1, 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load and Query CSV with SQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Register the DataFrame as a temporary table or view\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Run SQL queries on the DataFrame\n",
    "query_result = spark.sql(\"SELECT * FROM my_table WHERE column_name = 'value'\")\n",
    "\n",
    "# Show the query result\n",
    "query_result.show()\n",
    "\n",
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Access the SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "spark = SparkSession(sc) # Create a SparkSession from SparkContext\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create a context from SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Alternative : create spark context explicitly\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"YourAppName\").setMaster(\"local[*]\") # Set configuration for SparkContext\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "print(sc) # Verify SparkContext\n",
    "print(sc.version) # Print Spark version\n",
    "print(sc.pythonVer) # Print Python version\n",
    "print(sc.master) # Print the spark mode\n",
    "\n",
    "# Loading data (With specified number of partitions)\n",
    "numRDD = sc.parallelize(range(10), minPartitions = 6)\n",
    "fileRDD = sc.textFile(\"README.md\", minPartitions = 6)\n",
    "fileRDD.getNumPartitions() # See number of broken parts\n",
    "\n",
    "# Create a SparkSession from SparkContext\n",
    "spark = SparkSession(sc) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True) # load file\n",
    "df.printSchema() # Show the DataFrame schema\n",
    "df.show(5) # Show the first few rows of the DataFrame\n",
    "df.createOrReplaceTempView(\"table_name\") # Register DataFrame as a temporary view\n",
    "result = spark.sql(\"SELECT * FROM table_name\") # Run query on table\n",
    "result.show() # Show result\n",
    "\n",
    "df_pandas = df.toPandas() # Convert from spark dataframe to pandas dataframe\n",
    "df_spark = spark.createDataFrame(df_pandas) # Convert from pandas dataframe to spark dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "RDD = sc.textFile(\"README.md\", minPartitions = 5)\n",
    "RDD.getNumPartitions() # See number of partitions\n",
    "RDD = sc.parallelize([1,2,3,4])\n",
    "RDD_map = RDD.map(lambda x: x * x) # using map with an RDD\n",
    "RDD_filter = RDD.filter(lambda x: x > 2) # using filter with an RDD\n",
    "RDD_reduce = RDD.reduce(lambda x, y : x + y) # 10\n",
    "\n",
    "RDD.flatMap(lambda x: x.split(\" \")) # flatMap returns multiple values for each element in the original RDD\n",
    "combinedRDD = RDD1.union(RDD2) # Combining 2 RDDs\n",
    "RDD.collect() # Return all elements of dataset as an array\n",
    "RDD.take(2)  # Return first n elements of dataset\n",
    "RDD.first() # Return first element of dataset\n",
    "RDD.count() # Return no of elements in the RDD\n",
    "\n",
    "RDD.saveAsTextFile(\"tempFile\") # Save text file as multiple partition files\n",
    "RDD.coalesce(1).saveAsTextFile(\"tempFile\")  # Save text file as a single file\n",
    "\n",
    "# Working with paired data\n",
    "my_tuple = [(\"Messi\", 23), (\"Ronaldo\", 34), (\"Neymar\", 22), (\"Messi\", 24)]\n",
    "pairRDD = sc.parallelize(my_tuple)\n",
    "pairRDD.reduceByKey(lambda x,y : x + y).collect() # [('Neymar', 22), ('Ronaldo', 34), ('Messi', 47)]\n",
    "pairRDD = pairRDD.map(lambda x: (x[1], x[0])) # keys and values swap places\n",
    "pairRDD.sortByKey(ascending=False).collect()#  [(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]\n",
    "RDD1.join(RDD2).collect() # Joining two RDDs\n",
    "# Groupby operation\n",
    "grouped_RDD = pairRDD.groupByKey().collect() \n",
    "for key, val in grouped_RDD:\n",
    "    print(key, list(val))\n",
    "\n",
    "# Countby operation\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "countby_rdd = rdd.countByKey()\n",
    "for key, val in countby_rdd.items():\n",
    "    print(key, val) # ('a', 2) , ('b', 1)\n",
    "\n",
    "# Turning into dictionary\n",
    "pairRDD.collectAsMap()\n",
    "\n",
    "# Turning into dataframe\n",
    "RDD = sc.parallelize([(\"X10\", 2017, 5.65, 2.79, 6.13),\n",
    "                    (\"8Plus\", 2017, 6.23, 3.07, 7.12)])\n",
    "names = ['Model', 'Year', 'Height', 'Width', 'Weight']\n",
    "spark_df = spark.createDataFrame(iphones_RDD, schema=names) # spark is sparksession object\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create dataframe from RDD\n",
    "spark_df = spark.createDataFrame(RDD, schema=colname_list)\n",
    "# Loading file\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True) # .json, .txt\n",
    "df.show(3)\n",
    "df.printSchema() # See schema information\n",
    "df.describe().show() # Summary stats\n",
    "# Add a new result column\n",
    "df = df.withColumn(\"new_col\",df.old_col+10)\n",
    "# Selecting column\n",
    "df = df.select(df.col1, df.col2, df.col3)\n",
    "calculated_col = (df.col1/(df.col2/60)).alias(\"another_col\")\n",
    "df = df.select(\"col1\", \"col2\", \"col3\", calculated_col)\n",
    "df = df.selectExpr(\"col1\", \"col2\", \"col3\", \"col1/(col2/60) as another_col\")\n",
    "# Filtering (Both produces same results)\n",
    "df.filter(\"col_name > 120\").show()\n",
    "df.filter(df.col_name > 120).show()\n",
    "# Chaining filters\n",
    "filterA = df.col1 == \"SEA\"\n",
    "filterB = df.col2 == \"PDX\"\n",
    "result = temp.filter(filterA).filter(filterB)\n",
    "\n",
    "\n",
    "df.groupBy(\"col_name\").count().show() # Group by and count\n",
    "df.orderBy(\"col_name\").show(3) # order by and count\n",
    "# Aggregation\n",
    "df.filter(df.col == 'value').groupBy().max(\"another_col\").show()\n",
    "df = df.na.drop(subset=[\"col_name\"]) # Drop nulls\n",
    "df = df.dropDuplicates() # Drop duplicates\n",
    "# Rename column\n",
    "df = df.withColumnRenamed(\"old_col_name\", \"new_col_name\")\n",
    "\n",
    "# Casting / Converting column type\n",
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"col_name\", col(\"col_name\").cast(\"float\"))\n",
    "df = df.withColumn(\"col_name\", df.col_name.cast(\"float\"))\n",
    "\n",
    "# SQL with dataframe\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "df2 = spark.sql(\"SELECT * FROM table_name\")\n",
    "result = df2.collect() # Dataframe as list of rows tha you can iterate over\n",
    "\n",
    "## Visualization : Pyspark_dist_explore, pandas (NOT RECOMMENDED), HandySpark(RECOMMENDED)\n",
    "pandas_df = spark_df.toPandas()\n",
    "handy_df = spark_df.toHandy() # Convert to handyspark dataframe\n",
    "handy_df.cols[\"col_name\"].hist()\n",
    "spark_df = handy_df.to_spark() # Convert to pyspark dataframe\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
