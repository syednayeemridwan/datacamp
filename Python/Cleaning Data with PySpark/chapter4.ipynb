{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you parse some more complex data, your manager would like to see a simple pipeline example including the basic steps. For this example, you'll want to ingest a data file, filter a few rows, add an ID column to it, then write it out as JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data to a DataFrame\n",
    "departures_df = spark.read.csv('dataset/AA_DFW_2015_Departures_Short.csv', header=True)\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(\" 'Actual elapsed time (Minutes)' <> '0'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+---+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)| id|\n",
      "+-----------------+-------------+-------------------+-----------------------------+---+\n",
      "|       01/01/2015|         0005|                HNL|                          526|  0|\n",
      "|       01/01/2015|         0007|                OGG|                          517|  1|\n",
      "|       01/01/2015|         0023|                SFO|                          233|  2|\n",
      "|       01/01/2015|         0027|                LAS|                          165|  3|\n",
      "|       01/01/2015|         0029|                ONT|                            0|  4|\n",
      "+-----------------+-------------+-------------------+-----------------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "# departures_df.write.json('output.json')\n",
    "departures_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline data issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating your quick pipeline, you provide the json file to an analyst on your team. After loading the data and performing a couple exploratory tasks, the analyst tells you there's a problem in the dataset while trying to sort the duration data. She's not sure what the issue is beyond the sorting operation not working as expected.\n",
    "```\n",
    "Date          Flight Number   Airport     Duration    ID\n",
    "\n",
    "09/30/2015    2287            ANC         409         107962\n",
    "12/28/2015    1408            OKC         41          141917\n",
    "08/11/2015    2287            ANC         410         87978\n",
    "```\n",
    "After analyzing the data, which command would fix the issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `departures_df = departures_df.withColumn('Duration', departures_df['Duration'].cast(IntegerType()))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing commented lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your boss would like you to perform some complex parsing on a new dataset. The data represents annotation data for the ImageNet dataset, but focusing specifically on dog breeds and identifying them in images. Before any actual analysis can occur, you'll need to clear out several components of invalid / incorrect data. The general schema of the document is unknown so you'd like to import the rows into a single column, allowing for quick analysis.\n",
    "\n",
    "To start, you need to remove all commented rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full count: 139359\n",
      "Comment count: 0\n",
      "Remaining count: 139359\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv('dataset/AA_DFW_2017_Departures_Short.csv', sep='|')\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.filter(col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv('dataset/AA_DFW_2017_Departures_Short.csv', sep='|', comment='#')\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing invalid rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've successfully removed the commented rows, you have received some information about the general format of the data. There should be at minimum 5 tab separated columns in the DataFrame. Remember that your original DataFrame only has a single column, so you'll need to split the data on the tab (\\t) characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial count: 139359\n",
      "Final count: 0\n"
     ]
    }
   ],
   "source": [
    "initial_count = annotations_df.count()\n",
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Create the colcount column on the DataFrame\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "\n",
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (annotations_df.colcount < 5))\n",
    "\n",
    "# Count the number of rows\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've cleaned up your data considerably by removing the invalid rows from the DataFrame. Now you want to perform some further transformations by generating specific meaningful columns based on the DataFrame content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------+-----+------+--------------------+\n",
      "|                 _c0|colcount|              folder|filename|width|height|          split_cols|\n",
      "+--------------------+--------+--------------------+--------+-----+------+--------------------+\n",
      "|Date (MM/DD/YYYY)...|       1|Date (MM/DD/YYYY)...|    NULL| NULL|  NULL|[Date (MM/DD/YYYY...|\n",
      "|\"01/01/2017\",\"000...|       1|\"01/01/2017\",\"000...|    NULL| NULL|  NULL|[\"01/01/2017\",\"00...|\n",
      "+--------------------+--------+--------------------+--------+-----+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)\n",
    "split_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've molded this dataset into a significantly different format than it was before, but there are still a few things left to do. You need to prep the column data for use in later analysis and remove a few intermediary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "def retriever(cols, colcount):\n",
    "  # Return a list of dog data\n",
    "  return cols[4:colcount]\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
    "\n",
    "# Create a new column using your UDF\n",
    "split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\n",
    "\n",
    "# Remove the original column, split_cols, and the colcount\n",
    "split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139359"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_df.count()\n",
    "# split_df.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate rows via join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of filtering data is using joins to remove invalid entries. You'll need to verify the folder names are as expected based on a given DataFrame named valid_folders_df. The DataFrame split_df is as you last left it with a group of split columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename the column in valid_folders_df\n",
    "# valid_folders_df = valid_folders_df.withColumnRenamed('_c0','folder')\n",
    "\n",
    "# # Count the number of rows in split_df\n",
    "# split_count = split_df.count()\n",
    "\n",
    "# # Join the DataFrames\n",
    "# joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
    "\n",
    "# # Compare the number of rows remaining\n",
    "# joined_count = joined_df.count()\n",
    "# print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining invalid rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've successfully filtered out the rows using a join, but sometimes you'd like to examine the data that is invalid. This data can be stored for later processing or for troubleshooting your data sources.\n",
    "\n",
    "You want to find the difference between two DataFrames and store the invalid rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine the row counts for each DataFrame\n",
    "# split_count = split_df.count()\n",
    "# joined_count = joined_df.count()\n",
    "\n",
    "# # Create a DataFrame containing the invalid rows\n",
    "# invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'left_anti')\n",
    "\n",
    "# # Validate the count of the new DataFrame is as expected\n",
    "# invalid_count = invalid_df.count()\n",
    "# print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
    "\n",
    "# # Determine the number of distinct folder rows removed\n",
    "# invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "# print(\"%d distinct invalid folders found\" % invalid_folder_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've done a considerable amount of cleanup on the initial dataset, but now need to analyze the data a bit deeper. There are several questions that have now come up about the type of dogs seen in an image and some details regarding the images. You realize that to answer these questions, you need to process the data into a specific type. Before you can use it, you'll need to create a schema / type to represent the dog details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "# Select the dog details and show 10 untruncated rows\n",
    "# print(joined_df.select(\"dog_list\").show(10, truncate=False))\n",
    "\n",
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "\tStructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_y\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per image count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task in building a data pipeline for this dataset is to create a few analysis oriented columns. You've been asked to calculate the number of dogs found in each image based on your dog_list column created earlier. You have also created the DogType which will allow better parsing of the data within some of the data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a function to return the number and type of dogs as a tuple\n",
    "# def dogParse(doglist):\n",
    "#   dogs = []\n",
    "#   for dog in doglist:\n",
    "#     (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "#     dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "#   return dogs\n",
    "\n",
    "# # Create a UDF\n",
    "# udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# # Use the UDF to list of dogs and drop the old column\n",
    "# joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
    "\n",
    "# # Show the number of dogs in the first 10 rows\n",
    "# joined_df.select(F.size('dogs')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage dog pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final task for parsing the dog annotation data is to determine the percentage of pixels in each image that represents a dog (or dogs). You'll need to use the various techniques you've learned in this course to help calculate this information and add it as columns for later analysis.\n",
    "\n",
    "To calculate the percentage of pixels, first calculate the total number of pixels representing each dog then sum them for the image. You can calculate the bounding box with the formula:\n",
    "\n",
    "(Xend - Xstart) * (Yend - Ystart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a UDF to determine the number of pixels per image\n",
    "# def dogPixelCount(doglist):\n",
    "#   totalpixels = 0\n",
    "#   for dog in doglist:\n",
    "#     totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
    "#   return totalpixels\n",
    "\n",
    "# # Define a UDF for the pixel count\n",
    "# udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
    "# joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount(joined_df.dogs))\n",
    "\n",
    "# # Create a column representing the percentage of pixels\n",
    "# joined_df = joined_df.withColumn('dog_percent', (col('dog_pixels') / (joined_df.width * joined_df.height)) * 100)\n",
    "\n",
    "# # Show the first 10 annotations with more than 60% dog\n",
    "# joined_df.where(\"dog_percent > 60\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
