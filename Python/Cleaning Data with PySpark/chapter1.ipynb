{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load and Query CSV with SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many benefits for using Spark for data cleaning.\n",
    "Which of the following are benefits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Spark offers high performance.\n",
    "- Spark allows orderly data flows.\n",
    "- Spark can use strictly defined schemas while ingesting data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a defined schema helps with data quality and import performance. As mentioned during the lesson, we'll create a simple schema to read in the following columns:\n",
    "\n",
    "- Name\n",
    "- Age\n",
    "- City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immutability review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youâ€™ve just seen that immutability and lazy processing are fundamental concepts in the way Spark handles data. But why would Spark use immutable data frames to begin with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To efficiently handle data throughout the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using lazy processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Date\", DateType(), nullable=True),\n",
    "    StructField(\"Flight Number\", StringType(), nullable=True),\n",
    "    StructField(\"Destination Airport\", StringType(), nullable=True),\n",
    "    StructField(\"Actual elapsed time (Minutes)\", IntegerType(), nullable=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2017|         0005|                          537|    hnl|\n",
      "|       01/01/2017|         0007|                          498|    ogg|\n",
      "|       01/01/2017|         0037|                          241|    sfo|\n",
      "|       01/01/2017|         0043|                          134|    dtw|\n",
      "|       01/01/2017|         0051|                           88|    stl|\n",
      "|       01/01/2017|         0060|                          149|    mia|\n",
      "|       01/01/2017|         0071|                          203|    lax|\n",
      "|       01/01/2017|         0074|                           76|    mem|\n",
      "|       01/01/2017|         0081|                          123|    den|\n",
      "|       01/01/2017|         0089|                          161|    slc|\n",
      "|       01/01/2017|         0096|                           84|    stl|\n",
      "|       01/01/2017|         0103|                          216|    sjc|\n",
      "|       01/01/2017|         0119|                          514|    ogg|\n",
      "|       01/01/2017|         0123|                          529|    hnl|\n",
      "|       01/01/2017|         0126|                          171|    lga|\n",
      "|       01/01/2017|         0132|                          188|    ewr|\n",
      "|       01/01/2017|         0140|                          231|    sjc|\n",
      "|       01/01/2017|         0174|                          145|    rdu|\n",
      "|       01/01/2017|         0176|                          184|    bos|\n",
      "|       01/01/2017|         0190|                           76|    sat|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('dataset/AA_DFW_2017_Departures_Short.csv')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date (MM/DD/YYYY)', 'string'),\n",
       " ('Flight Number', 'string'),\n",
       " ('Actual elapsed time (Minutes)', 'string'),\n",
       " ('airport', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_dfw_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a DataFrame in Parquet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides a lot of flexibility for the types of data to load, but it is not an optimal format for Spark. The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 139358\n",
      "df2 Count: 140604\n"
     ]
    }
   ],
   "source": [
    "df1 = aa_dfw_df\n",
    "df2 = spark.read.format('csv').options(Header=True).schema(schema).load('dataset/AA_DFW_2016_Departures_Short.csv')\n",
    "\n",
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "# df3.write.format('parquet').save('dataset/AA_DFW_ALL.parquet')\n",
    "# df3.write.parquet('AA_DFW_ALL', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "# print(spark.read.parquet('dataset/AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL and Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Date (MM/DD/YYYY)', 'string'), ('Flight Number', 'string'), ('Actual elapsed time (Minutes)', 'string'), ('airport', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(aa_dfw_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(avg(CAST(Actual elapsed time (Minutes) AS INT))=151.99931112673832)\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "# flights_df = spark.read.csv('dataset/AA_DFW_2017_Departures_Short.csv', header=True)\n",
    "flights_df = aa_dfw_df\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(CAST(`Actual elapsed time (Minutes)` AS INTEGER)) FROM flights').collect()[0]\n",
    "# print('The average flight time is: %d' % avg_duration)\n",
    "print(avg_duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
