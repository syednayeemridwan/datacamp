{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date (MM/DD/YYYY)', 'string'),\n",
       " ('Flight Number', 'string'),\n",
       " ('Destination Airport', 'string'),\n",
       " ('Actual elapsed time (Minutes)', 'string')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "departure_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('Date (MM/DD/YYYY)', DataType(), True),\n",
    "  StructField('Flight Number', StringType(), True),\n",
    "  StructField('Destination Airport', StringType(), True),\n",
    "  StructField('Actual elapsed time (Minutes)', IntegerType(), True),\n",
    "])\n",
    "\n",
    "departures_df = spark.read.csv(\"dataset/AA_DFW_2017_Departures_Short.csv\", header=True)\n",
    "# departures_df.show(3)\n",
    "departures_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139358 rows took 1.847591 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139358 rows again took 0.148730 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing a DataFrame from cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've finished the analysis tasks with the departures_df DataFrame, but have some other processing to do. You'd like to remove the DataFrame from the cache to prevent any excess memory usage on your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File size optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider if you're given 2 large data files on a cluster with 10 nodes. Each file contains 10M rows of roughly the same size. While working with your data, the responsiveness is acceptable but the initial read from the files takes a considerable period of time. Note that you are the only one who will use the data and it changes for each run.\n",
    "\n",
    "Which of the following is the best option to improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the 2 files into 50 files of 400K rows each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File import performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the full and split files into DataFrames\n",
    "# full_df = spark.read.csv('departures_full.txt.gz')\n",
    "# split_df = spark.read.csv('departures_*.txt.gz') \n",
    "\n",
    "# # Print the count and run time for each DataFrame\n",
    "# start_time_a = time.time()\n",
    "# print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "# print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "# start_time_b = time.time()\n",
    "# print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "# print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Spark configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've recently configured a cluster via a cloud provider. Your only access is via the command shell or your python code. You'd like to verify some Spark settings to validate the configuration of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: example\n",
      "Driver TCP port: 63379\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get(\"spark.app.name\")\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get(\"spark.driver.port\")\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Spark configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've reviewed some of the Spark configurations on your cluster, you want to modify some of the settings to tune Spark to your needs. You'll import some data to review that your changes have affected the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store the number of partitions in variable\n",
    "# before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# # Configure Spark to use 500 partitions\n",
    "# spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# # Recreate the DataFrame using the departures data file\n",
    "# departures_df = spark.read.csv('departures.txt.gz').distinct()\n",
    "\n",
    "# # Print the number of partitions for each instance\n",
    "# print(\"Partition count before change: %d\" % before)\n",
    "# print(\"Partition count after change: %d\" % departures_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've been given two DataFrames to combine into a single useful DataFrame. Your first task is to combine the DataFrames normally and view the execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2016|         0005|                HNL|                          529|\n",
      "|       01/01/2016|         0007|                OGG|                          512|\n",
      "|       01/01/2016|         0025|                PHL|                          161|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2017|         0005|                HNL|                          537|\n",
      "|       01/01/2017|         0007|                OGG|                          498|\n",
      "|       01/01/2017|         0037|                SFO|                          241|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df = spark.read.csv(\"dataset/AA_DFW_2016_Departures_Short.csv\", header=True)\n",
    "airports_df =  spark.read.csv(\"dataset/AA_DFW_2017_Departures_Short.csv\", header=True)\n",
    "flights_df.show(3)\n",
    "airports_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Destination Airport#516], [Destination Airport#541], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(Destination Airport#516)\n",
      "   :  +- FileScan csv [Date (MM/DD/YYYY)#514,Flight Number#515,Destination Airport#516,Actual elapsed time (Minutes)#517] Batched: false, DataFilters: [isnotnull(Destination Airport#516)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Datacamp/Python/Cleaning Data with PySpark/dataset/AA_DFW_201..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[2, string, false]),false), [plan_id=580]\n",
      "      +- Filter isnotnull(Destination Airport#541)\n",
      "         +- FileScan csv [Date (MM/DD/YYYY)#539,Flight Number#540,Destination Airport#541,Actual elapsed time (Minutes)#542] Batched: false, DataFilters: [isnotnull(Destination Airport#541)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Datacamp/Python/Cleaning Data with PySpark/dataset/AA_DFW_201..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"Destination Airport\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using broadcasting on Spark joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required and can have a negative impact on performance. Instead, we're going to use Spark's broadcast operations to give each node a copy of the specified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Destination Airport#516], [Destination Airport#541], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(Destination Airport#516)\n",
      "   :  +- FileScan csv [Date (MM/DD/YYYY)#514,Flight Number#515,Destination Airport#516,Actual elapsed time (Minutes)#517] Batched: false, DataFilters: [isnotnull(Destination Airport#516)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Datacamp/Python/Cleaning Data with PySpark/dataset/AA_DFW_201..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[2, string, false]),false), [plan_id=603]\n",
      "      +- Filter isnotnull(Destination Airport#541)\n",
      "         +- FileScan csv [Date (MM/DD/YYYY)#539,Flight Number#540,Destination Airport#541,Actual elapsed time (Minutes)#542] Batched: false, DataFilters: [isnotnull(Destination Airport#541)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Datacamp/Python/Cleaning Data with PySpark/dataset/AA_DFW_201..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"Destination Airport\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing broadcast vs normal joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've created two types of joins, normal and broadcasted. Now your manager would like to know what the performance improvement is by using Spark optimizations. If the results are promising, you'll be given more opportunity to tweak the Spark setup as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count:\t\t369539750\tduration: 23.009646\n",
      "Broadcast count:\t369539750\tduration: 19.118006\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
