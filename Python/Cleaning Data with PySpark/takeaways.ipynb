{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create dataframe from RDD\n",
    "spark_df = spark.createDataFrame(RDD, schema=colname_list)\n",
    "\n",
    "# Loading file (folder name will make the spark load all files in that folder in parallel mode)\n",
    "from pyspark.sql.types import *\n",
    "dataSchema = StructType([ StructField('col1', StringType(), , nullable=True),\n",
    "                            StructField('col2', StringType(), , nullable=False)])\n",
    "df = spark.read.csv(\"file.csv\", header=True, schema=dataSchema) # .json, .txt, .load for parquet\n",
    "df = spark.read.format('csv').options(Header=True).load(name='filename.csv', inferSchema=True) # schema=dataSchema\n",
    "df.write.parquet('filename.parquet', mode='overwrite') # Save file (parquet is more efficient, binary format for big data)\n",
    "df.write.format('parquet').save('filename.parquet')\n",
    "df.show(3) # Show first 3 rows\n",
    "df.collect() # Store result as list of tuples\n",
    "df.limit(3) # Same as show\n",
    "df.dtypes # See datatype of each column\n",
    "df.printSchema() # See schema information\n",
    "result.columns # See result table columns\n",
    "df = df.na.drop(subset=[\"col_name\"]) # Drop nulls\n",
    "df = df.drop(subset=[\"col_name\"]) # Drop column\n",
    "df = df.dropDuplicates() # Drop duplicates\n",
    "df = df.withColumn(\"col_name\", col(\"col_name\").cast(\"float\"))  # Way 1 : Casting a column to another data type\n",
    "df = df.withColumn(\"col_name\", df.col_name.cast(\"float\")) # Way 2 : Casting a column to another data type\n",
    "df.describe().show() # Summary stats\n",
    "df = df.repartition(4, 'some_col') # create 4 partitions using same column values of specified column\n",
    "print(df.rdd.getNumPartitions()) # See no of partitions of the dataset\n",
    "\n",
    "df = df.select(df.col1, df.col2, df.col3) # way1 : select column from dataframe\n",
    "df = df.select(\"col1\", \"col2\") # way2 : select column from dataframe\n",
    "df.select(col('col1'), col('col2')) # way3 : select column from dataframe,  import col from sql.functions\n",
    "df = df.withColumn(\"new_col\",df.old_col+10) # Add a new result column\n",
    "df = df.withColumnRenamed(\"old_col_name\", \"new_col_name\") # Rename column\n",
    "df = df.select(col('col1').alias('col1_renamed'), 'col2')\n",
    "df = df.selectExpr(\"col1\", \"col2\", \"col3\", \"col1/(col2/60) as another_col\")\n",
    "df = df.withColumn(\"idx\", monotonically_increasing_id()) # Creating id column\n",
    "df.where(array_contains('col', 'abc')) # Check if an element is inside an array\n",
    "df1 = df1.withColumn(\"source\", lit(\"df1\")) # Adding constants in a column\n",
    "\n",
    "df_vertical = df1.union(df2) # Vertical join (append rows vertically)\n",
    "df_horizontal = df1.join(df1, on=['common_col1', 'common_col2'], how=\"left\") (append columns horizontally with join)\n",
    "df_cross = df1.crossJoin(df2) # Cross Join (Horizontally appending columns of possible combinations)\n",
    "\n",
    "# Filtering (Both produces same results)\n",
    "df = df.filter(\"col_name > 120\").show()\n",
    "df = df.where(\"Value > 120\")\n",
    "df = df.filter(df.col_name > 120).show()\n",
    "df = df.where(df.Value > 120)\n",
    "filterA = df.col1 == \"SEA\"\n",
    "result = temp.filter(filterA).filter(filterB) # Chaining filters\n",
    "df.groupBy(\"col_name\").count() # Group by and count\n",
    "df.orderBy(\"col_name\") # order by \n",
    "df.filter(df.col == 'value').groupBy().max(\"another_col\") # Multiple chaining aggregation\n",
    "\n",
    "df.createOrReplaceTempView(\"table_name\") # Register DataFrame as a temporary talbe in catalog\n",
    "spark.catalog.listTables() # See all table information in the catalog\n",
    "spark.catalog.dropTempView('table_name') # Remove temp table from catalog\n",
    "spark_df = spark.table(\"table_name\") # start using a spark table as spark dataframe\n",
    "result = spark.sql(\"SELECT * FROM table_name\") # Run query on table\n",
    "\n",
    "# Using Custom function to double the value of a column\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "def double_val(col):\n",
    "    return col * 2 # Make sure any new data is casted to proper type\n",
    "double_val_udf = udf(double_val, IntegerType()) # Register UDF with custom function and return type\n",
    "df = df.withColumn(\"DoubledCol\", double_val_udf(df[\"col\"]))\n",
    "\n",
    "## Visualization : Pyspark_dist_explore, pandas (NOT RECOMMENDED), HandySpark(RECOMMENDED)\n",
    "pandas_df = spark_df.toPandas()\n",
    "handy_df = spark_df.toHandy() # Convert to handyspark dataframe\n",
    "handy_df.cols[\"col_name\"].hist()\n",
    "spark_df = handy_df.to_spark() # Convert to pyspark dataframe\n",
    "\n",
    "## NOTE\n",
    "# Array: [1.0, 0.0, 0.0, 3.0]\n",
    "# Sparse vector: (4, [0, 3], [1.0, 3.0])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
