{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| NULL|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| NULL|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
     ]
    }
   ],
   "source": [
    "# Import the SparkSession class\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Read data from CSV file\n",
    "flights = spark.read.csv(\"dataset/flights.csv\",\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding flight origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The org column in the flights data is a categorical variable giving the airport from which a flight departs.\n",
    "since this is a categorical variable, it needs to be one-hot encoded before it can be used in a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|org_idx|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| NULL|    2.0|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|    0.0|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|    1.0|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"org\", outputCol=\"org_idx\")\n",
    "flights = indexer.fit(flights).transform(flights)\n",
    "flights.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x0000022762F7D430>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\88016\\anaconda3\\envs\\env_py\\lib\\site-packages\\pyspark\\ml\\wrapper.py\", line 53, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'LinearRegression' object has no attribute '_java_obj'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "|org|org_idx|    org_dummy|\n",
      "+---+-------+-------------+\n",
      "|ORD|    0.0|(7,[0],[1.0])|\n",
      "|SFO|    1.0|(7,[1],[1.0])|\n",
      "|JFK|    2.0|(7,[2],[1.0])|\n",
      "|LGA|    3.0|(7,[3],[1.0])|\n",
      "|SJC|    4.0|(7,[4],[1.0])|\n",
      "|SMF|    5.0|(7,[5],[1.0])|\n",
      "|TUS|    6.0|(7,[6],[1.0])|\n",
      "|OGG|    7.0|    (7,[],[])|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoder(inputCols=[\"org_idx\"], outputCols=[\"org_dummy\"])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights)\n",
    "flights_onehot = onehot.transform(flights)\n",
    "\n",
    "# Check the results\n",
    "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().orderBy('org_idx').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding shirt sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have data for a consignment of t-shirts. The data includes the size of the shirt, which is given as either S, M, L or XL.\n",
    "\n",
    "Here are the counts for the different sizes:\n",
    "```\n",
    "+----+-----+\n",
    "|size|count|\n",
    "+----+-----+\n",
    "|   S|    8|\n",
    "|   M|   15|\n",
    "|   L|   20|\n",
    "|  XL|    7|\n",
    "+----+-----+\n",
    "```\n",
    "The sizes are first converted to an index using StringIndexer and then one-hot encoded using OneHotEncoder.\n",
    "\n",
    "Which of the following is true:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S shirts get index 2.0 and are one-hot encoded as `(3,[2],[1.0])`\n",
    "- M shirts get index 1.0 and are one-hot encoded as `(3,[1],[1.0])`\n",
    "- L shirts get index 0.0 and are one-hot encoded as `(3,[0],[1.0])`\n",
    "- XL shirts get index 3.0 and are one-hot encoded as `(3,[3],[1.0])` (FALSE, it should be `(3,[],[])` for being least frequent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight duration model: Just distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you'll build a regression model to predict flight duration (the duration column).\n",
    "\n",
    "For the moment you'll keep the model simple, including only the distance of the flight (the km column) as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    km|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+\n",
      "| 11| 20|  6|     US|    19|JFK|  9.48|     351| NULL|    2.0|3465.0|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 16.33|      82|   30|    0.0| 509.0|\n",
      "|  2| 20|  4|     UA|   226|SFO|  6.17|      82|   -8|    1.0| 542.0|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "flights = flights.withColumn('km', round(flights.mile * 1.60934, 0)).drop('mile')\n",
    "flights.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|duration|prediction        |\n",
      "+--------+------------------+\n",
      "|560     |560.7038535078656 |\n",
      "|310     |346.93268176430854|\n",
      "|90      |85.06866770419609 |\n",
      "|130     |133.6152160803417 |\n",
      "|251     |245.45375976931263|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.096730471353727"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vector_assembler = VectorAssembler(inputCols=['km'], outputCol='feature')\n",
    "\n",
    "# Apply the VectorAssembler to the DataFrame\n",
    "flights = vector_assembler.transform(flights)\n",
    "flights_train , flights_test = flights.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(featuresCol= \"feature\", labelCol=\"duration\").fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol=\"duration\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model for flight duration as a function of distance takes the form of which you need to find co-efficients of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.38635769427969\n",
      "[0.07561767659835762]\n",
      "0.07561767659835762\n",
      "793.465267634303\n"
     ]
    }
   ],
   "source": [
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# Average minutes per km\n",
    "minutes_per_km = regression.coefficients[0]\n",
    "print(minutes_per_km)\n",
    "\n",
    "# Average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight duration model: Adding origin airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some airports are busier than others. Some airports are bigger than others too. Flights departing from large or busy airports are likely to spend more time taxiing or waiting for their takeoff slot. So it stands to reason that the duration of a flight might depend not only on the distance being covered but also the airport from which the flight departs.\n",
    "\n",
    "You are going to make the regression model a little more sophisticated by including the departure airport as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.096730471353727"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(featuresCol=\"feature\", labelCol=\"duration\").fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(metricName=\"rmse\",predictionCol=\"prediction\", labelCol=\"duration\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you'll be using the intercept and coefficients attributes to interpret the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "onehot = OneHotEncoder(inputCol=\"org_idx\", outputCol=\"onehot_features\")\n",
    "flights = flights.drop(\"feature\")\n",
    "flights = onehot.fit(flights).transform(flights)\n",
    "assembler = VectorAssembler(inputCols=[\"onehot_features\", \"km\"], outputCol=\"features\")\n",
    "flights = assembler.transform(flights)\n",
    "flights_train , flights_test = flights.randomSplit([0.8, 0.2], seed=42)\n",
    "regression = LinearRegression(featuresCol= \"features\", labelCol=\"duration\").fit(flights_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+---------------+--------------------+-----------------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    km|onehot_features|            features|       prediction|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+---------------+--------------------+-----------------+\n",
      "|  0|  1|  2|     AA|    73|ORD|  9.08|     560|   39|    0.0|6828.0|  (7,[0],[1.0])|(8,[0,7],[1.0,682...|551.5973962731919|\n",
      "|  0|  1|  2|     AA|   254|OGG| 15.33|     310|  173|    7.0|4001.0|      (7,[],[])|    (8,[7],[4001.0])|313.1084698157231|\n",
      "|  0|  1|  2|     AA|   321|ORD| 14.17|      90| NULL|    0.0| 538.0|  (7,[0],[1.0])|(8,[0,7],[1.0,538...|84.30558533338989|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+---------------+--------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = regression.transform(flights_test)\n",
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1076569514466468\n",
      "15.86927497150248\n",
      "62.631386351468024\n",
      "34.156885367050286\n"
     ]
    }
   ],
   "source": [
    "# Average speed in km per hour\n",
    "avg_speed_hour = 60/regression.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# Average minutes on ground at OGG\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Average minutes on ground at JFK\n",
    "avg_ground_jfk = inter + regression.coefficients[3]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# Average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print(avg_ground_lga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing departure time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time of day data are a challenge with regression models. They are also a great candidate for bucketing.\n",
    "\n",
    "In this lesson you will convert the flight departure times from numeric values between 0 (corresponding to 00:00) and 24 (corresponding to 24:00) to binned values. You'll then take those binned values and one-hot encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|depart|depart_bucket|\n",
      "+------+-------------+\n",
      "|  9.48|          3.0|\n",
      "| 16.33|          5.0|\n",
      "|  6.17|          2.0|\n",
      "| 10.33|          3.0|\n",
      "|  8.92|          2.0|\n",
      "+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------------+-------------+\n",
      "|depart|depart_bucket| depart_dummy|\n",
      "+------+-------------+-------------+\n",
      "|  9.48|          3.0|(7,[3],[1.0])|\n",
      "| 16.33|          5.0|(7,[5],[1.0])|\n",
      "|  6.17|          2.0|(7,[2],[1.0])|\n",
      "| 10.33|          3.0|(7,[3],[1.0])|\n",
      "|  8.92|          2.0|(7,[2],[1.0])|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoder\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[0, 3, 6, 9, 12, 15, 18, 21, 24], inputCol=\"depart\", outputCol=\"depart_bucket\")\n",
    "\n",
    "# Bucket the departure times\n",
    "bucketed = buckets.transform(flights)\n",
    "bucketed.select(\"depart\", \"depart_bucket\").show(5)\n",
    "\n",
    "# Create a one-hot encoder\n",
    "onehot = OneHotEncoder(inputCols=[\"depart_bucket\"], outputCols=[\"depart_dummy\"])\n",
    "\n",
    "# One-hot encode the bucketed departure times\n",
    "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
    "flights_onehot.select(\"depart\", \"depart_bucket\" , \"depart_dummy\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight duration model: Adding departure time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise the departure time was bucketed and converted to dummy variables. Now you're going to include those dummy variables in a regression model for flight duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+---------------+--------------------+-------------+-------------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    km|onehot_features|            features|depart_bucket| depart_dummy|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+---------------+--------------------+-------------+-------------+\n",
      "| 11| 20|  6|     US|    19|JFK|  9.48|     351| NULL|    2.0|3465.0|  (7,[2],[1.0])|(8,[2,7],[1.0,346...|          3.0|(7,[3],[1.0])|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 16.33|      82|   30|    0.0| 509.0|  (7,[0],[1.0])|(8,[0,7],[1.0,509...|          5.0|(7,[5],[1.0])|\n",
      "|  2| 20|  4|     UA|   226|SFO|  6.17|      82|   -8|    1.0| 542.0|  (7,[1],[1.0])|(8,[1,7],[1.0,542...|          2.0|(7,[2],[1.0])|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+---------------+--------------------+-------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_onehot.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+---------------+-------------+--------------------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    km|onehot_features| depart_dummy|            features|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+---------------+-------------+--------------------+\n",
      "| 11| 20|  6|     US|    19|JFK|  9.48|     351| NULL|    2.0|3465.0|  (7,[2],[1.0])|(7,[3],[1.0])|(15,[2,7,11],[1.0...|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 16.33|      82|   30|    0.0| 509.0|  (7,[0],[1.0])|(7,[5],[1.0])|(15,[0,7,13],[1.0...|\n",
      "|  2| 20|  4|     UA|   226|SFO|  6.17|      82|   -8|    1.0| 542.0|  (7,[1],[1.0])|(7,[2],[1.0])|(15,[1,7,10],[1.0...|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+------+---------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights = flights_onehot.drop(\"features\",\"depart_bucket\")\n",
    "assembler = VectorAssembler(inputCols=[\"onehot_features\", \"km\",\"depart_dummy\"], outputCol=\"features\")\n",
    "flights = assembler.transform(flights)\n",
    "flights_train , flights_test = flights.randomSplit([0.8, 0.2], seed=42)\n",
    "regression = LinearRegression(featuresCol= \"features\", labelCol=\"duration\").fit(flights_train)\n",
    "flights.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 11.06612411700972\n",
      "37.600937704813745\n",
      "11.108296473968375\n",
      "56.847536232973404\n"
     ]
    }
   ],
   "source": [
    "# Find the RMSE on testing data\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "rmse = RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 21:00 and 24:00\n",
    "avg_eve_ogg = regression.intercept + regression.coefficients[0]\n",
    "print(avg_eve_ogg)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 03:00 and 06:00\n",
    "avg_night_ogg = regression.intercept + regression.coefficients[9]\n",
    "print(avg_night_ogg)\n",
    "\n",
    "# Average minutes on ground at JFK for flights departing between 03:00 and 06:00\n",
    "avg_night_jfk = regression.intercept + regression.coefficients[3] + regression.coefficients[9]\n",
    "print(avg_night_jfk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight duration model: More features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add more features to our model. This will not necessarily result in a better model. Adding some features might improve the model. Adding other features might make it worse.\n",
    "\n",
    "More features will always make the model more complicated and difficult to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 10.739666414749705\n",
      "[27.21148698285345,20.120371742311743,51.660771743136635,45.73923975900503,17.54570737854217,14.961066263965431,17.2773184530285,0.07437193971557562,-14.687939367843523,0.718845752008081,4.190872842376593,6.96789335124698,4.704065371399039,8.882119147405417,8.783608300881815]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fit linear regression model to training data\n",
    "regression = LinearRegression(labelCol=\"duration\").fit(flights_train)\n",
    "\n",
    "# Make predictions on testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol=\"duration\").evaluate(predictions)\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight duration model: Regularization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise you added more predictors to the flight duration model. The model performed well on testing data, but with so many coefficients it was difficult to interpret.\n",
    "\n",
    "In this exercise you'll use Lasso regression (regularized with a L1 penalty) to create a more parsimonious model. Many of the coefficients in the resulting model will be set to zero. This means that only a subset of the predictors actually contribute to the model. Despite the simpler model, it still produces a good RMSE on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 11.633714942221816\n",
      "[5.50105829928655,0.0,28.8707664794491,22.01523873805985,0.0,-2.3916616818485816,0.0,0.07351140786201599,0.0,0.0,0.0,0.0,0.0,1.0280434522136144,1.1412353904957169]\n",
      "Number of coefficients equal to 0: 8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fit Lasso model (λ = 1, α = 1) to training data\n",
    "regression = LinearRegression(labelCol=\"duration\", regParam=1, elasticNetParam=1).fit(flights_train)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol=\"duration\").evaluate(regression.transform(flights_test))\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)\n",
    "\n",
    "# Number of zero coefficients\n",
    "zero_coeff = sum([beta==0 for beta in regression.coefficients])\n",
    "print(\"Number of coefficients equal to 0:\", zero_coeff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
