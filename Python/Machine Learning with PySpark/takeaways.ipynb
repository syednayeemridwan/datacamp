{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "\t.master('local[*]') \\ # Location of cluster, use all cores of local computer\n",
    "    .appName(\"Load and Query CSV with SQL\") \\\n",
    "    .getOrCreate()\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "StructField(\"col1\", StringType()),\n",
    "StructField(\"col2\", IntegerType()),\n",
    "StructField(\"col3\", DoubleType())\n",
    "])\n",
    "# Load the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"file.csv\",sep=',', header=True, inferSchema=True, nullValue='NA') # schema= schema\n",
    "# Check column types\n",
    "df.printSchema()\n",
    "df.dtypes\n",
    "# Register the DataFrame as a temporary table or view\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "# Run SQL queries on the DataFrame\n",
    "query_result = spark.sql(\"SELECT * FROM my_table WHERE column_name = 'value'\")\n",
    "query_result.show()\n",
    "\n",
    "sc = spark.sparkContext # Access the SparkContext from SparkSession\n",
    "spark = SparkSession(sc) # Create a SparkSession from SparkContext\n",
    "spark.stop() # Stop SparkSession\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# lowercase the strings in a column\n",
    "df = df.select(lower(col('col_name'))) \n",
    "# replace string or characters\n",
    "df = df1.select(regexp_replace('col_name', 'old', 'new').alias('new_col'))\n",
    "# Split a string on space\n",
    "df = df.select(split('string_col', '[ ]').alias('word_list'))\n",
    "# Split string using any given symbol\n",
    "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()\"\n",
    "df = df.select(split('string_col', '[ %s]' % punctuation).alias('word_list'))\n",
    "# Filter out empty strings from the resulting list\n",
    "df = df.filter(col('word_list') != '')\n",
    "# Explode the string list column so that each row contains one value of list\n",
    "df = df.select(explode('word_list').alias('word'))\n",
    "\n",
    "### dealing with NLP related features\n",
    "# replace unwanted characters\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "REGEX = '[,\\\\-]'\n",
    "df = df.withColumn('text', regexp_replace(df.text, REGEX, ' '))\n",
    "# Tokenize words\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "df = Tokenizer(inputCol=\"text\", outputCol=\"tokens\").transform(df)\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopwords = StopWordsRemover(inputCol='tokens', outputCol='words')\n",
    "stopwords.getStopWords() # Take a look at the list of stop words when stopwords = StopWordsRemover()\n",
    "df = stopwords.transform(df)\n",
    "# Hash the features\n",
    "from pyspark.ml.feature import HashingTF\n",
    "hasher = HashingTF(inputCol=\"words\", outputCol=\"hash\", numFeatures=32)\n",
    "df = hasher.transform(df)\n",
    "# Normalize the text features (TF-IDF)\n",
    "from pyspark.ml.feature import IDF\n",
    "df = IDF(inputCol=\"hash\", outputCol=\"features\").fit(df).transform(df)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
