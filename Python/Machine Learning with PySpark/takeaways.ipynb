{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "\t.master('local[*]') \\ # Location of cluster, use all cores of local computer\n",
    "    .appName(\"Load and Query CSV with SQL\") \\\n",
    "    .getOrCreate()\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "StructField(\"col1\", StringType()),\n",
    "StructField(\"col2\", IntegerType()),\n",
    "StructField(\"col3\", DoubleType())\n",
    "])\n",
    "# Load the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"file.csv\",sep=',', header=True, inferSchema=True, nullValue='NA') # schema= schema\n",
    "# Check column types\n",
    "df.printSchema()\n",
    "df.dtypes\n",
    "# Register the DataFrame as a temporary table or view\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "# Run SQL queries on the DataFrame\n",
    "query_result = spark.sql(\"SELECT * FROM my_table WHERE column_name = 'value'\")\n",
    "query_result.show()\n",
    "\n",
    "sc = spark.sparkContext # Access the SparkContext from SparkSession\n",
    "spark = SparkSession(sc) # Create a SparkSession from SparkContext\n",
    "spark.stop() # Stop SparkSession\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# lowercase the strings in a column\n",
    "df = df.select(lower(col('col_name'))) \n",
    "# replace string or characters\n",
    "df = df1.select(regexp_replace('col_name', 'old', 'new').alias('new_col'))\n",
    "# Split a string on space\n",
    "df = df.select(split('string_col', '[ ]').alias('word_list'))\n",
    "# Split string using any given symbol\n",
    "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()\"\n",
    "df = df.select(split('string_col', '[ %s]' % punctuation).alias('word_list'))\n",
    "# Filter out empty strings from the resulting list\n",
    "df = df.filter(col('word_list') != '')\n",
    "# Explode the string list column so that each row contains one value of list\n",
    "df = df.select(explode('word_list').alias('word'))\n",
    "pivot_df = df.groupBy('col1', 'col2').pivot('word').count()\n",
    "\n",
    "### dealing with NLP related features\n",
    "# replace unwanted characters\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "REGEX = '[,\\\\-]'\n",
    "df = df.withColumn('text', regexp_replace(df.text, REGEX, ' '))\n",
    "# Tokenize words\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "df = Tokenizer(inputCol=\"text\", outputCol=\"tokens\").transform(df)\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopwords = StopWordsRemover(inputCol='tokens', outputCol='words')\n",
    "stopwords.getStopWords() # Take a look at the list of stop words when stopwords = StopWordsRemover()\n",
    "df = stopwords.transform(df)\n",
    "# Hash the features\n",
    "from pyspark.ml.feature import HashingTF\n",
    "hasher = HashingTF(inputCol=\"words\", outputCol=\"hash\", numFeatures=32)\n",
    "df = hasher.transform(df)\n",
    "# Normalize the text features (TF-IDF)\n",
    "from pyspark.ml.feature import IDF\n",
    "df = IDF(inputCol=\"hash\", outputCol=\"features\").fit(df).transform(df)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Binarizing (create column with value to 0 or 1)\n",
    "from pyspark.ml.feature import Binarizer\n",
    "df = df.withColumn('val', df['val'].cast('double'))\n",
    "bin = Binarizer(threshold=0.0, inputCol='val', outputCol='binary_col')\n",
    "df = bin.transform(df)\n",
    "\n",
    "# Bucketing \n",
    "from pyspark.ml.feature import Bucketizer\n",
    "splits = [0, 1, 2, 3, 4, float('Inf')]\n",
    "# Create bucketing transformer\n",
    "buck = Bucketizer(splits=splits, inputCol='BATHSTOTAL', outputCol='baths')\n",
    "# Apply transformer\n",
    "df = buck.transform(df)\n",
    "\n",
    "# One-hot encoding (Can be used with PYSPARK PIPELINE and PYSPARK MACHINE LEARNING model)\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "string_indexer = StringIndexer(inputCol='cat_col', outputCol='Cat_Index') # Map strings to numbers with string indexer\n",
    "indexed_df = string_indexer.fit(df).transform(df)\n",
    "encoder = OneHotEncoder(inputCol='Cat_Index', outputCol='Onehot_feature') # Onehot encode indexed values\n",
    "encoded_df = encoder.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "# Using Pipeline to do many steps at once\n",
    "from pyspark.ml import Pipeline\n",
    "features_cols = list(df.columns) # Check for non-null columns\n",
    "features_cols.remove('some_null_col') # Remove the dependent variable from the list\n",
    "df = df.fillna(-1) # Vector Assembler should not take in any nulls\n",
    "vec_assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"Onehot_feature\"], outputCol=\"features\") # features_cols\n",
    "pipeline = Pipeline(stages=[string_indexer, encoder, vec_assembler]) # Last stage is model: eg : stages=[.., model]\n",
    "pipeline_model = pipeline.fit(df)\n",
    "transformed_df = pipeline_model.transform(df)\n",
    "\n",
    "# SPLIT DATA\n",
    "\n",
    "# Create Model\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"SALESCLOSEPRICE\",\n",
    "                    predictionCol=\"Prediction_Price\", seed=42 )\n",
    "model = rf.fit(train_df) # Train model\n",
    "predictions = model.transform(test_df)\n",
    "model.save('rfr_model') # Save model\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "model2 = RandomForestRegressionModel.load('rfr_model') # Load the model\n",
    "# Evaluate Model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"SALESCLOSEPRICE\", predictionCol=\"Prediction_Price\")\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Feature importance\n",
    "import pandas as pd\n",
    "# Convert feature importances to a pandas column\n",
    "importance_df = pd.DataFrame(model.featureImportances.toArray(), columns=['importance'])\n",
    "importance_df['features'] = pd.Series(feature_cols) # Create a new column to hold feature names\n",
    "importance_df.sort_values(by=['importance'], ascending=False, inplace=True) # Sort the data based on feature importance\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# One-hot encoding\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "# StringIndexer does indexing for each category. this step allows handling unseen category in testing set\n",
    "string_indexer1 = StringIndexer(inputCol=\"cat_col\",outputCol=\"string_index\") \n",
    "one_hot_encoder1 = OneHotEncoder(inputCol=\"string_index\",outputCol=\"onehot_feature\") # One-hot encoding using the category indices\n",
    "# Combine all features\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\", \"onehot_feature1\", \"onehot_feature2\"], outputCol=\"features\")\n",
    "\n",
    "# Define the model\n",
    "model_rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10) # from pyspark.ml.classification import RandomForestClassifier\n",
    "model_lr1 = LogisticRegression(featuresCol=\"features\", labelCol=\"label\") # from pyspark.ml.classification import LogisticRegression\n",
    "# elasticNetParam = 0 is ridge regression, elasticNetParam = 1 is lasso regression (regularization parameters)\n",
    "model_lr2 = LinearRegression(featuresCol=\"features\", labelCol=\"label\", elasticNetParam=0, regParam=0.1) # from pyspark.ml.regression import LinearRegression\n",
    "model_kmeans = KMeans(featuresCol=\"features\", predictionCol=\"kmeans_prediction\", k=3) # from pyspark.ml.clustering import KMeans\n",
    "# deep learninng\n",
    "layers = [len(feature_cols) + 2, 5, 2]  # Input layer size, hidden layer sizes, output layer size\n",
    "model_dl = MultilayerPerceptronClassifier(layers=layers, labelCol=\"label\", featuresCol=\"features\", seed=123)\n",
    "# Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[string_indexer1, one_hot_encoder1, string_indexer2, one_hot_encoder2, vec_assembler, model_xx])\n",
    "\n",
    "# Create the parameter grid\n",
    "import pyspark.ml.tuning as tune\n",
    "paramGrid = tune.ParamGridBuilder()\\\n",
    "        .addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "        .addGrid(lr.elasticNetParam, [0, 1])\n",
    "        .build()\n",
    "\n",
    "# Evaluation metric\n",
    "import pyspark.ml.evaluation as evals\n",
    "evaluator_logistic = evals.BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "evaluator_reg = evals.RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_rf_dl = evals.MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\") \n",
    "# Create cross-validator\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3) \n",
    "\n",
    "# Split the data into training and test sets\n",
    "training, test = transformed_df.randomSplit([.6, .4])\n",
    "\n",
    "cvModel = cv.fit(training) # Fit the dataframe\n",
    "bestModel = cvModel.bestModel # Best model\n",
    "bestParams = bestModel.stages[-1].extractParamMap() # See best parameters\n",
    "test_results = bestModel.transform(test) # Use the model to predict the test set\n",
    "predictions = cvModel.transform(test) # Predict using testing set\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "test_results.groupBy(\"label\", \"prediction\").count().show() # Confusion matrix\n",
    "feature_importances = bestModel.stages[-1].featureImportances\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
