{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create dataframe from RDD\n",
    "spark_df = spark.createDataFrame(RDD, schema=colname_list)\n",
    "# Loading file (folder name will make the spark load all files in that folder in parallel mode)\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True) # .json, .txt, .load for parquet\n",
    "df.show(3) # Show first 3 rows\n",
    "df.collect() # Store result as list of tuples\n",
    "df.limit(3) # Same as show\n",
    "df.dtypes # See datatype of each column\n",
    "df.printSchema() # See schema information\n",
    "result.columns # See result table columns\n",
    "df = df.na.drop(subset=[\"col_name\"]) # Drop nulls\n",
    "df = df.drop(subset=[\"col_name\"]) # Drop column\n",
    "df = df.dropDuplicates() # Drop duplicates\n",
    "df = df.withColumn(\"col_name\", col(\"col_name\").cast(\"float\"))  # Way 1 : Casting a column to another data type\n",
    "df = df.withColumn(\"col_name\", df.col_name.cast(\"float\")) # Way 2 : Casting a column to another data type\n",
    "df.describe().show() # Summary stats\n",
    "df = df.repartition(4, 'some_col') # create 4 partitions using same column values of specified column\n",
    "print(df.rdd.getNumPartitions()) # See no of partitions of the dataset\n",
    "\n",
    "df = df.select(df.col1, df.col2, df.col3) # way1 : select column from dataframe\n",
    "df = df.select(\"col1\", \"col2\") # way2 : select column from dataframe\n",
    "df.select(col('col1'), col('col2')) # way3 : select column from dataframe,  import col from sql.functions\n",
    "df = df.withColumn(\"new_col\",df.old_col+10) # Add a new result column\n",
    "df = df.withColumnRenamed(\"old_col_name\", \"new_col_name\") # Rename column\n",
    "df = df.select(col('col1').alias('col1_renamed'), 'col2')\n",
    "df = df.selectExpr(\"col1\", \"col2\", \"col3\", \"col1/(col2/60) as another_col\")\n",
    "df = df.withColumn(\"idx\", monotonically_increasing_id()) # Creating id column\n",
    "df.where(array_contains('col', 'abc')) # Check if an element is inside an array\n",
    "df1 = df1.withColumn(\"source\", lit(\"df1\")) # Adding constants in a column\n",
    "\n",
    "df_vertical = df1.union(df2) # Vertical join (append rows vertically)\n",
    "df_horizontal = df1.join(df1, on=['common_col1', 'common_col2'], how=\"left\") (append columns horizontally with join)\n",
    "df_cross = df1.crossJoin(df2) # Cross Join (Horizontally appending columns of possible combinations)\n",
    "\n",
    "# Filtering (Both produces same results)\n",
    "df = df.filter(\"col_name > 120\").show()\n",
    "df = df.where(\"Value > 120\")\n",
    "df = df.filter(df.col_name > 120).show()\n",
    "df = df.where(df.Value > 120)\n",
    "filterA = df.col1 == \"SEA\"\n",
    "result = temp.filter(filterA).filter(filterB) # Chaining filters\n",
    "df.groupBy(\"col_name\").count() # Group by and count\n",
    "df.orderBy(\"col_name\") # order by \n",
    "df.filter(df.col == 'value').groupBy().max(\"another_col\") # Multiple chaining aggregation\n",
    "\n",
    "df.createOrReplaceTempView(\"table_name\") # Register DataFrame as a temporary talbe in catalog\n",
    "spark.catalog.listTables() # See all table information in the catalog\n",
    "spark.catalog.dropTempView('table_name') # Remove temp table from catalog\n",
    "spark_df = spark.table(\"table_name\") # start using a spark table as spark dataframe\n",
    "result = spark.sql(\"SELECT * FROM table_name\") # Run query on table\n",
    "\n",
    "# Using Custom function to double the value of a column\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "def double_val(col):\n",
    "    return col * 2 # Make sure any new data is casted to proper type\n",
    "double_val_udf = udf(double_val, IntegerType()) # Register UDF with custom function and return type\n",
    "df = df.withColumn(\"DoubledCol\", double_val_udf(df[\"col\"]))\n",
    "\n",
    "## Visualization : Pyspark_dist_explore, pandas (NOT RECOMMENDED), HandySpark(RECOMMENDED)\n",
    "pandas_df = spark_df.toPandas()\n",
    "handy_df = spark_df.toHandy() # Convert to handyspark dataframe\n",
    "handy_df.cols[\"col_name\"].hist()\n",
    "spark_df = handy_df.to_spark() # Convert to pyspark dataframe\n",
    "\n",
    "## NOTE\n",
    "# Array: [1.0, 0.0, 0.0, 3.0]\n",
    "# Sparse vector: (4, [0, 3], [1.0, 3.0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df = spark.read.csv(\"filename.csv\", header=True)\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "result = spark.sql(\"SELECT * FROM table_name\") # simple query, result saved as dataframe\n",
    "result.show()\n",
    "result = spark.sql(\"DESCRIBE tablename\") # See table information\n",
    "\n",
    "# Window functions\n",
    "query = \"\"\"\n",
    "SELECT *,\n",
    "ROW_NUMBER() OVER(PARTITION BY train_id ORDER BY time) AS id\n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query)\n",
    "\n",
    "# equivalent dot notation\n",
    "window = Window.partitionBy('train_id').orderBy('time')\n",
    "dfx = df.withColumn('id', row_number().over(window))\n",
    "\n",
    "# CASE.. WHEN\n",
    "query = \"\"\"\n",
    "SELECT id,\n",
    "    CASE\n",
    "        WHEN id < 25000 THEN 'Preface'\n",
    "        WHEN id < 50000 THEN 'Chapter 1'\n",
    "        WHEN id < 75000 THEN 'Chapter 2'\n",
    "        ELSE 'Chapter 3'\n",
    "    END AS title\n",
    "FROM df\n",
    "\"\"\"\n",
    "spark.sql(query)\n",
    "\n",
    "# equivalent dot notation\n",
    "df2 = df.withColumn('title', when(df.id < 25000, 'Preface')\n",
    ".when(df.id < 50000, 'Chapter 1')\n",
    ".when(df.id < 75000, 'Chapter 2')\n",
    ".otherwise('Chapter 3'))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Regex Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"John\", \"Contact me at (123) 456-7890 or (456) 789 0123\"),\n",
    "        (\"Alice\", \"No phone number in this text\"),\n",
    "        (\"Bob\", \"Call me at (555) 555-5555 or (555) 123-4567\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"name\", \"text\"])\n",
    "# Pattern Matching: Check if each string contains a phone number in the specified format\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df = df.withColumn(\"has_phone_number\", df[\"text\"].rlike(r'\\(\\d{3}\\) \\d{3}[- ]\\d{4}'))\n",
    "\n",
    "# Group Matching: Extract area code, exchange code, and subscriber number\n",
    "phone_pattern = r'\\((\\d{3})\\) (\\d{3})[- ](\\d{4})'\n",
    "df = df.withColumn(\"area_code\", regexp_extract(\"text\", phone_pattern, 1))\n",
    "df = df.withColumn(\"exchange_code\", regexp_extract(\"text\", phone_pattern, 2))\n",
    "df = df.withColumn(\"subscriber_number\", regexp_extract(\"text\", phone_pattern, 3))\n",
    "\n",
    "# Replace: Replace phone numbers with \"PHONE_NUMBER_REDACTED\"\n",
    "df = df.withColumn(\"redacted_text\", regexp_replace(\"text\", phone_pattern, \"PHONE_NUMBER_REDACTED\"))\n",
    "\n",
    "# Show DataFrame\n",
    "# df.show() # truncate=False\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# lowercase the strings in a column\n",
    "df = df.select(lower(col('col_name'))) \n",
    "# replace string or characters\n",
    "df = df1.select(regexp_replace('col_name', 'old', 'new').alias('new_col'))\n",
    "# Split a string on space\n",
    "df = df.select(split('string_col', '[ ]').alias('word_list'))\n",
    "# Split string using any given symbol\n",
    "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()\"\n",
    "df = df.select(split('string_col', '[ %s]' % punctuation).alias('word_list'))\n",
    "# Filter out empty strings from the resulting list\n",
    "df = df.filter(col('word_list') != '')\n",
    "# Explode the string list column so that each row contains one value of list\n",
    "df = df.select(explode('word_list').alias('word'))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching, Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- caching is lazy operation\n",
    "- Only cache when necessary (multiple Operation requires the dataframe or table)\n",
    "- Least Recently Used (LRU) as eviction policy\n",
    "- Eviction happens independently on each worker\n",
    "- uncache when it is no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Caching dataframe\n",
    "df.persist(storageLevel=pyspark.StorageLevel.MEMORY_AND_DISK) # cache a fataframe\n",
    "df.cache() # Alternative : cache a fataframe\n",
    "df.is_cached # Check if the dataframe is cached\n",
    "df.storageLevel # How the dataframe is cached (useDisk, useMemory, useOffHeap, deserialized, replication)\n",
    "df.unpersist() # uncache a fataframe\n",
    "# Caching table\n",
    "df.createOrReplaceTempView('df') # Register dataframe as table\n",
    "spark.catalog.cacheTable('df') # Cache the table\n",
    "spark.catalog.isCached(tableName='df') # Check if the table is cached\n",
    "spark.catalog.uncacheTable('df') # Uncache table\n",
    "spark.catalog.clearCache() # Clear all cache\n",
    "\n",
    "# Visualize cache operations, query plans at >>>>>> localhost:4040\n",
    "\n",
    "# Logging for inspecting\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # DEBUG, \n",
    "logging.info(\"Hello %s\", \"world\") # INFO level print\n",
    "logging.debug(\"Hello, take %d\", 2) # DEBUG level print\n",
    "ENABLED = False\n",
    "t = timer()\n",
    "logger.info(\"No action here.\")\n",
    "elapsed1 = t - timer()\n",
    "if ENABLED: # Managing processor usage leakage\n",
    "    logger.info(\"df has %d rows.\", df.count())\n",
    "elapsed2 = t - timer()\n",
    "\n",
    "# Log columns of text_df as debug message\n",
    "logging.deug(\"text_df columns: %s\", df1.columns)\n",
    "# Log whether table1 is cached as info message\n",
    "logging.info(\"table1 is cached: %s\", spark.catalog.isCached(tableName=\"table1\"))\n",
    "# Log first row of text_df as warning message\n",
    "logging.warning(\"The first row of text_df:\\n %s\", df1.first())\n",
    "# Log selected columns of text_df as error message\n",
    "logging.error(\"Selected columns: %s\", df1.select(\"id\", \"word\"))\n",
    "logging.disable(logging.DEBUG) # Turn off logging\n",
    "\n",
    "# Query plan analysis\n",
    "df.cache()\n",
    "df.explain() # Analysis on datafrmae (use bottom-up approach)\n",
    "spark.sql('EXPLAIN SELECT * FROM df').first() # Analysis using sql\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s', \n",
    "                              '%m-%d-%Y %H:%M:%S')\n",
    "\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "stdout_handler.setLevel(logging.DEBUG)\n",
    "stdout_handler.setFormatter(formatter)\n",
    "\n",
    "file_handler = logging.FileHandler('logs.log')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(stdout_handler)\n",
    "\n",
    "########### Simple way #############\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='test.log', format='%(filename)s: %(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "logging.debug('This is a debug message')\n",
    "logging.info('This is an info message')\n",
    "logging.warning('This is a warning message')\n",
    "logging.error('This is an error message')\n",
    "logging.critical('This is a critical message')\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector, Vectors\n",
    "dense_vector = DenseVector([1.0, 0.0, 3.0, 0.0, 5.0]) # [1.0,0.0,3.0,0.0,5.0]\n",
    "sparse_vector = SparseVector(5, [0, 2, 4], [1.0, 3.0, 5.0]) # (size, index, values) representation. Others are 0s\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "# Given you have \"spark\" session and \"dense_array\" column\n",
    "########### DENSE TO SPARSE #############\n",
    "def dense_to_sparse(dense_array):\n",
    "    return Vectors.dense(dense_array).toSparse()\n",
    "\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "dense_to_sparse_udf = udf(dense_to_sparse, ArrayType(DoubleType()))\n",
    "sparse_df = dense_df.withColumn(\"sparse_array\", dense_to_sparse_udf(\"dense_array\"))\n",
    "\n",
    "########### SPARSE TO DENSE  #############\n",
    "def sparse_to_dense(sparse_array):\n",
    "    return Vectors.sparse(len(sparse_array), [i for i, v in enumerate(sparse_array) if v != 0], \n",
    "                          [v for v in sparse_array if v != 0]).toArray()\n",
    "\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "sparse_to_dense_udf = udf(sparse_to_dense, ArrayType(DoubleType()))\n",
    "dense_df = sparse_df.withColumn(\"dense_array\", sparse_to_dense_udf(\"sparse_array\"))\n",
    "\n",
    "############## FOR TEXT DATA ###################\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, VectorAssembler # Efficient sparse representation for text\n",
    "vectorizer = CountVectorizer(inputCol=\"text_col\", outputCol=\"features\") \n",
    "vectorizer_model = vectorizer.fit(text_df)\n",
    "sparse_df = vectorizer_model.transform(text_df)\n",
    "\n",
    "# Convert sparse vectors to dense vectors\n",
    "vector_assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"dense_features\")\n",
    "dense_df = vector_assembler.transform(sparse_df).select(\"dense_features\")\n",
    "\n",
    "# Convert sparse vector column to array\n",
    "def sparse_to_array(sparse_vector):\n",
    "    return sparse_vector.toArray().tolist()\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "sparse_to_array_udf = udf(sparse_to_array, ArrayType(DoubleType()))\n",
    "array_df = sparse_df.withColumn(\"array_features\", sparse_to_array_udf(\"features\"))\n",
    "\n",
    "### Checking if sparse array is empty\n",
    "def is_sparse_array_empty(sparse_array):\n",
    "    return sparse_array.numNonzeros() == 0\n",
    "\n",
    "is_sparse_array_empty_udf = udf(is_sparse_array_empty, BooleanType())\n",
    "df_with_boolean = df.withColumn(\"is_empty\", is_sparse_array_empty_udf(\"sparse_col\"))\n",
    "\n",
    "### Using machine learning\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=123)\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "predictions = lr_model.transform(test_data)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions) # lr_model.evaluate(test_data)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
