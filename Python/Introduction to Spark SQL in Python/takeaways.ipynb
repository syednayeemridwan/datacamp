{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create dataframe from RDD\n",
    "spark_df = spark.createDataFrame(RDD, schema=colname_list)\n",
    "# Loading file (folder name will make the spark load all files in that folder in parallel mode)\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True) # .json, .txt, .load for parquet\n",
    "df.show(3)\n",
    "df.printSchema() # See schema information\n",
    "result.columns # See result table columns\n",
    "df.describe().show() # Summary stats\n",
    "df.createOrReplaceTempView(\"table_name\") # Register DataFrame as a temporary view\n",
    "result = spark.sql(\"SELECT * FROM table_name\") # Run query on table\n",
    "spark_df = spark.table(\"table_name\") # start using a spark table as spark dataframe\n",
    "# Add a new result column\n",
    "df = df.withColumn(\"new_col\",df.old_col+10)\n",
    "# Selecting column\n",
    "df = df.select(df.col1, df.col2, df.col3) # way1\n",
    "df.select(df.col1, df.col2) # way2\n",
    "from pyspark.sql.functions import col # way3\n",
    "df.select(col('col1'), col('col2'))\n",
    "calculated_col = (df.col1/(df.col2/60)).alias(\"another_col\")\n",
    "df = df.select(\"col1\", \"col2\", \"col3\", calculated_col)\n",
    "df = df.selectExpr(\"col1\", \"col2\", \"col3\", \"col1/(col2/60) as another_col\")\n",
    "df = df.select(col('col1').alias('col1_renamed'), 'col2')\n",
    "# Filtering (Both produces same results)\n",
    "df.filter(\"col_name > 120\").show()\n",
    "df.filter(df.col_name > 120).show()\n",
    "# Chaining filters\n",
    "filterA = df.col1 == \"SEA\"\n",
    "filterB = df.col2 == \"PDX\"\n",
    "result = temp.filter(filterA).filter(filterB)\n",
    "df = df.withColumn(\"idx\", monotonically_increasing_id()) # Creating id column\n",
    "df.groupBy(\"col_name\").count().show() # Group by and count\n",
    "df.orderBy(\"col_name\").show(3) # order by and count\n",
    "# Aggregation\n",
    "df.filter(df.col == 'value').groupBy().max(\"another_col\").show()\n",
    "df = df.na.drop(subset=[\"col_name\"]) # Drop nulls\n",
    "df = df.dropDuplicates() # Drop duplicates\n",
    "# Rename column\n",
    "df = df.withColumnRenamed(\"old_col_name\", \"new_col_name\")\n",
    "\n",
    "# Casting / Converting column type\n",
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"col_name\", col(\"col_name\").cast(\"float\"))\n",
    "df = df.withColumn(\"col_name\", df.col_name.cast(\"float\"))\n",
    "# Repartitioning based on similar values on a column for distributed computing\n",
    "df = df.repartition(4, 'some_col') # create 4 partitions\n",
    "print(df.rdd.getNumPartitions())\n",
    "# SQL with dataframe\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "df2 = spark.sql(\"SELECT * FROM table_name\")\n",
    "result = df2.collect() # Dataframe as list of rows tha you can iterate over\n",
    "\n",
    "## Visualization : Pyspark_dist_explore, pandas (NOT RECOMMENDED), HandySpark(RECOMMENDED)\n",
    "pandas_df = spark_df.toPandas()\n",
    "handy_df = spark_df.toHandy() # Convert to handyspark dataframe\n",
    "handy_df.cols[\"col_name\"].hist()\n",
    "spark_df = handy_df.to_spark() # Convert to pyspark dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df = spark.read.csv(\"filename.csv\", header=True)\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "result = spark.sql(\"SELECT * FROM table_name\") # simple query, result saved as dataframe\n",
    "result.show()\n",
    "result = spark.sql(\"DESCRIBE tablename\") # See table information\n",
    "\n",
    "# Window functions\n",
    "query = \"\"\"\n",
    "SELECT *,\n",
    "ROW_NUMBER() OVER(PARTITION BY train_id ORDER BY time) AS id\n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query)\n",
    "\n",
    "# equivalent dot notation\n",
    "window = Window.partitionBy('train_id').orderBy('time')\n",
    "dfx = df.withColumn('id', row_number().over(window))\n",
    "\n",
    "# CASE.. WHEN\n",
    "query = \"\"\"\n",
    "SELECT id,\n",
    "    CASE\n",
    "        WHEN id < 25000 THEN 'Preface'\n",
    "        WHEN id < 50000 THEN 'Chapter 1'\n",
    "        WHEN id < 75000 THEN 'Chapter 2'\n",
    "        ELSE 'Chapter 3'\n",
    "    END AS title\n",
    "FROM df\n",
    "\"\"\"\n",
    "spark.sql(query)\n",
    "\n",
    "# equivalent dot notation\n",
    "df2 = df.withColumn('title', when(df.id < 25000, 'Preface')\n",
    ".when(df.id < 50000, 'Chapter 1')\n",
    ".when(df.id < 75000, 'Chapter 2')\n",
    ".otherwise('Chapter 3'))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Regex Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"John\", \"Contact me at (123) 456-7890 or (456) 789 0123\"),\n",
    "        (\"Alice\", \"No phone number in this text\"),\n",
    "        (\"Bob\", \"Call me at (555) 555-5555 or (555) 123-4567\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"name\", \"text\"])\n",
    "# Pattern Matching: Check if each string contains a phone number in the specified format\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df = df.withColumn(\"has_phone_number\", df[\"text\"].rlike(r'\\(\\d{3}\\) \\d{3}[- ]\\d{4}'))\n",
    "\n",
    "# Group Matching: Extract area code, exchange code, and subscriber number\n",
    "phone_pattern = r'\\((\\d{3})\\) (\\d{3})[- ](\\d{4})'\n",
    "df = df.withColumn(\"area_code\", regexp_extract(\"text\", phone_pattern, 1))\n",
    "df = df.withColumn(\"exchange_code\", regexp_extract(\"text\", phone_pattern, 2))\n",
    "df = df.withColumn(\"subscriber_number\", regexp_extract(\"text\", phone_pattern, 3))\n",
    "\n",
    "# Replace: Replace phone numbers with \"PHONE_NUMBER_REDACTED\"\n",
    "df = df.withColumn(\"redacted_text\", regexp_replace(\"text\", phone_pattern, \"PHONE_NUMBER_REDACTED\"))\n",
    "\n",
    "# Show DataFrame\n",
    "# df.show() # truncate=False\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# lowercase the strings in a column\n",
    "df = df.select(lower(col('col_name'))) \n",
    "# replace string or characters\n",
    "df = df1.select(regexp_replace('col_name', 'old', 'new').alias('new_col'))\n",
    "# Split a string on space\n",
    "df = df.select(split('string_col', '[ ]').alias('word_list'))\n",
    "# Split string using any given symbol\n",
    "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()\"\n",
    "df = df.select(split('string_col', '[ %s]' % punctuation).alias('word_list'))\n",
    "# Filter out empty strings from the resulting list\n",
    "df = df.filter(col('word_list') != '')\n",
    "# Explode the string list column so that each row contains one value of list\n",
    "df = df.select(explode('word_list').alias('word'))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
