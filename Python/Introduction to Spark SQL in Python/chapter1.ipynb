{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SQL table from a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe can be used to create a temporary table. A temporary table is one that will not exist after the session ends. Spark documentation also refers to this type of table as a SQL temporary view. In the documentation this is referred to as to register the dataframe as a SQL temporary view. This command is called on the dataframe itself, and creates a table if it does not already exist, replacing it with the current data from the dataframe if it does already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trainsched.txt\n",
    "df = spark.read.csv(\"dataset/trainsched.txt\", header=True)\n",
    "\n",
    "# Create temporary table called table1\n",
    "df.createOrReplaceTempView(\"schedule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the column names of a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important to know column names because in practice relational tables are typically provided without additional documentation giving the table schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|train_id|   string|   NULL|\n",
      "| station|   string|   NULL|\n",
      "|    time|   string|   NULL|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the columns in the table schedule\n",
    "spark.sql(\"DESCRIBE schedule\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running sums using window function SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A window function is like an aggregate function, except that it gives an output for every row in the dataset instead of a single row per group.\n",
    "\n",
    "You can do aggregation along with window functions. A running sum using a window function is simpler than what is required using joins. The query duration can also be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+-------------+\n",
      "|train_id|      station| time|running_total|\n",
      "+--------+-------------+-----+-------------+\n",
      "|     217|       Gilroy|6:06a|         NULL|\n",
      "|     217|   San Martin|6:15a|         NULL|\n",
      "|     217|  Morgan Hill|6:21a|         NULL|\n",
      "|     217| Blossom Hill|6:36a|         NULL|\n",
      "|     217|      Capitol|6:42a|         NULL|\n",
      "|     217|       Tamien|6:50a|         NULL|\n",
      "|     217|     San Jose|6:59a|         NULL|\n",
      "|     324|San Francisco|7:59a|         NULL|\n",
      "|     324|  22nd Street|8:03a|         NULL|\n",
      "|     324|     Millbrae|8:16a|         NULL|\n",
      "|     324|    Hillsdale|8:24a|         NULL|\n",
      "|     324| Redwood City|8:31a|         NULL|\n",
      "|     324|    Palo Alto|8:37a|         NULL|\n",
      "|     324|     San Jose|9:05a|         NULL|\n",
      "+--------+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add col running_total that sums diff_min col in each group\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time, \n",
    "SUM(time) OVER (PARTITION BY train_id ORDER BY time) AS running_total\n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and display the result\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the broken query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query runs correctly, but gives an incorrect result in one of the rows because of an omission in the OVER clause. Can you locate the bug? Can you modify the query to make it give a reasonable result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-----+---------+\n",
      "|row|train_id|      station| time|time_next|\n",
      "+---+--------+-------------+-----+---------+\n",
      "|  1|     217|       Gilroy|6:06a|    6:15a|\n",
      "|  2|     217|   San Martin|6:15a|    6:21a|\n",
      "|  3|     217|  Morgan Hill|6:21a|    6:36a|\n",
      "|  4|     217| Blossom Hill|6:36a|    6:42a|\n",
      "|  5|     217|      Capitol|6:42a|    6:50a|\n",
      "|  6|     217|       Tamien|6:50a|    6:59a|\n",
      "|  7|     217|     San Jose|6:59a|    7:59a|\n",
      "|  8|     324|San Francisco|7:59a|    8:03a|\n",
      "|  9|     324|  22nd Street|8:03a|    8:16a|\n",
      "| 10|     324|     Millbrae|8:16a|    8:24a|\n",
      "| 11|     324|    Hillsdale|8:24a|    8:31a|\n",
      "| 12|     324| Redwood City|8:31a|    8:37a|\n",
      "| 13|     324|    Palo Alto|8:37a|    9:05a|\n",
      "| 14|     324|     San Jose|9:05a|     NULL|\n",
      "+---+--------+-------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-----+---------+\n",
      "|row|train_id|      station| time|time_next|\n",
      "+---+--------+-------------+-----+---------+\n",
      "|  1|     217|       Gilroy|6:06a|    6:15a|\n",
      "|  2|     217|   San Martin|6:15a|    6:21a|\n",
      "|  3|     217|  Morgan Hill|6:21a|    6:36a|\n",
      "|  4|     217| Blossom Hill|6:36a|    6:42a|\n",
      "|  5|     217|      Capitol|6:42a|    6:50a|\n",
      "|  6|     217|       Tamien|6:50a|    6:59a|\n",
      "|  7|     217|     San Jose|6:59a|     NULL|\n",
      "|  1|     324|San Francisco|7:59a|    8:03a|\n",
      "|  2|     324|  22nd Street|8:03a|    8:16a|\n",
      "|  3|     324|     Millbrae|8:16a|    8:24a|\n",
      "|  4|     324|    Hillsdale|8:24a|    8:31a|\n",
      "|  5|     324| Redwood City|8:31a|    8:37a|\n",
      "|  6|     324|    Palo Alto|8:37a|    9:05a|\n",
      "|  7|     324|     San Jose|9:05a|     NULL|\n",
      "+---+--------+-------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Give the number of the bad row as an integer\n",
    "bad_row = 7\n",
    "\n",
    "# Provide the missing clause, SQL keywords in upper case\n",
    "clause = 'PARTITION BY train_id'\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (PARTITION BY train_id ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are also cases where the dot notation gives a counterintuitive result, such as when a second aggregation on a column clobbers a prior aggregation on that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217|6:06a|\n",
      "|     324|7:59a|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Give the identical result in each command\n",
    "spark.sql('SELECT train_id, MIN(time) AS start FROM schedule GROUP BY train_id').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217|6:06a|\n",
      "|     324|7:59a|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('train_id').agg({'time':'min'}).withColumnRenamed('min(time)', 'start').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+\n",
      "|train_id|min(time)|max(time)|\n",
      "+--------+---------+---------+\n",
      "|     217|    6:06a|    6:59a|\n",
      "|     324|    7:59a|    9:05a|\n",
      "+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the second column of the result\n",
    "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM schedule GROUP BY train_id').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|train_id|max(time)|\n",
      "+--------+---------+\n",
      "|     217|    6:59a|\n",
      "|     324|    9:05a|\n",
      "+--------+---------+\n",
      "\n",
      "max(time)\n"
     ]
    }
   ],
   "source": [
    "result = df.groupBy('train_id').agg({'time':'min', 'time':'max'})\n",
    "result.show()\n",
    "print(result.columns[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating the same column twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are cases where dot notation can be more cumbersome than SQL. This exercise calculates the first and last times for each train line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|train_id|start|  end|\n",
      "+--------+-----+-----+\n",
      "|     217|6:06a|6:59a|\n",
      "|     324|7:59a|9:05a|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, col\n",
    "expr = [min(col(\"time\")).alias('start'), max(col(\"time\")).alias('end')]\n",
    "dot_df = df.groupBy(\"train_id\").agg(*expr)\n",
    "dot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|train_id|start|  end|\n",
      "+--------+-----+-----+\n",
      "|     217|6:06a|6:59a|\n",
      "|     324|7:59a|9:05a|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a SQL query giving a result identical to dot_df\n",
    "query = \"SELECT train_id, MIN(time) as start, MAX(time) as end FROM schedule GROUP BY train_id\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate dot SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses SQL to set the value of a dataframe called df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----+---------+\n",
      "|train_id|     station| time|time_next|\n",
      "+--------+------------+-----+---------+\n",
      "|     217|      Gilroy|6:06a|    6:15a|\n",
      "|     217|  San Martin|6:15a|    6:21a|\n",
      "|     217| Morgan Hill|6:21a|    6:36a|\n",
      "|     217|Blossom Hill|6:36a|    6:42a|\n",
      "|     217|     Capitol|6:42a|    6:50a|\n",
      "+--------+------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT *, \n",
    "LEAD(time,1) OVER(PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----+---------+\n",
      "|train_id|     station| time|time_next|\n",
      "+--------+------------+-----+---------+\n",
      "|     217|      Gilroy|6:06a|    6:15a|\n",
      "|     217|  San Martin|6:15a|    6:21a|\n",
      "|     217| Morgan Hill|6:21a|    6:36a|\n",
      "|     217|Blossom Hill|6:36a|    6:42a|\n",
      "|     217|     Capitol|6:42a|    6:50a|\n",
      "+--------+------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead\n",
    "# Obtain the identical result using dot notation \n",
    "dot_df = df.withColumn('time_next', lead('time', 1)\n",
    "        .over(Window.partitionBy('train_id')\n",
    "        .orderBy('time')))\n",
    "dot_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert window function from dot notation to SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to add a column to a train schedule so that each row contains the number of minutes for the train to reach its next stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "window = Window.partitionBy('train_id').orderBy('time')\n",
    "dot_df = df.withColumn('diff_min', \n",
    "                    (unix_timestamp(lead('time', 1).over(window),'H:m') \n",
    "                     - unix_timestamp('time', 'H:m'))/60)\n",
    "# dot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQL query to obtain an identical result to dot_df\n",
    "query = \"\"\"\n",
    "SELECT *, \n",
    "(UNIX_TIMESTAMP(LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time),'H:m') \n",
    " - UNIX_TIMESTAMP(time, 'H:m'))/60 AS diff_min \n",
    "FROM schedule \n",
    "\"\"\"\n",
    "sql_df = spark.sql(query)\n",
    "# sql_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
