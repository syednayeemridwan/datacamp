{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a dataframe from a parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe file called sherlock_sentences.parquet is available in your workspace. Each row of this dataframe contains a single clause. Each clause is a sequence of words that is separated from other clauses by punctuation, such as periods, quotes, and other natural language delimiters that signify a sentence or sentence fragment. Your mission, if you choose to accept it, is to load this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|word  |id |\n",
      "+------+---+\n",
      "|it    |71 |\n",
      "|do    |72 |\n",
      "|not   |73 |\n",
      "|change|74 |\n",
      "|or    |75 |\n",
      "+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataframe\n",
    "df = spark.read.parquet('dataset/sherlock.parquet')\n",
    "\n",
    "# Filter and show the first 5 rows\n",
    "df.where('id > 70').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and explode a text column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each clause is a string containing one or more words separated by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+\n",
      "|_c0                                                                 |\n",
      "+--------------------------------------------------------------------+\n",
      "|The Project Gutenberg EBook of The Adventures of Sherlock Holmes    |\n",
      "|by Sir Arthur Conan Doyle                                           |\n",
      "|(#15 in our series by Sir Arthur Conan Doyle)                       |\n",
      "|Copyright laws are changing all over the world. Be sure to check the|\n",
      "|copyright laws for your country before downloading or redistributing|\n",
      "+--------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('dataset/sherlock.txt', header=False)\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+\n",
      "|words                                                                              |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "|[The, Project, Gutenberg, EBook, of, The, Adventures, of, Sherlock, Holmes]        |\n",
      "|[by, Sir, Arthur, Conan, Doyle]                                                    |\n",
      "|[, #15, in, our, series, by, Sir, Arthur, Conan, Doyle, ]                          |\n",
      "|[Copyright, laws, are, changing, all, over, the, world, , Be, sure, to, check, the]|\n",
      "|[copyright, laws, for, your, country, before, downloading, or, redistributing]     |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "# Split the clause column into a column called words \n",
    "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()\"\n",
    "split_df = df.select(split('_c0', '[ %s]' % punctuation).alias('words'))\n",
    "split_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|       The|\n",
      "|Adventures|\n",
      "|        of|\n",
      "|  Sherlock|\n",
      "|    Holmes|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Number of rows:  939694\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Explode the words column into a column called word \n",
    "exploded_df = split_df.select(explode('words').alias('word'))\n",
    "exploded_df.show(10)\n",
    "\n",
    "# Count the resulting number of rows in exploded_df\n",
    "print(\"\\nNumber of rows: \", exploded_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows:  807668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Explode the words column into a column called word \n",
    "exploded_df = exploded_df.filter(\"LEN(word) > 0\")\n",
    "\n",
    "# Count the resulting number of rows in exploded_df\n",
    "print(\"\\nNumber of rows: \", exploded_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using monotonically_increasing_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "monotonically_increasing_id is efficient at generating a column of integers that is always increasing. It is useful for creating a unique id per row. Which of the following sequences would not be generated by the `monotonically_increasing_id` operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1, 2, 3, 3, 4, 5, 5, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating context window feature data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moving window technique is useful for machine learning algorithms models that use context window feature data.\n",
    "\n",
    "A table text having columns id, word, part, title is available in your workspace. It contains chapters 9, 10, 11 and 12 of the Sherlock Holmes book. The words are already processed and organized into one word per row. Each word has a unique integer index provided by the column id. The id column is lower for words that appear earlier in the text and greater for words appearing later in the text.\n",
    "\n",
    "The first 10 rows of the dataset for chapter 12 are printed to the console as Table1. The first ten rows of the desired result, constrained to show part 12 (Chapter 12) are printed to the console as Table2. In Table2, the \"given\" word for the row is provided in column w3. Columns w1 and w2 give the two words immediately prior to the given word. Columns w4 and w5 give the two words immediately after the given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|      word| id|\n",
      "+----------+---+\n",
      "|       The|  0|\n",
      "|   Project|  1|\n",
      "| Gutenberg|  2|\n",
      "|     EBook|  3|\n",
      "|        of|  4|\n",
      "|       The|  5|\n",
      "|Adventures|  6|\n",
      "|        of|  7|\n",
      "|  Sherlock|  8|\n",
      "|    Holmes|  9|\n",
      "+----------+---+\n",
      "only showing top 10 rows\n",
      "\n",
      "807668\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "exploded_df = exploded_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "exploded_df.createOrReplaceTempView(\"text\")\n",
    "query = \"\"\"\n",
    "SELECT  *\n",
    "FROM text\n",
    "\"\"\"\n",
    "spark.sql(query).show(10)\n",
    "print(exploded_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+\n",
      "|      word| id|part|\n",
      "+----------+---+----+\n",
      "|       The|  0|   1|\n",
      "|   Project|  1|   1|\n",
      "| Gutenberg|  2|   1|\n",
      "|     EBook|  3|   1|\n",
      "|        of|  4|   1|\n",
      "|       The|  5|   1|\n",
      "|Adventures|  6|   1|\n",
      "|        of|  7|   1|\n",
      "|  Sherlock|  8|   1|\n",
      "|    Holmes|  9|   1|\n",
      "+----------+---+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate total count of rows\n",
    "total_count = exploded_df.count()\n",
    "\n",
    "# Calculate number of rows per part\n",
    "rows_per_part = total_count // 12\n",
    "\n",
    "# Calculate part number for each row\n",
    "df_with_part = exploded_df.withColumn(\"part\", F.expr(f\"ntile(12) over (order by id)\"))\n",
    "\n",
    "# Show DataFrame with part column\n",
    "df_with_part.show(10)\n",
    "df_with_part.createOrReplaceTempView(\"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+-------+-------+-------+\n",
      "|part|     w1|     w2|     w3|     w4|     w5|\n",
      "+----+-------+-------+-------+-------+-------+\n",
      "|  12|   NULL|   NULL|  Petya|     Go|     go|\n",
      "|  12|   NULL|  Petya|     Go|     go|    she|\n",
      "|  12|  Petya|     Go|     go|    she|     is|\n",
      "|  12|     Go|     go|    she|     is|calling|\n",
      "|  12|     go|    she|     is|calling|    and|\n",
      "|  12|    she|     is|calling|    and|weeping|\n",
      "|  12|     is|calling|    and|weeping|   like|\n",
      "|  12|calling|    and|weeping|   like|      a|\n",
      "|  12|    and|weeping|   like|      a|  child|\n",
      "|  12|weeping|   like|      a|  child|    and|\n",
      "+----+-------+-------+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "part,\n",
    "LAG(word, 2) OVER(PARTITION BY part ORDER BY id) AS w1,\n",
    "LAG(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\n",
    "word AS w3,\n",
    "LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w4,\n",
    "LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w5\n",
    "FROM text\n",
    "\"\"\"\n",
    "text_df = spark.sql(query).where(\"part = 12\")\n",
    "text_df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartitioning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe text_df is currently in a single partition. Suppose that you know that the upcoming processing steps are going to be grouping the data on chapters. Processing the data will be most efficient if each chapter stays within a single machine. To avoid unnecessary shuffling of the data from one machine to another, let's repartition the dataframe into one partition per chapter, using the repartition and getNumPartitions commands taught in the first video lesson to this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition text_df into 12 partitions on 'chapter' column\n",
    "repart_df = text_df.repartition(12, 'part')\n",
    "\n",
    "# Prove that repart_df has 12 partitions\n",
    "repart_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What type of data is this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words, song ids, and video ids are all examples of what type of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding common word sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our objective is to create a dataset where each row corresponds to a 5-tuple, having a count indicating how many times the tuple occurred in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+\n",
      "|      word| id|part|\n",
      "+----------+---+----+\n",
      "|       The|  0|   1|\n",
      "|   Project|  1|   1|\n",
      "| Gutenberg|  2|   1|\n",
      "|     EBook|  3|   1|\n",
      "|        of|  4|   1|\n",
      "|       The|  5|   1|\n",
      "|Adventures|  6|   1|\n",
      "|        of|  7|   1|\n",
      "|  Sherlock|  8|   1|\n",
      "|    Holmes|  9|   1|\n",
      "+----------+---+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM text\n",
    "\"\"\"\n",
    "ttext_df = spark.sql(query)\n",
    "ttext_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+---------+----------+-----+\n",
      "|           w1|       w2|       w3|       w4|        w5|count|\n",
      "+-------------+---------+---------+---------+----------+-----+\n",
      "|      Project|Gutenberg| Literary|  Archive|Foundation|   31|\n",
      "|          the|    other|     side|       of|       the|   24|\n",
      "|           in|      the|   region|       of|       the|   24|\n",
      "|           on|      the|     same|    lines|        as|   21|\n",
      "|Illustration:|     From|       an|      old|     print|   18|\n",
      "|           in|      the|   middle|       of|       the|   18|\n",
      "|          the|  Project|Gutenberg| Literary|   Archive|   18|\n",
      "|    Copyright|       by|Underwood|      and| Underwood|   17|\n",
      "|Illustration:|Copyright|       by|Underwood|       and|   17|\n",
      "|           up|      and|     down|      the|      room|   17|\n",
      "+-------------+---------+---------+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the top 10 sequences of five words\n",
    "query = \"\"\"\n",
    "SELECT w1, w2, w3, w4, w5, COUNT(*) AS count FROM (\n",
    "   SELECT word AS w1,\n",
    "   LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
    "   LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3,\n",
    "   LEAD(word,3) OVER(PARTITION BY part ORDER BY id ) AS w4,\n",
    "   LEAD(word,4) OVER(PARTITION BY part ORDER BY id ) AS w5\n",
    "   FROM text\n",
    ")\n",
    "GROUP BY w1, w2, w3, w4, w5\n",
    "ORDER BY count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique 5-tuples in sorted order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A previous lesson taught an operation that eliminates duplicates, fetching unique records. In a previous exercise you obtained common 5-tuples. We will combine these two capabilities to find the unique 5-tuples, sorted alphabetically in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+----------+------------+\n",
      "|       w1|        w2|         w3|        w4|          w5|\n",
      "+---------+----------+-----------+----------+------------+\n",
      "|        ~|        be|       used|        to|      convey|\n",
      "|zygomatic|       and|    frontal|     bones|       vault|\n",
      "|   zygoma|        in|      front|        of|         the|\n",
      "|       zu|      sein|       Vera|        at|         the|\n",
      "|  zoology|       was|        not|    merely|acknowledged|\n",
      "|  zoology|   observe|       only|       the|    muscular|\n",
      "|zone--not|       the|        red|margin--an|  artificial|\n",
      "|     zone|     which|       lies|     about|        half|\n",
      "|     zone|     which|corresponds|        to|         the|\n",
      "|     zone|separating|        the|    shadow|          of|\n",
      "+---------+----------+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unique 5-tuples sorted in descending order\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT w1, w2, w3, w4, w5 FROM (\n",
    "   SELECT word AS w1,\n",
    "   LEAD(word, 1) OVER(PARTITION BY PART ORDER BY id ) AS w2,\n",
    "   LEAD(word, 2) OVER(PARTITION BY PART ORDER BY id ) AS w3,\n",
    "   LEAD(word, 3) OVER(PARTITION BY PART ORDER BY id ) AS w4,\n",
    "   LEAD(word, 4) OVER(PARTITION BY PART ORDER BY id ) AS w5\n",
    "   FROM text\n",
    ")\n",
    "ORDER BY w1 DESC, w2 DESC, w3 DESC, w4 DESC, w5 DESC \n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most frequent 3-tuples per chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use a query as a subquery in a larger query. Spark SQL supports advanced features of SQL. Previously you learned how to find the most common word sequences over an entire book having 12 chapters. Now you will obtain the most frequent 3-tuple for each of the 12 chapters. You will do this using a window function to retrieve the top row per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+-------+\n",
      "| id|     word|part|chapter|\n",
      "+---+---------+----+-------+\n",
      "|  0|      The|   1|      1|\n",
      "|  1|  Project|   1|      1|\n",
      "|  2|Gutenberg|   1|      1|\n",
      "|  3|    EBook|   1|      1|\n",
      "|  4|       of|   1|      1|\n",
      "+---+---------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"text\")\n",
    "df = df.withColumn(\"chapter\", df[\"part\"])\n",
    "df = df.select(df.id, df.word, df.part, df.chapter)\n",
    "df.createOrReplaceTempView(\"text\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+-----+\n",
      "|chapter|  w1|   w2|   w3|count|\n",
      "+-------+----+-----+-----+-----+\n",
      "|      1|   I|think| that|   34|\n",
      "|      1| one|   of|  the|   24|\n",
      "|      1|that|   it|  was|   24|\n",
      "|      1|Lord|   St|Simon|   23|\n",
      "|      1|  It|   is|    a|   22|\n",
      "+-------+----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subquery = \"\"\"\n",
    "SELECT chapter, w1, w2, w3, COUNT(*) as count\n",
    "FROM\n",
    "(\n",
    "    SELECT\n",
    "    chapter,\n",
    "    word AS w1,\n",
    "    LEAD(word, 1) OVER(PARTITION BY chapter ORDER BY id ) AS w2,\n",
    "    LEAD(word, 2) OVER(PARTITION BY chapter ORDER BY id ) AS w3\n",
    "    FROM text\n",
    ")\n",
    "GROUP BY chapter, w1, w2, w3\n",
    "ORDER BY chapter, count DESC\n",
    "\"\"\"\n",
    "spark.sql(subquery).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------+-----+\n",
      "|chapter|       w1|    w2|    w3|count|\n",
      "+-------+---------+------+------+-----+\n",
      "|      1|        I| think|  that|   34|\n",
      "|      2|      the|United|States|   57|\n",
      "|      3|      the|United|States|  100|\n",
      "|      4|      the|United|States|  100|\n",
      "|      5|      met|  with|    in|   77|\n",
      "|      6|       of|   the|  bone|   56|\n",
      "|      7|commander|    in| chief|   51|\n",
      "|      8|     said|Prince|Andrew|   26|\n",
      "|      9|        I|   don|     t|   38|\n",
      "|     10|      out|    of|   the|   38|\n",
      "|     11|       of|   the|French|   40|\n",
      "|     12|      the|  will|    of|   30|\n",
      "+-------+---------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#   Most frequent 3-tuple per chapter\n",
    "query = \"\"\"\n",
    "SELECT chapter, w1, w2, w3, count FROM\n",
    "(\n",
    "  SELECT\n",
    "  chapter,\n",
    "  ROW_NUMBER() OVER (PARTITION BY chapter ORDER BY count DESC) AS row,\n",
    "  w1, w2, w3, count\n",
    "  FROM ( %s )\n",
    ")\n",
    "WHERE row = 1\n",
    "ORDER BY chapter ASC\n",
    "\"\"\" % subquery\n",
    "\n",
    "spark.sql(query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
