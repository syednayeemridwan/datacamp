{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing caching: part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe df1 is loaded from a csv file. Several processing steps are performed on it. As df1 is to be used more than once, it is a candidate for caching.\n",
    "\n",
    "A second dataframe df2 is created by performing additional compute-intensive steps on df1. It is also a candidate for caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"dataset/sherlock.txt\")\n",
    "print(df1.is_cached)\n",
    "\n",
    "# Cache df1\n",
    "df1.cache()\n",
    "\n",
    "# Prove df1 is cached\n",
    "print(df1.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing caching: the SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we examined two DataFrames: df1 and df2 (which is created from df1). We tried caching df1, but not df2. In this exercise, we'll examine the effects of caching df2, but not df1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "df2 = spark.read.csv(\"dataset/trainsched.txt\")\n",
    "print(df2.is_cached)\n",
    "\n",
    "\n",
    "# Persist df2 using memory and disk storage level \n",
    "df2.persist(storageLevel=pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "print(df2.is_cached)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing caching: putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the best approach to caching df1 and df2 and why?\n",
    "\n",
    "Your results will vary; but here is one (random) result for each of the two approaches:\n",
    "\n",
    "First answer (cache df1):\n",
    "```\n",
    "df1_1st : 2.4s\n",
    "df1_2nd : 0.1s\n",
    "df2_1st : 0.3s\n",
    "df2_2nd : 0.2s\n",
    "Overall elapsed : 3.9\n",
    "```\n",
    "Second answer (cache df2):\n",
    "```\n",
    "df1_1st : 2.3s\n",
    "df1_2nd : 1.1s\n",
    "df2_1st : 1.7s\n",
    "df2_2nd : 0.1s\n",
    "Overall elapsed : 6.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cache df1, because it improves the time of the 2nd, 3rd, and 4th action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching and uncaching tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lesson we learned that tables can be cached. Whereas a dataframe is cached using a cache or persist operation, a table is cached using a cacheTable operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables:\n",
      " [Table(name='table1', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True), Table(name='table2', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n",
      "table1 is cached:  True\n",
      "table1 is cached:  False\n"
     ]
    }
   ],
   "source": [
    "df1.createOrReplaceTempView(\"table1\")\n",
    "df2.createOrReplaceTempView(\"table2\")\n",
    "# List the tables\n",
    "print(\"Tables:\\n\", spark.catalog.listTables())\n",
    "\n",
    "# Cache table1 and Confirm that it is cached\n",
    "spark.catalog.cacheTable('table1')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('table1'))\n",
    "\n",
    "# Uncache table1 and confirm that it is uncached\n",
    "spark.catalog.uncacheTable('table1')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('table1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark UI storage tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A folder sherlock_parts exists on disk containing twelve text files.\n",
    "```\n",
    "ls sherlock_parts\n",
    "sherlock_part0.txt   sherlock_part2.txt   sherlock_part7.txt\n",
    "sherlock_part1.txt   sherlock_part3.txt   sherlock_part8.txt\n",
    "sherlock_part10.txt  sherlock_part4.txt   sherlock_part9.txt\n",
    "sherlock_part11.txt  sherlock_part5.txt\n",
    "sherlock_part12.txt  sherlock_part6.txt\n",
    "```\n",
    "When loaded, this creates a dataframe having seven partitions.\n",
    "```\n",
    "partitioned_df = spark.read.text('sherlock_parts')\n",
    "partitioned_df.rdd.getNumPartitions()\n",
    "7\n",
    "```\n",
    "A table is created, and the table is cached:\n",
    "```\n",
    "partitioned_df.createOrReplaceTempView('text')\n",
    "spark.catalog.cacheTable('text')\n",
    "```\n",
    "What will appear in the Spark UI Storage tab once the cache operation is triggered by an action?\n",
    "<center><img src=\"images/03.01.png\"  style=\"width: 400px, height: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.02.png\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting cache in the Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe partitioned_df is available. It is used to register a temporary table called text. text is then cached using spark.catalog.cacheTable('text'). If you were running Spark locally, then the Spark UI would be available at http://localhost:4040/storage/. For the purpose of this exercise, examine the following image. It shows what the Spark UI would display once the cache for text is loaded:\n",
    "\n",
    "<center><img src=\"images/03.03.png\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "\n",
    "\n",
    "This shows that a table called text having seven partitions is cached in memory. Which of the following would immediately cause the above to appear in Spark UI?\n",
    "\n",
    "1. Performing a transform on the underlying dataframe, for example df = partitioned_df.distinct().\n",
    "2. Counting the underlying dataframe, for example: partitioned_df.count()\n",
    "3. Querying the table using, say: spark.sql(\"select count(*) from text\")\n",
    "4. Querying and showing the result, say: spark.sql(\"select count(*) from text\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (2) and (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now practice these logging operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - text_df columns: ['_c0']\n",
      "DEBUG - Command to send: c\n",
      "o73\n",
      "isCached\n",
      "stable1\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybfalse\n",
      "INFO - table1 is cached: False\n",
      "DEBUG - Command to send: c\n",
      "o39\n",
      "limit\n",
      "i1\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro110\n",
      "DEBUG - Command to send: c\n",
      "o13\n",
      "setCallSite\n",
      "sfirst at C:\\\\Users\\\\88016\\\\AppData\\\\Local\\\\Temp/ipykernel_12448/2156017374.py:12\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: c\n",
      "o110\n",
      "collectToPython\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yto111\n",
      "DEBUG - Command to send: c\n",
      "o13\n",
      "setCallSite\n",
      "n\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: a\n",
      "e\n",
      "o111\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi3\n",
      "DEBUG - Command to send: a\n",
      "g\n",
      "o111\n",
      "i0\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi57774\n",
      "DEBUG - Command to send: a\n",
      "e\n",
      "o111\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi3\n",
      "DEBUG - Command to send: a\n",
      "g\n",
      "o111\n",
      "i1\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ys0961058a4144d1c421e124e522ecfb9047eb4b6ce8c18ecd3c484e7565325ad2\n",
      "WARNING - The first row of text_df:\n",
      " Row(_c0='The Project Gutenberg EBook of The Adventures of Sherlock Holmes')\n",
      "DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "col\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ym\n",
      "DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "col\n",
      "s_c0\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro112\n",
      "DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ym\n",
      "DEBUG - Command to send: i\n",
      "java.util.ArrayList\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ylo113\n",
      "DEBUG - Command to send: c\n",
      "o113\n",
      "add\n",
      "ro112\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybtrue\n",
      "DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "ro113\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro114\n",
      "DEBUG - Command to send: c\n",
      "o39\n",
      "select\n",
      "ro114\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro115\n",
      "DEBUG - Command to send: c\n",
      "o24\n",
      "sessionState\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro116\n",
      "DEBUG - Command to send: c\n",
      "o116\n",
      "conf\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro117\n",
      "DEBUG - Command to send: c\n",
      "o117\n",
      "isReplEagerEvalEnabled\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybfalse\n",
      "DEBUG - Command to send: c\n",
      "o115\n",
      "schema\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro118\n",
      "DEBUG - Command to send: c\n",
      "o118\n",
      "json\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ys{\"type\":\"struct\",\"fields\":[{\"name\":\"_c0\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}\n",
      "ERROR - Selected columns: DataFrame[_c0: string]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Command to send: m\n",
      "d\n",
      "o111\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o113\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, format='%(levelname)s - %(message)s')\n",
    "\n",
    "# Log columns of text_df as debug message\n",
    "logging.debug(\"text_df columns: %s\", df1.columns)\n",
    "\n",
    "# Log whether table1 is cached as info message\n",
    "logging.info(\"table1 is cached: %s\", spark.catalog.isCached(tableName=\"table1\"))\n",
    "\n",
    "# Log first row of text_df as warning message\n",
    "logging.warning(\"The first row of text_df:\\n %s\", df1.first())\n",
    "\n",
    "# Log selected columns of text_df as error message\n",
    "logging.error(\"Selected columns: %s\", df1.select(\"_c0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice logging 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lesson we learned that Spark operations that trigger an action must be logged with care to avoid stealth loss of compute resources. You will now practice identifying logging statements that trigger an action on a dataframe or table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, format='%(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - text_df columns: ['_c0']\n",
      "DEBUG - Command to send: c\n",
      "o73\n",
      "isCached\n",
      "stable1\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybfalse\n",
      "INFO - table1 is cached: False\n",
      "DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "col\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ym\n",
      "DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "col\n",
      "s_c0\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro131\n",
      "DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ym\n",
      "DEBUG - Command to send: i\n",
      "java.util.ArrayList\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ylo132\n",
      "DEBUG - Command to send: c\n",
      "o132\n",
      "add\n",
      "ro131\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybtrue\n",
      "DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "ro132\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro133\n",
      "DEBUG - Command to send: c\n",
      "o39\n",
      "select\n",
      "ro133\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro134\n",
      "DEBUG - Command to send: c\n",
      "o24\n",
      "sessionState\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro135\n",
      "DEBUG - Command to send: c\n",
      "o135\n",
      "conf\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro136\n",
      "DEBUG - Command to send: c\n",
      "o136\n",
      "isReplEagerEvalEnabled\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybfalse\n",
      "DEBUG - Command to send: c\n",
      "o134\n",
      "schema\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro137\n",
      "DEBUG - Command to send: c\n",
      "o137\n",
      "json\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ys{\"type\":\"struct\",\"fields\":[{\"name\":\"_c0\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}\n",
      "ERROR - Selected columns: DataFrame[_c0: string]\n",
      "DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "toArray\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ym\n",
      "DEBUG - Command to send: i\n",
      "java.util.ArrayList\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ylo138\n",
      "DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "toArray\n",
      "ro138\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yto139\n",
      "DEBUG - Command to send: c\n",
      "o24\n",
      "sql\n",
      "sshow tables\n",
      "ro139\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro140\n",
      "DEBUG - Command to send: c\n",
      "o13\n",
      "setCallSite\n",
      "scollect at C:\\\\Users\\\\88016\\\\AppData\\\\Local\\\\Temp/ipykernel_12448/3093851631.py:7\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: c\n",
      "o140\n",
      "collectToPython\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yto141\n",
      "DEBUG - Command to send: c\n",
      "o13\n",
      "setCallSite\n",
      "n\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: a\n",
      "e\n",
      "o141\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi3\n",
      "DEBUG - Command to send: a\n",
      "g\n",
      "o141\n",
      "i0\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi57834\n",
      "DEBUG - Command to send: a\n",
      "e\n",
      "o141\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi3\n",
      "DEBUG - Command to send: a\n",
      "g\n",
      "o141\n",
      "i1\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ys0961058a4144d1c421e124e522ecfb9047eb4b6ce8c18ecd3c484e7565325ad2\n",
      "INFO - Tables: [Row(namespace='', tableName='table1', isTemporary=True), Row(namespace='', tableName='table2', isTemporary=True)]\n",
      "DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "toArray\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ym\n",
      "DEBUG - Command to send: i\n",
      "java.util.ArrayList\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ylo142\n",
      "DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "toArray\n",
      "ro142\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yto143\n",
      "DEBUG - Command to send: c\n",
      "o24\n",
      "sql\n",
      "sSELECT * FROM table1 limit 1\n",
      "ro143\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro144\n",
      "DEBUG - Command to send: c\n",
      "o24\n",
      "sessionState\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro145\n",
      "DEBUG - Command to send: c\n",
      "o145\n",
      "conf\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro146\n",
      "DEBUG - Command to send: c\n",
      "o146\n",
      "isReplEagerEvalEnabled\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybfalse\n",
      "DEBUG - Command to send: c\n",
      "o144\n",
      "schema\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro147\n",
      "DEBUG - Command to send: c\n",
      "o147\n",
      "json\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ys{\"type\":\"struct\",\"fields\":[{\"name\":\"_c0\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}\n",
      "DEBUG - First row: DataFrame[_c0: string]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Command to send: m\n",
      "d\n",
      "o132\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o138\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o141\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o142\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n"
     ]
    }
   ],
   "source": [
    "text_df = df1\n",
    "# Uncomment the 5 statements that do NOT trigger text_df\n",
    "logging.debug(\"text_df columns: %s\", text_df.columns)\n",
    "logging.info(\"table1 is cached: %s\", spark.catalog.isCached(tableName=\"table1\"))\n",
    "# logging.warning(\"The first row of text_df: %s\", text_df.first())\n",
    "logging.error(\"Selected columns: %s\", text_df.select(\"_c0\"))\n",
    "logging.info(\"Tables: %s\", spark.sql(\"show tables\").collect())\n",
    "logging.debug(\"First row: %s\", spark.sql(\"SELECT * FROM table1 limit 1\"))\n",
    "# logging.debug(\"Count: %s\", spark.sql(\"SELECT COUNT(*) AS count FROM table1\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice query plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe text_df is available. This dataframe is registered as a table called table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [_c0#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Datacamp/Python/Introduction to Spark SQL in Python/dataset/s..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.disable(logging.DEBUG)\n",
    "# Run explain on text_df\n",
    "text_df.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[count(1)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=147]\n",
      "      +- HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "         +- FileScan csv [] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Datacamp/Python/Introduction to Spark SQL in Python/dataset/s..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run explain on \"SELECT COUNT(*) AS count FROM table1\" \n",
    "spark.sql(\"SELECT COUNT(*) AS count FROM table1\").explain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[count(distinct _c0#17)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=170]\n",
      "      +- HashAggregate(keys=[], functions=[partial_count(distinct _c0#17)])\n",
      "         +- HashAggregate(keys=[_c0#17], functions=[])\n",
      "            +- Exchange hashpartitioning(_c0#17, 200), ENSURE_REQUIREMENTS, [plan_id=166]\n",
      "               +- HashAggregate(keys=[_c0#17], functions=[])\n",
      "                  +- FileScan csv [_c0#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Datacamp/Python/Introduction to Spark SQL in Python/dataset/s..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run explain on \"SELECT COUNT(DISTINCT word) AS words FROM table1\"\n",
    "spark.sql(\"SELECT COUNT(DISTINCT _c0) AS words FROM table1\").explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice reading query plans 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three dataframes are available: part2_df, part3_df, and part4_df. The questions posed in this exercise can be answered by inspecting the explain() output of each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [_c0#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Datacamp/Python/Introduction to Spark SQL in Python/dataset/s..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
