{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing creating a UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have two objectives to fulfill:\n",
    "\n",
    "- Ensure that the transformed data consists of nonempty vectors.\n",
    "- A dataframe has a column that contains arrays of string, where each array has a single item. You'd like to transform this column to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType, BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "# Returns true if the value is a nonempty vector\n",
    "nonempty_udf = udf(lambda x:  \n",
    "    True if (x and hasattr(x, \"toArray\") and x.numNonzeros())\n",
    "    else False, BooleanType())\n",
    "\n",
    "# Returns first element of the array as string\n",
    "s_udf = udf(lambda x: str(x[0]) if (x and type(x) is list and len(x) > 0)\n",
    "    else '', StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing array column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TRIVIAL_TOKENS` variable is a set. It contains certain words that we want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|by Sir Arthur Con...|\n",
      "|(#15 in our serie...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "digits = set(str(i) for i in range(10))  # Set of digits from 0 to 9\n",
    "characters = set(chr(i) for i in range(ord('a'), ord('z')+1)) - {'a', 'd', 'i'}  # Set of characters excluding 'a', 'd', 'i'\n",
    "\n",
    "# Combine the sets\n",
    "TRIVIAL_TOKENS = digits.union(characters)\n",
    "df_before = spark.read.csv(\"dataset/sherlock.txt\")\n",
    "df_before.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the rows where doc contains the item '5'\n",
    "# df_before.where(array_contains('doc', '5')).show()\n",
    "\n",
    "# # UDF removes items in TRIVIAL_TOKENS from array\n",
    "# rm_trivial_udf = udf(lambda x:\n",
    "#                      list(set(x) - TRIVIAL_TOKENS) if x\n",
    "#                      else x,\n",
    "#                      ArrayType(StringType()))\n",
    "\n",
    "# # Remove trivial tokens from 'in' and 'out' columns of df2\n",
    "# df_after = df_before.withColumn('in', rm_trivial_udf('in'))\\\n",
    "#                     .withColumn('out', rm_trivial_udf('out'))\n",
    "\n",
    "# # Show the rows of df_after where doc contains the item '5'\n",
    "# df_after.where(array_contains('doc','5')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a UDF for vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe df is available, having a column output of type vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Selects the first element of a vector column\n",
    "# first_udf = udf(lambda x:\n",
    "#             float(x.indices[0]) \n",
    "#             if (x and hasattr(x, \"toArray\") and x.numNonzeros()) \n",
    "#             else 0.0,\n",
    "#             FloatType())\n",
    "\n",
    "# # Apply first_udf to the output column\n",
    "# df.select(first_udf(\"output\").alias(\"result\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying a UDF to vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe is available called df having a column output of type vector. Its first five rows are shown in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add label by applying the get_first_udf to output column\n",
    "# df_new = df.withColumn('label', get_first_udf('output'))\n",
    "\n",
    "# # Show the first five rows \n",
    "# df_new.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming text to vector format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You learned how to split sentences and transform an array of words into a numerical vector using a `CountVectorizer`.You will first perform a transform that adds an invec column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform df using model\n",
    "# result = model.transform(df.withColumnRenamed('in', 'words'))\\\n",
    "#         .withColumnRenamed('words', 'in')\\\n",
    "#         .withColumnRenamed('vec', 'invec')\n",
    "# result.drop('sentence').show(3, False)\n",
    "\n",
    "# # Add a column based on the out column called outvec\n",
    "# result = model.transform(result.withColumnRenamed('out', 'words'))\\\n",
    "#         .withColumnRenamed('words', 'out')\\\n",
    "#         .withColumnRenamed('vec', 'outvec')\n",
    "# result.select('invec', 'outvec').show(3, False)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe df is available having columns endword: string, features: vector, and outvec: vector. You are to select the rows where endword equals \"him\", and add a column label having the integer value 1. Then, use the union operation to add an equal number of rows having endword not equals to him, such that these additional rows have label = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the lit function\n",
    "# from pyspark.sql.functions import lit\n",
    "\n",
    "# # Select the rows where endword is 'him' and label 1\n",
    "# df_pos = df.where(\"endword = 'him'\")\\\n",
    "#            .withColumn('label', lit(1))\n",
    "\n",
    "# # Select the rows where endword is not 'him' and label 0\n",
    "# df_neg = df.where(\"endword <> 'him'\")\\\n",
    "#            .withColumn('label', lit(0))\n",
    "\n",
    "# # Union pos and neg in equal number\n",
    "# df_examples = df_pos.union(df_neg.limit(df_pos.count()))\n",
    "# print(\"Number of examples: \", df_examples.count())\n",
    "# df_examples.where(\"endword <> 'him'\").sample(False, .1, 42).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe df_examples is available having columns endword: string, features: vector, outvec: vector, and label: int. You're going to split it to obtain training and testing set, which you will use to train and test a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the examples into train and test, use 80/20 split\n",
    "# df_trainset, df_testset = df_examples.randomSplit((.8,.2), 42)\n",
    "\n",
    "# # Print the number of training examples\n",
    "# print(\"Number training: \", df_trainset.count())\n",
    "\n",
    "# # Print the number of test examples\n",
    "# print(\"Number test: \", df_testset.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe df_trainset you created in the previous exercise is available. You're now going to use it to train a Logistic Regression Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the logistic regression classifier\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# # Instantiate logistic setting elasticnet to 0.0\n",
    "# logistic = LogisticRegression(maxIter=100, regParam=0.4, elasticNetParam=0.0)\n",
    "\n",
    "# # Train the logistic classifer on the trainset\n",
    "# df_fitted = logistic.fit(df_trainset)\n",
    "\n",
    "# # Print the number of training iterations\n",
    "# print(\"Training iterations: \", df_fitted.summary.totalIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trained logistic regression model df_fitted is available. A dataframe df_testset is available containing test data for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score the model on test data\n",
    "# testSummary = df_fitted.evaluate(df_testset)\n",
    "\n",
    "# # Print the AUC metric\n",
    "# print(\"\\ntest AUC: %.3f\" % testSummary.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fitted logistic model df_fitted is available. A dataframe df_testset is available containing test data for this model. A variable fields is available, containing the list ['prediction', 'label', 'endword', 'doc', 'probability']; this is used to specify which prediction fields to print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields = ['prediction', 'label', 'endword', 'doc', 'probability']\n",
    "\n",
    "# # Apply the model to the test data\n",
    "# predictions = df_fitted.transform(df_testset).select(fields)\n",
    "\n",
    "# # Print incorrect if prediction does not match label\n",
    "# for x in predictions.take(8):\n",
    "#     print()\n",
    "#     if x.label != int(x.prediction):\n",
    "#         print(\"INCORRECT ==> \")\n",
    "#     for y in fields:\n",
    "#         print(y,\":\", x[y])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
